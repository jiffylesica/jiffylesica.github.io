[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Jiffy and this is my blog for CS451!"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html",
    "href": "posts/Implementing Perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "Link to Perceptron Implementation: Perceptron pyfile"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#abstract",
    "href": "posts/Implementing Perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "Abstract:",
    "text": "Abstract:\nThis blog post demonstrates an implementation and evaluation of the perceptron algorithm. It walks through the coding of a perceptron class, which inherits and extends functionality of a LinearModel class. This implementation includes the calculation of data sample score, and gradient descent updates to a weight vector. Following this implementation, the blog post validates the source code by performing experiments on both linearly separable and non-separable datasets. Additionally, it explores the effect of modifying the algorithm to allow for minibatch updates, and analyzes runtime complexities, offering a comprehensive view of how these factors impact convergence and overall performance.\n\nimport torch\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nIn the perceptron.py source code, I have implemented a Perceptron class. Of not is the gradient descent function - grad() - which performs gradient descent on the data contained in our instance of the Perceptron model:\nStep 1: Score Computation - The function computes a score for each data point in X by performing matrix multiplication between the feature matrix - X - and the weight vector - self.w.\nscore = X @ self.w\nStep 2: Update labels - since the perceptron update requires labels of -1 or 1, the function converts the binary labels y (currently 0 or 1) to these values.\ny_ = 2 * y - 1\nStep 3: Identify misclassifications - for each data point, the function checks if the product of the new label - -1 or 1 - and the data points score is negative. If it is negative, this means that there is a misclassification. Since the perceptron algorithm thresholds scores at 0 - i.e. positive scores are classified as positive predictions, and negative scores as negative predictions - a negative product between target value and score indicates a misalignment between the two.\nmisclassified = 1.0 * (y_ * score &lt; 0).float()\nStep 4: Reshaping for Multiplication - both our target value vector and misclassified vector are reshaped into column vectors to allow for multiplication in the perceptron update (technical code, not essential to math of function)\nStep 5: Compute the update - the update term - which is returned to the step function which actually updates the perceptron weight vector - is calculated as the mean over all datapoints of X which have been evaluated for misclassification, allowing us to subtract the gradient in the optimizer’s step function.\nreturn torch.mean((- (misclassified * y_) * X), dim=0)\nPut together, this results in the full grad() function:\ndef grad(self, X, y):\n        score = X @ self.w\n        y_ = 2 * y - 1\n        misclassified = 1.0 * (y_ * score &lt; 0).float()\n        # Had to reshape both to [300,1] tensors to multiply\n        # https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html\n        misclassified = misclassified.reshape(misclassified.shape[0], 1)\n        y_ = y_.reshape(y_.shape[0], 1)\n\n\n        # Return update\n        return torch.mean((- (misclassified * y_) * X), dim = 0)\n\nTensor shape exploration\nFirst, I set up small tensor examples to visualize their shapes throughout the empirical risk minimization process, and how we can reshape a tensor to ensure proper dimensions for matrix multiplication.\n\n# Code which helps visualize the dimensionality of tensors used in grad function\ntest_X = torch.tensor([\n    [2.0, -1.0, 0.5],\n    [1.0, -2.0, 1.5],\n    [3.0, 0.0, -0.5]\n])\nprint(\"test_X\")\nprint(test_X)\nprint(test_X.shape, end=\"\\n\\n\")\n\ntest_w = torch.tensor([0.5, 0.5, 0.5]) \nprint(\"test_w\")\nprint(test_w)\nprint(test_w.shape, end=\"\\n\\n\")\n\ntest_y = torch.tensor([1.0, 1.0, 0.0]) \nprint(\"test_y\")\nprint(test_y)\nprint(test_y.shape, end=\"\\n\\n\")\n\nscores = test_X @ test_y\nprint(\"scores\")\nprint(scores)\nprint(scores.shape, end=\"\\n\\n\")\n\ntest_y_ = 2 * test_y - 1  \nprint(\"test_y_\")\nprint(test_y_)\nprint(test_y_.shape, end=\"\\n\\n\")\n\nmisclassified = 1.0 * (test_y_ * scores &lt; 0).float()\nprint(\"misclassified\")\nprint(misclassified)\nprint(misclassified.shape, end=\"\\n\\n\")\n\nout_tensor = - (misclassified * test_y_) * test_X\nprint(\"out_tensor\")\nprint(out_tensor)\nprint(out_tensor.shape, end=\"\\n\\n\")\n\nmean_test = torch.mean(out_tensor, dim=0) # Seems dim = 0 is column-wise, dim = 1 is row\nprint(\"mean_test\")\nprint(mean_test)\nprint(mean_test.shape)\n\ntest_X\ntensor([[ 2.0000, -1.0000,  0.5000],\n        [ 1.0000, -2.0000,  1.5000],\n        [ 3.0000,  0.0000, -0.5000]])\ntorch.Size([3, 3])\n\ntest_w\ntensor([0.5000, 0.5000, 0.5000])\ntorch.Size([3])\n\ntest_y\ntensor([1., 1., 0.])\ntorch.Size([3])\n\nscores\ntensor([ 1., -1.,  3.])\ntorch.Size([3])\n\ntest_y_\ntensor([ 1.,  1., -1.])\ntorch.Size([3])\n\nmisclassified\ntensor([0., 1., 1.])\ntorch.Size([3])\n\nout_tensor\ntensor([[-0.0000,  1.0000,  0.5000],\n        [-0.0000,  2.0000,  1.5000],\n        [-0.0000, -0.0000, -0.5000]])\ntorch.Size([3, 3])\n\nmean_test\ntensor([0.0000, 1.0000, 0.5000])\ntorch.Size([3])\n\n\n\n\nHelper Functions\nBelow are a series of helper functions which will facilitate clean code/effective abstraction through this blog post.\n\nperceptron_data: creates set of labeled data points\nplot_perceptron_data: visualizes labeled date points using matplotlib’s pyplot\ndraw_line: plots decision boundary of a given weight vector (used in plots of data points after gradient descent). This function comes from Professor Phil Chodrow’s lecture notes for his Machine Learning course at Middlebury College.\n\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2): \n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nPerceptron Training Loop and Visualization\nThe below function - perceptron_loss_and_plot - trains the perceptron using the perceptron update rule. It logs the loss of the current weight vector after each iteration, and visualizes the final decision boundary (if the input data is 2-dimensional).\n\ndef perceptron_loss_and_plot(X, y, k_choice = 5):\n    # Generate 2d data\n\n    # Instantiate the model and optimizer\n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n\n    # Training loop\n    loss_vec = []\n    # Set max iterations to avoid infinite loop in nonseparable cases\n    max_iterations = 1000 \n    iteration = 0\n    loss = p.loss(X, y)\n\n    while loss &gt; 0 and iteration &lt; max_iterations:\n        loss = p.loss(X, y)\n        loss_vec.append(loss.item())\n        k = k_choice\n        idx = torch.randperm(X.size(0))[:k]\n        X_pass = X[idx, :]\n        y_pass = y[idx]\n        opt.step(X_pass, y_pass)\n        iteration += 1\n\n    print(\"Final Loss:\", loss.item(), \"after\", iteration, \"iterations.\")\n\n    # Plot loss per point\n    plt.figure()\n    plt.plot(loss_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n    plt.xlabel(\"Perceptron Iteration (Updates Only)\")\n    plt.ylabel(\"loss\")\n    \n    if X.shape[1] &lt;= 3:\n        # Data with separating line\n        fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        plot_perceptron_data(X, y, ax)\n        draw_line(p.w, -1, 2, ax, color = \"black\")\n        plt.show()"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#experiments",
    "href": "posts/Implementing Perceptron/index.html#experiments",
    "title": "Implementing Perceptron",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Generate a linearly separable, 2D dataset, and show that the perceptron algorithm converges to weight vector tha describes a separating line (i.e. achieves loss of 0)\n\n'''\nMaintaining low noise - and thus low standard deviation of data - produces linearly separable data\n'''\n\nX, y = perceptron_data(n_points=300, noise=0.2, p_dims=2)\nperceptron_loss_and_plot(X, y)\n\nFinal Loss: 0.0 after 180 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow visualization comes from Prof. Phil Chodrow’s Machine Learning Course, Lecture 7 - Introduction to Classification: The Perceptron (Subsection: A Complete Run)\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\nn= X.shape[0]\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0 and current_ax &lt; 6:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nExperiment 2: Generate a non-linearly separable, 2D dataset, and show that the perceptron algorithm does not converge on a final weight vector (i.e. does not achieve loss of 0). Instead, it runs max iterations without achieving perfect accuracy.\n\n'''\nBy increasing the noise parameter, we increase the standard deviation of the data points of a class.\nThis leads to more overlap across elements with different target values, increasing the likelihood\nof non-linearly separable data\n'''\nX, y = perceptron_data(n_points=300, noise=0.4, p_dims=2)\nperceptron_loss_and_plot(X, y)\n\nFinal Loss: 0.03333333507180214 after 1000 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 3: Show that the perceptron algorithm can run on data with at least 5 features, and visualize the evolution of the loss score over the raining period.\n\n'''\nAdjusting p_dims parameters allows us to modify number of features in data. \np_dims = 5 corresponds to 5 features.\n'''\nX, y = perceptron_data(n_points=300, noise=0.4, p_dims=5)\nperceptron_loss_and_plot(X, y)\n\nFinal Loss: 0.0 after 46 iterations.\n\n\n\n\n\n\n\n\n\nNote: This 5-feature data is linearly separable as the loss reaches 0.0!"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#minibatch-perceptron-experiments",
    "href": "posts/Implementing Perceptron/index.html#minibatch-perceptron-experiments",
    "title": "Implementing Perceptron",
    "section": "Minibatch Perceptron Experiments:",
    "text": "Minibatch Perceptron Experiments:\n\nX, y = perceptron_data(n_points=300, noise=0.2, p_dims=2)\n\n\nMinibatch Experiment 1: Test with a single sample to show that minibatch behaves like standard perceptron\n\nperceptron_loss_and_plot(X, y, k_choice=1)\n\nFinal Loss: 0.0 after 24 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen passing a batch size of 1 sample to the minibatch version of our algorithm, our implementation still behaves like a standard perceptron, performing gradient descent over one data point at a time.\n\n\nMinibatch Experiment 1: Test with a multiple samples - a small batch - to show that minibatch can still find separating line on 2D data\n\nperceptron_loss_and_plot(X, y, k_choice=10)\n\nFinal Loss: 0.0 after 9 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEven though each step of the perceptron algorithm performs computations on multiple data points, the minibatch algorithm can still find a separating line on linearly separable data, as shown by it achieving a final loss of 0.0\n\n\nMinibatch Experiment 1: Test with full dataset as batch - size of the batch is the number of samples in dataset - to show that minibatch can still converge even when data isn’t perfectly separable.\n\nX, y = perceptron_data(n_points=300, noise=0.4, p_dims=2)\nperceptron_loss_and_plot(X, y, k_choice=X.shape[0])\n\nFinal Loss: 0.02666666731238365 after 1000 iterations."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#runtime-complexity-of-standard-perceptron-single-iteration",
    "href": "posts/Implementing Perceptron/index.html#runtime-complexity-of-standard-perceptron-single-iteration",
    "title": "Implementing Perceptron",
    "section": "Runtime complexity of standard perceptron single iteration",
    "text": "Runtime complexity of standard perceptron single iteration\nA single iteration of the standard perceptron algorithm performs operations on only a single data point. So, runtime complexity will be dependent on the number of features which it is matrix multiplied with during score calculation and the update of the weight vector. Thus, for an input with k features, the standard perceptron single iteration runtime is O(k)."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#runtime-of-minibatch-perceptron",
    "href": "posts/Implementing Perceptron/index.html#runtime-of-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Runtime of Minibatch perceptron",
    "text": "Runtime of Minibatch perceptron\nThe runtime of the Minibatch perceptron has an upper bound complexity limit of O(n x k), where n is the number of samples operated on in an iteration, and k is again the number of features. Since we have shown in our experiments above that the minibatch can still converge when the batch size equals the number of samples in a data set of length n, n x p is the upper bounded complexity of a single iteration of the Minbatch algorithm."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#conclusion",
    "href": "posts/Implementing Perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn summary, the experiments confirm that the perceptron algorithm correctly converges on linearly separable data while revealing its limitations on non-linearly separable datasets. The analysis of runtime complexities shows that a standard perceptron iteration operates in O(k) time, while a minibatch implementation of the perceptron algorithm has an upper complexity bound of O(n × k). This exploration provides insights into how feature/weight-vector optimization can enhance our ability to classify data."
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Goals: 1. Train a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including gender, and 2. Perform a bias audit of our algorithm to determine whether it displays gender bias.\nIn this blog post, I trained a machine learning classifier on PUMS data from the state of Connecticut to predict an individual’s employment status based on demographic features. After preparing the data with the folktables library, I experimented with a various classifier models, ultimately deciding on a Random Forest model whose performance I evaluated using metrics like accuracy, Positive Prediction Value, and False Positive/Negative Rates. I then conducted a bias audit by examining false positive rates (FPR), false negative rates (FNR), and positive predictive value (PPV) across the gender groups available in the data - Male and Female. I then plotted feasible FNRs & FPRs to visualize the relationship of error rates between groups, and understand how much one group’s error rate would have to change to match the others. Our results showed notable differences in error rates and PPVs between groups, highlighting areas where the model may inadvertently misclassify women more frequently.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"CT\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000191\n1\n1\n302\n1\n9\n1013097\n40\n90\n...\n82\n39\n38\n81\n0\n38\n0\n0\n36\n34\n\n\n1\nP\n2018GQ0000285\n1\n1\n101\n1\n9\n1013097\n70\n18\n...\n69\n134\n7\n70\n6\n68\n141\n68\n132\n145\n\n\n2\nP\n2018GQ0000328\n1\n1\n1101\n1\n9\n1013097\n17\n54\n...\n37\n35\n16\n16\n0\n17\n18\n19\n18\n16\n\n\n3\nP\n2018GQ0000360\n1\n1\n905\n1\n9\n1013097\n47\n18\n...\n46\n48\n4\n90\n87\n84\n90\n3\n47\n48\n\n\n4\nP\n2018GQ0000428\n1\n1\n903\n1\n9\n1013097\n35\n96\n...\n32\n35\n36\n71\n36\n3\n37\n2\n2\n35\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n90\n16.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n2\n1\n6.0\n\n\n1\n18\n16.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n17.0\n5\n17\n1\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n3\n18\n19.0\n5\n17\n2\nNaN\n3\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n4\n96\n16.0\n2\n16\n1\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n1.0\n2\n1\n6.0"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#my-version",
    "href": "posts/Auditing Bias/index.html#my-version",
    "title": "Auditing Bias",
    "section": "My Version",
    "text": "My Version\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n\nfeatures_to_use1 = [f for f in possible_features if f not in [\"ESR\", \"SEX\"]]\n\n\nEmploymentProblemSex = BasicProblem(\n    features=features_to_use1,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblemSex.df_to_numpy(acs_data)\n\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nConvert Folktables data back to Pandas DataFrame for ease of descriptive analysis\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use1)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\n1. How many individuals are in the data?\n\nprint(f\"There are {df.shape[0]} individuals in the data for the state of CT\")\n\nThere are 29029 individuals in the data for the state of CT\n\n\n\n\n2. How Many Individuals Have a Target Label == 1 (i.e. How Many Individuals are Employed)?\n\nprint(f\"Of the {df.shape[0]} in the data set, {(df['label'].mean() * 100):.2f}% - or {df['label'].sum()} individuals - are employed\")\n\nOf the 29029 in the data set, 48.43% - or 14060 individuals - are employed\n\n\n\n\n3. Of the Employed Individuals, How Many are Male (1) and How Many are Female (2)?\n\nemployed_by_group = df.groupby(\"group\")[\"label\"].sum()\nprint(f\"Of the 14060 employed individuals, {employed_by_group.iloc[0]} are male and {employed_by_group.iloc[1]} are female\")\n\nOf the 14060 employed individuals, 7169 are male and 6891 are female\n\n\n\n\n4. In Each Group, What Proportion of Individuals Have Target Label Equal to 1 (i.e. Are Employed)?\n\nproportion_employed_by_group = df.groupby(\"group\")[\"label\"].mean()\nprint(f\"According to the Data,{proportion_employed_by_group.iloc[0]*100: .2f}% of male individuals are employed and{proportion_employed_by_group.iloc[1]*100: .2f}% female individuals are employed\")\n\nAccording to the Data, 51.17% of male individuals are employed and 45.88% female individuals are employed\n\n\n\n\n5. Intersectional Trends\n\n# Since RAC1P Has Values &gt; 2, we must filter to only 1.0 and 2.0\ndf_filtered = df[(df[\"RAC1P\"] == 1.0) | (df[\"RAC1P\"] == 2.0)].copy()\n# Now, we convert RAC1P values to ints to match type\ndf_filtered[\"RAC1P\"] = df_filtered[\"RAC1P\"].astype(int)\n# Since I want to use categorical labels, and not just numbered labels, we use Pandas map function on a series\n# https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html\ndf_filtered[\"group\"] = df_filtered[\"group\"].map({1 : \"Male\", 2 : \"Female\"})\ndf_filtered[\"RAC1P\"] = df_filtered[\"RAC1P\"].map({1 : \"White\", 2 : \"Black\"})\nproportion_employed_by_group = df_filtered.groupby([\"group\", \"RAC1P\"])[\"label\"].mean()\n\ngroup   RAC1P\nFemale  Black    0.466667\n        White    0.464758\nMale    Black    0.371805\n        White    0.535252\nName: label, dtype: float64\n\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Using seaborn barplot uses mean as default, which represents the proportional insights we are looking for\nsns.barplot(df_filtered, x = \"group\", y = \"label\", hue = \"RAC1P\", width=.8, gap=.2)\nplt.title(\"Intersectional Trends\")\n\nText(0.5, 1.0, 'Intersectional Trends')\n\n\n\n\n\n\n\n\n\nTraining my model with different classifiers\n\n# Include alternative classifiers from sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#logistic-regression",
    "href": "posts/Auditing Bias/index.html#logistic-regression",
    "title": "Auditing Bias",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n# Logistic Regression Classifier (playing with polynomial features)\n# Discovered we can ass PolynomialFeatures to pipeline!\nfrom sklearn.preprocessing import PolynomialFeatures\n# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\nmodel_sex_LR = make_pipeline(PolynomialFeatures(degree = 2), StandardScaler(), LogisticRegression(max_iter=1000))\nmodel_sex_LR.fit(X_train.copy(), y_train)\n\ny_hat_LR = model_sex_LR.predict(X_test)\n\n# Calculate Overall Values\nTP_LR = ((y_hat_LR == 1) & (y_test == 1)).sum()\nFP_LR = ((y_hat_LR == 1) & (y_test == 0)).sum()\nTN_LR = ((y_hat_LR == 0) & (y_test == 0)).sum()\nFN_LR = ((y_hat_LR == 0) & (y_test == 1)).sum()\n\nPPV_LR = TP_LR / (y_hat_LR == 1).sum()\nFPR_LR = FP_LR / (TP_LR + FP_LR)\nFNR_LR = FN_LR / (TP_LR + FN_LR)\n\nprint(f\"Accuracy for Logistic Regression:{((y_hat_LR == y_test).mean())*100: .2f}%\")\nprint(f\"PPV for Logistic Regression:{PPV_LR*100: .2f}%\")\nprint(f\"Overall FPR for LR:{FPR_LR*100: .2f}%\")\nprint(f\"Overall FNR for LR:{FNR_LR*100: .2f}%\")\n\nAccuracy for Logistic Regression: 82.57%\nPPV for Logistic Regression: 79.65%\nOverall FPR for LR: 20.35%\nOverall FNR for LR: 12.74%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_LR = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_LR[\"group\"] = group_test\ndf_test_LR[\"label\"] = y_test\ndf_test_LR[\"predicted_value\"] = y_hat_LR\ndf_test_LR[\"correct_prediction\"] = df_test_LR[\"predicted_value\"] == df_test_LR[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_LR[\"correct_prediction\"] = df_test_LR[\"predicted_value\"] == df_test_LR[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n \\n {df_test_LR.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_LR[\"true_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 1)\ndf_test_LR[\"false_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_LR = df_test_LR.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_LR = df_test_LR.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_LR / (TP_per_group_LR + FP_per_group_LR)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_LR[\"false_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n \\n {df_test_LR.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_LR[\"false_negative\"] = (df_test_LR[\"predicted_value\"] == 0) & (df_test_LR[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n \\n {df_test_LR.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_LR = df_test_LR.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_LR = df_test_LR.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_LR = (predicted_positives_per_group_LR / per_group_total_LR) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_LR}%\")\n\n\n\nAccuracy By Group: \n \n group\n1    84.209040\n2    81.011296\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    83.887734\n2    75.639764\ndtype: float64\n\nFPR By Group: \n \n group\n1     8.757062\n2    13.313609\nName: false_positive, dtype: float64\n\nFNR By Group: \n \n group\n1    7.033898\n2    5.675094\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    54.350282\n2    54.653039\nName: predicted_value, dtype: float64%\n\n\n\nDiscussion:\nBy implementing the PolynomialFeatures preprocessing function/module, I was able to add polynomial feature adjustments directly into the model pipeline. By calculating all polynomial combinations of my features with a degree of 2, I achieved my best accuracy of 82.57%."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#svc",
    "href": "posts/Auditing Bias/index.html#svc",
    "title": "Auditing Bias",
    "section": "SVC",
    "text": "SVC\n\n# SVC Classifier\nmodel_sex_SVC = make_pipeline(StandardScaler(), SVC(C = 3.0))\nmodel_sex_SVC.fit(X_train, y_train)\n\ny_hat_SVC = model_sex_SVC.predict(X_test)\n\n# Calculate Overall Values\nTP_SVC = ((y_hat_SVC == 1) & (y_test == 1)).sum()\nFP_SVC = ((y_hat_SVC == 1) & (y_test == 0)).sum()\nTN_SVC = ((y_hat_SVC == 0) & (y_test == 0)).sum()\nFN_SVC = ((y_hat_SVC == 0) & (y_test == 1)).sum()\n\nPPV_SVC = TP_SVC / (y_hat_SVC == 1).sum()\nFPR_SVC = FP_SVC / (TP_SVC + FP_SVC)\nFNR_SVC = FN_SVC / (TP_SVC + FN_SVC)\n\nprint(f\"Accuracy for SVC:{((y_hat_SVC == y_test).mean())*100: .2f}%\")\nprint(f\"PPV for SVC:{PPV_SVC*100: .2f}%\")\nprint(f\"Overall FPR for SVC:{FPR_SVC*100: .2f}%\")\nprint(f\"Overall FNR for SVC:{FNR_SVC*100: .2f}%\")\n\nAccuracy for SVC: 82.41%\nPPV for SVC: 78.79%\nOverall FPR for SVC: 21.21%\nOverall FNR for SVC: 11.55%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_SVC = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_SVC[\"group\"] = group_test\ndf_test_SVC[\"label\"] = y_test\ndf_test_SVC[\"predicted_value\"] = y_hat_SVC\ndf_test_SVC[\"correct_prediction\"] = df_test_SVC[\"predicted_value\"] == df_test_SVC[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_SVC[\"correct_prediction\"] = df_test_SVC[\"predicted_value\"] == df_test_SVC[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n {df_test_SVC.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_SVC[\"true_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 1)\ndf_test_SVC[\"false_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_SVC = df_test_SVC.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_SVC = df_test_SVC.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_SVC / (TP_per_group_SVC + FP_per_group_SVC)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_SVC[\"false_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n {df_test_SVC.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_SVC[\"false_negative\"] = (df_test_SVC[\"predicted_value\"] == 0) & (df_test_SVC[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n {df_test_SVC.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_SVC = df_test_SVC.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_SVC = df_test_SVC.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_SVC = (predicted_positives_per_group_SVC / per_group_total_SVC) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_SVC}\")\n\n\nAccuracy By Group: \n group\n1    84.406780\n2    80.500269\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    83.324860\n2    74.508864\ndtype: float64\n\nFPR By Group: \n group\n1     9.265537\n2    14.308768\nName: false_positive, dtype: float64\n\nFNR By Group: \n group\n1    6.327684\n2    5.190963\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    55.564972\n2    56.132329\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nFrom what I could find on sklearn and online (this Medium article: https://medium.com/@myselfaman12345/c-and-gamma-in-svm-e6cee48626be) C - to put it generally - indicates how much we want to avoid misclassification on our training data. A low C value leads to low training error, and high C allows for more error on training data. However, minimizing C - and therefore training classification error - too far seems to lead to over fitting. As I dropped the C value below 1.0 (the default sklearn value) I found my classification accuracy begin to drop. Setting it too high led to the same effect. It seems that the “optimal” C value is context-dependent. Interestingly, my accuracy was very similary at a C value of 0.8 (82.35%) and 3.0 (82.41%). The default C value of 1.0 resulted in an 83.32% accuracy.\nExtract predicitions on all test sets modeled with each classifier"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#decision-tree",
    "href": "posts/Auditing Bias/index.html#decision-tree",
    "title": "Auditing Bias",
    "section": "Decision Tree",
    "text": "Decision Tree\n\n# Decision Tree Classifier\nmodel_sex_DT = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=12, min_samples_split=8))\nmodel_sex_DT.fit(X_train, y_train)\n\ny_hat_DT = model_sex_DT.predict(X_test)\n\n# Calculate Overall Values\nTP_DT = ((y_hat_DT == 1) & (y_test == 1)).sum()\nFP_DT = ((y_hat_DT == 1) & (y_test == 0)).sum()\nTN_DT = ((y_hat_DT == 0) & (y_test == 0)).sum()\nFN_DT = ((y_hat_DT == 0) & (y_test == 1)).sum()\n\nPPV_DT = TP_DT / (y_hat_DT == 1).sum()\nFPR_DT = FP_DT / (TP_DT + FP_DT)\nFNR_DT = FN_DT / (TP_DT + FN_DT)\n\nprint(f\"Accuracy for Decision Tree:{((y_hat_DT == y_test).mean())*100: .2f}%\")\n# No Parameters: 77.28%, With Parameters: 83.12% \nprint(f\"PPV for Decision Tree:{PPV_DT*100: .2f}%\")\nprint(f\"Overall FPR for Decision Tree:{FPR_DT*100: .2f}%\")\nprint(f\"Overall FNR for Decision Tree:{FNR_DT*100: .2f}%\")\n\nAccuracy for Decision Tree: 83.11%\nPPV for Decision Tree: 80.41%\nOverall FPR for Decision Tree: 19.59%\nOverall FNR for Decision Tree: 12.68%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_DT = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_DT[\"group\"] = group_test\ndf_test_DT[\"label\"] = y_test\ndf_test_DT[\"predicted_value\"] = y_hat_DT\ndf_test_DT[\"correct_prediction\"] = df_test_DT[\"predicted_value\"] == df_test_DT[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_DT[\"correct_prediction\"] = df_test_DT[\"predicted_value\"] == df_test_DT[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n {df_test_DT.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_DT[\"true_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 1)\ndf_test_DT[\"false_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_DT = df_test_DT.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_DT = df_test_DT.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_DT / (TP_per_group_DT + FP_per_group_DT)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_DT[\"false_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n {df_test_DT.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_DT[\"false_negative\"] = (df_test_DT[\"predicted_value\"] == 0) & (df_test_DT[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n {df_test_DT.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_DT = df_test_DT.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_DT = df_test_DT.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_DT = (predicted_positives_per_group_DT / per_group_total_DT) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_DT}\")\n\n\nAccuracy By Group: \n group\n1    85.084746\n2    81.226466\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    85.039370\n2    76.041667\ndtype: float64\n\nFPR By Group: \n group\n1     8.050847\n2    12.990855\nName: false_positive, dtype: float64\n\nFNR By Group: \n group\n1    6.864407\n2    5.782679\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    53.813559\n2    54.222700\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nThe Decision Tree Classifier achieved it’s highest accuracy of 83.12% with a max-depth of 12 and a min_samples_split - the minimum number of samples required to split an internal node - of 8. This classifier also ran the fasted of all the classifiers tested with parameters."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#random-forest",
    "href": "posts/Auditing Bias/index.html#random-forest",
    "title": "Auditing Bias",
    "section": "Random Forest",
    "text": "Random Forest\n\n# Random Forest Classifier\nmodel_sex_RF = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=400, max_depth=16))\nmodel_sex_RF.fit(X_train, y_train)\n\ny_hat_RF = model_sex_RF.predict(X_test)\n\n# Calculate Overall Values\nTP_RF = ((y_hat_RF == 1) & (y_test == 1)).sum()\nFP_RF = ((y_hat_RF == 1) & (y_test == 0)).sum()\nTN_RF = ((y_hat_RF == 0) & (y_test == 0)).sum()\nFN_RF = ((y_hat_RF == 0) & (y_test == 1)).sum()\n\nPPV_RF = TP_RF / (y_hat_RF == 1).sum()\nFPR_RF = FP_RF / (TP_RF + FP_RF)\nFNR_RF = FN_RF / (TP_RF + FN_RF)\n\nprint(f\"Accuracy for Random Forest:{((y_hat_RF == y_test).mean())*100: .2f}%\")\n# Without Parameters: 80.92%, With Parameters: 83.74%\nprint(f\"Overall PPV for Random Forest:{PPV_RF*100: .2f}%\")\nprint(f\"Overall FPR for Random Forest:{FPR_RF*100: .2f}%\")\nprint(f\"Overall FNR for Random Forest:{FNR_RF*100: .2f}%\")\n\n\nAccuracy for Random Forest: 83.70%\nOverall PPV for Random Forest: 80.81%\nOverall FPR for Random Forest: 19.19%\nOverall FNR for Random Forest: 11.82%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_RF = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_RF[\"group\"] = group_test\ndf_test_RF[\"label\"] = y_test\ndf_test_RF[\"predicted_value\"] = y_hat_RF\ndf_test_RF[\"correct_prediction\"] = df_test_RF[\"predicted_value\"] == df_test_RF[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_RF[\"correct_prediction\"] = df_test_RF[\"predicted_value\"] == df_test_RF[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n \\n {df_test_RF.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_RF[\"true_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_RF = df_test_RF.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_RF = df_test_RF.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_RF / (TP_per_group_RF + FP_per_group_RF)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n \\n {df_test_RF.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_RF[\"false_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n \\n {df_test_RF.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_RF = df_test_RF.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_RF = df_test_RF.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_RF = (predicted_positives_per_group_RF / per_group_total_RF) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_RF}\")\n\n\nAccuracy By Group: \n \n group\n1    85.706215\n2    81.791286\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    85.356957\n2    76.496784\ndtype: float64\n\nFPR By Group: \n \n group\n1     7.937853\n2    12.775686\nName: false_positive, dtype: float64\n\nFNR By Group: \n \n group\n1    6.355932\n2    5.433029\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    54.209040\n2    54.357181\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nWhile increasing max_depth to 16 itself led to the largest increase in classifier accuracy, adding more estimators - n_estimators set to 400 - to the model led to a further increase in accuracy from 83.55% to 83.74%. Random Forest achieved the highest prediction accuracy of all models tested."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#bias-measures",
    "href": "posts/Auditing Bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures:",
    "text": "Bias Measures:\n\nA model is consider well calibrated if it reflects the same likelihood of positive prediction irrespective an individuals’ group membership. In other words, the model should be free from predictive bias. It appears that none of the models are well-calibrated as across all of them, the positive prediction rate of male employment is several percentage points higher than female employment. More specifically, the PPV ranges from anywhere between ~8%-9% across all of the models.\nAcross the board, again, the models do not satisfy error rate balance. The False Positive Rate (FPR) is higher for females across all four, whereas the False Negative Rate (FNR) is higher for males across all four models. Even further, the margin of difference between groups for FPR is much greater than that for FNR across all Models. The FPR difference between groups is at its lowest ~4.56% in our LR model, whereas the largest difference in FNRs is ~1.36%, also found in our LR model.\nThe best performing fairness metric across our models is statistical parity, with no models differing more than .7% in acceptance rate (i.e. employment prediction) between male and females."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#plotting-feasible-fnr-and-fpr-rates",
    "href": "posts/Auditing Bias/index.html#plotting-feasible-fnr-and-fpr-rates",
    "title": "Auditing Bias",
    "section": "Plotting Feasible FNR and FPR Rates",
    "text": "Plotting Feasible FNR and FPR Rates\nUsing Random Forest Model because it had the best accuracy per group\n\nfrom matplotlib import pyplot as plt\n\nprevalence = df_test_RF.groupby(\"group\")[\"label\"].mean()\np_male = prevalence.loc[1]\np_female = prevalence.loc[2]\n\n# Recalculating FNR here to make continuity more clear/avoid using variables from other cells\n\n# Calculate true positives and false positives by group\ndf_test_RF[\"true_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0)\ndf_test_RF[\"false_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"true_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_RF = df_test_RF.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_RF = df_test_RF.groupby(\"group\")[\"false_positive\"].sum()\nFN_per_group_RF = df_test_RF.groupby(\"group\")[\"false_negative\"].sum()\nTN_per_group_RF = df_test_RF.groupby(\"group\")[\"true_negative\"].sum()\n\nPPV_per_group = TP_per_group_RF / (TP_per_group_RF + FP_per_group_RF)\nPPV_male = PPV_per_group.loc[1]\nPPV_female = PPV_per_group.loc[2]\n\n# Calculate FNR\nFNR = FN_per_group_RF / (FN_per_group_RF + TP_per_group_RF)\nFNR_male = FNR.loc[1]\nFNR_female = FNR.loc[2]\n\n# Calculate FPR\nFPR_male = (p_male / (1 - p_male)) * ((1 - PPV_male) / PPV_male) * (1 - FNR_male)\nFPR_female = (p_female / (1 - p_female)) * ((1 - PPV_female) / PPV_female) * (1 - FNR_female)\n\nfeasible_FNR_range = np.linspace(0, 1, 100)\n\nFPR_male_line = (p_male / (1 - p_male)) * ((1 - PPV_female) / PPV_female) * (1 - feasible_FNR_range)\nFPR_female_line = (p_female / (1 - p_female)) * ((1 - PPV_female) / PPV_female) * (1 - feasible_FNR_range)\n\nplt.figure(figsize=(8, 5))\n\nplt.plot(FNR_male, FPR_male, 'o', color='orange', label='Male')\nplt.plot(FNR_female, FPR_female, 'o', color='black', label='Female')\n\nplt.plot(feasible_FNR_range, FPR_male_line, '-', color='orange')\nplt.plot(feasible_FNR_range, FPR_female_line, '-', color='black')\n\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible FNRs and FPRs\")\nplt.legend()\n\n0.8535695674830641\n0.7649678377041069\n\n\n\n\n\n\n\n\n\nTuning the classifier threshold: Looking at the feasibility curves for males and females, if we set the threshold so that both groups have an FPR of 0.15, the male curve corresponds to an FNR of 0.25, while the female curve corresponds to an FNR of 0.40. Thus, to achieve the same FPR across groups, we must allow the female group’s FNR to increase from 0.25 to 0.40, a difference of 15%."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#concluding-discussion",
    "href": "posts/Auditing Bias/index.html#concluding-discussion",
    "title": "Auditing Bias",
    "section": "Concluding Discussion:",
    "text": "Concluding Discussion:\nWhat groups of people could stand to benefit?\nThis model predicts whether an individual is employed, so it could benefit recruiting agencies or HR departments seeking to identify jobseekers or allocate training resources - specifically understanding what feature groups may be searching for a job/have more possible unemployed candidates available to interview.\nImpact of deploying model?\nFrom the bias audit, we see differences in false positive and false negative rates across groups. If deployed widely, these disparities could systematically disadvantage certain populations—for instance, if a higher false negative rate leads to fewer recognized as “employed,” those individuals might miss job opportunities or loans. Conversely, higher false positives could unfairly label individuals as employed when they are not, whereas false negatives could mean that hiring/search resources are allocated to the wrong demographic areas, disadvantaging those who don’t fit the template of an employed/unemployed individual. Even more, the differences in the bias audit reveal the way in which employment data is a proxy for historical prejudice - and in my case, specifically gender-based hiring prejudice - which could lead to reinforced stereotypes of who is “employable” and who is not.\nDoes the Model Display Bias?\nFrom the bias audit (examining accuracy, PPV, FNR, and FPR by group), we see that the model’s performance differs between males and females for several fairness criterion. For example, it appears that none of the models are well-calibrated as across all we see the positive prediction rate of male employment is several percentage points higher than female employment. More specifically, the PPV ranges from anywhere between ~8%-9% across all of the models. There is also error rate imbalance: the False Positive Rate (FPR) is higher for females across all four, whereas the False Negative Rate (FNR) is higher for males across all four models. Even further, the margin of difference between groups for FPR is much greater than that for FNR across all Models. The FPR difference between groups is at its lowest ~4.56% in our LR model, whereas the largest difference in FNRs is ~1.36%, also found in our LR model. This could lead to prejudicial assumptions which have harmful effects on fair hiring practices. For instance, if one group has a statistically higher false negative rate, that group may be systematically overlooked for certain opportunities.\nFurther Concerns?\nThe model analyzed here is quite complex. Specifically, it involves the correlation/scoring of a large set of features, and classifier/modeling methods that are not very accessible. In other words, it isn’t a very transparent model, and that can lead to a lot of mistrust about how the decisions are being made and why. I also want to know more about where the data is being collected in a state, what areas/communities it is taken from, and how representative or random are those samples. To address these issues, we could continue ollect more diverse data, and promote education and transparency initiatives about Machine Learning. Furthermore, it is always important to include human oversight as a “second set of eyes” in decisions made about another person’s life from data."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Introduction:",
    "text": "Introduction:\nThe purpose of this blog post is to explore the quantitative and ethical depths of decision theory in classification. The specific goal of the assignment was to try and identify a scoring function and threshold which would optimize expected profit per borrower for a bank who is deciding who to give loans to, using any of the features contained in our data set and any method. I started by exploring the data visually, trying to understand how different feature columns related to each other, and how bivariate feature combinations may correlate to loan status - i.e. whether a borrower repaid their loan or defaulted. After this, I fit a logistic regression classification onto the data set using varying feature combinations to see if there were any particular combinations which optimized initial classification accuracy. Once a model was fit, I used the model weights to calculate linear risk scores for each of the data entries/borrowers, and used these scores to identify a threshold risk score (borrowers lower than the threshold were approved, and above the threshold denied) which maximized profit per borrower for the bank. I found that the optimal threshold score was 1.414, which produced a profit per borrower of $1391.53 on the training data. Using this score as a threshold on the test data, I identified a maximized profit of $1102.18 per borrower. While this was lower than the training value, that is expected. However, upon processing through the test data to find its specific optimizing threshold, I found that my proposed threshold of 1.414 was not maximizing profit to its full potential on the test data. Instead, maximum profits of $1347.19 was found at a threshold of 1.010. So, while the proposed system kept profit high, it did not achieve its highest potential on the test data.\n\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 1: Explore the Data",
    "text": "Part 1: Explore the Data\nRemember, 1 means a person defaulted on loan, and 0 means they repaid in full\n\n# Import seaborn, our visualization library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a copy of our dataset, as since we use df_train later in this blogpost we want to prevent\n# any errors from being passed on to these visualizations due to the scope of df_train\nfor_viz = df_train.copy()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#exploring-borrower-features-with-facetgrid",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#exploring-borrower-features-with-facetgrid",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Exploring Borrower Features with FacetGrid",
    "text": "Exploring Borrower Features with FacetGrid\nUsing Seaborn’s FacetGrid, I plot the distribution of borrower characteristics across different loan intents. This allows us to compare how features like age, employment length, and home ownership vary by loan purpose.\n\nfeatures = [\"person_age\", \"person_emp_length\", \"person_home_ownership\"]\n\n\nfor feature in features:\n    g = sns.FacetGrid(data=for_viz, col=\"loan_intent\", hue=\"loan_intent\", col_wrap=3)\n    g.map(sns.histplot, feature, bins=20, alpha=0.7)\n    g.set_titles(col_template=\"{col_name} loans\")\n    g.figure.subplots_adjust(top=0.85)\n    g.figure.suptitle(f\"Distribution of {feature.replace('_', ' ').title()} by Loan Intent\", fontsize=14)\n    g.add_legend()\n    plt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 1:",
    "text": "Figure 1:\nFirst, we want to understand how loan intent varies with different features, specifically borrower age, length of employment, and homeownership status. Not only with this offer us qualitative insights, but also help us understand the distribution of borrowers we are encountering, and what patterns may emerge in the data.\n\nsns.displot(data = for_viz, x = \"person_age\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe are dealing with, for the most part, younger borrowers. The vast majority of borrowers across intents are 40 or younger, and above 40 years of age we see a steep drop off in borrowers. We see an especially high number of young borrowers in the data set seeking a loan for education."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 2:",
    "text": "Figure 2:\n\nsns.displot(data = for_viz, x = \"person_emp_length\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe see similar shapes in these visualizations as in that comparing borrower age and loan intent, but the context of course gives it a different meaning. Across the board, we see that the majority of borrowers have been employed for under 20 years. Again, the highest count of borrowers is found in the subset of those seeking education loans. After 20 years of employment, loan seekers effectively disappear from view. The lowest number of loan seekers are found for home improvement loans."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 3:",
    "text": "Figure 3:\n\nsns.displot(data = for_viz, x = \"person_home_ownership\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nIn nearly all of the visualizations, the highest number of borrowers seeking a loan are also renters. From highest to lowest counts, the borrowers’ home ownership status is RENT, MORTGAGE, and OWN. This makes sense intuitively as home ownership is likely a proxy for financial secure individuals who may be less likely to seek a loan as a form of financial support. The only place we do not see this trend is in the Home Improvement visualization. Here, the plurality of borrowers are mortgaging their home. This also makes sense, however, because those who have mortgages are likely “newer” homeowners who are undergoing more renovations/home improvements than those who have owned a home for a long time."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 4:",
    "text": "Figure 4:\nNext, I want to see the relationship between loan interest rates and loans as a percent of a borrowers income, and how this relationship may shape loan status - i.e. whether a borrower repays or defaults. Additionally, it will be helpful to compare visualization methods to see which Seaborn plots are more effective for such a large dataset\n\nsns.jointplot(for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\", kind = \"kde\", height=6)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\")\n\n\n\n\n\n\n\n\n\nDiscussion:\nAs seen above, using a scatterplot to visualize such a large set of data points is not a very effective approach. There is a great deal of overlap across our points, meaning that classification trends may be getting lost behind other data points. When looking at the joint plot, we first note from the edge plots that there are far more repaying than defaulting borrowers in our data set. Additionally, a higher distribution of borrowers repay loans they have a low interest rate, and their loan is a lower percent of their income. However, the reality remains that - based on the inner JointGrid - that there is still a great deal of overlap between borrowers of both loan status in relation to these two variables. Feature selection could not be done here purely through visualization."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 2: Building a Model",
    "text": "Part 2: Building a Model\n\n# First, establish the target column. This won't change so I initialize it here.\nTARGET_COL = \"loan_status\"\n\n# Even before processing the data, we perform some preliminary changes to the DataFrame to ensure\n# future processing does not interfere with indexing, such as dropping entries with empty values,\n# and converting qualitative features to one-hot encoded dummies\ndf_train = pd.get_dummies(df_train)\ndf_train = df_train.dropna()\n\n\n# Create a preprocessing function that will scale data and \n# create an X_train and y_train based on chosen feature columns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\ndef preprocess(df, quant_cols, qual_cols, target_col):\n\n    df_new = pd.DataFrame()\n    df_new[quant_cols] = df[quant_cols]\n    df_new[qual_cols] = df[qual_cols]\n    df_new_target = df[target_col]\n\n    scaler = StandardScaler()\n    \n    df_new[quant_cols] = scaler.fit_transform(df_new[quant_cols])\n    df_new[qual_cols] = df_new[qual_cols]\n    \n    return df_new, df_new_target\n\n\nExperiment with features/models\n\n# Model imports\nfrom sklearn.linear_model import LogisticRegression\n\n\n# First, I will test a model on all quantitative/qualitative features in the dataset\n\nX1_quant_cols = [\"person_age\", \n                    \"person_income\", \n                    \"person_emp_length\",\n                    \"loan_int_rate\", \n                    \"loan_amnt\", \n                    \"loan_percent_income\",\n                    \"cb_person_default_on_file_N\",\n                    \"cb_person_default_on_file_Y\",\n                    \"cb_person_cred_hist_length\"]\n\nX1_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\",\n                    \"loan_intent_VENTURE\",\n                    \"loan_intent_EDUCATION\",\n                    \"loan_intent_MEDICAL\",\n                    \"loan_intent_HOMEIMPROVEMENT\",\n                    \"loan_intent_PERSONAL\",\n                    \"loan_intent_DEBTCONSOLIDATION\", ]\n\n\nX1_train, y_train = preprocess(df_train, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\nLR1 = LogisticRegression()\nfit1 = LR1.fit(X1_train, y_train)\nLR1.score(X1_train, y_train)\n\n0.8498712184048544\n\n\n\n# Having fit and scored a model on all feature columns, we now explore smaller feature combinations,\n# looking to see if any particular combination maximizes classification accuracy\nX2_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"person_emp_length\"]\n\nX2_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX2_train, y_train = preprocess(df_train, X2_quant_cols, X2_qual_cols, TARGET_COL)\n\nLR2 = LogisticRegression()\nfit2 = LR2.fit(X2_train, y_train)\nLR2.score(X2_train, y_train)\n\n0.8468153839437726\n\n\n\n# Similar to above block but for alternative feature combination\n\nX3_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"loan_amnt\"]\n\nX3_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX3_train, y_train = preprocess(df_train, X3_quant_cols, X3_qual_cols, TARGET_COL)\n\nLR3 = LogisticRegression()\nfit3 = LR3.fit(X3_train, y_train)\nLR3.score(X3_train, y_train)\n\n0.844632645043\n\n\n\n# Now that we have fit a Logistic Regression model on our data, we have access to the\n# Model weights via the .coef_ attribute.\n# However, we to transform this array into a column because otherwise it is just a row,\n# complicating necessary computations in the future\nprint(fit1.coef_.T)\n\n[[-0.0423979 ]\n [ 0.04612073]\n [-0.02431996]\n [ 1.03609245]\n [-0.57955282]\n [ 1.3239725 ]\n [-0.0143236 ]\n [ 0.0143236 ]\n [-0.00851694]\n [-0.27565869]\n [ 0.46232549]\n [-1.75917877]\n [-0.87364387]\n [-0.6441703 ]\n [-0.02154856]\n [ 0.24354326]\n [-0.46649075]\n [ 0.15978062]]"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 3: Find a Threshold",
    "text": "Part 3: Find a Threshold\n\n# Define a function that calculates the linear scores of our model\n# By calculating the cross product between our predictors and model weights\ndef linear_score(X, w):\n    return X@w.T\n\nX1_scores = linear_score(X1_train, fit1.coef_)\nX1_scores = X1_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX2_scores = linear_score(X2_train, fit2.coef_)\nX2_scores = X2_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX3_scores = linear_score(X3_train, fit3.coef_)\nX3_scores = X3_scores.iloc[:, 0] # All rows and first column to convert to Series\n\n\n# Plot the scores of our linear score function to see score distribution\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(X3_scores, bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\n\n# Now that we know our score distributions, we can select a range of possible thresholds\n# and define a function to identify the threshold which maximizes profit\ndef calculate_repaid_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"]) / 100) ** 10) - df[\"loan_amnt\"])\n\ndef calculate_default_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"] / 100)) ** 3) - (1.7 * df[\"loan_amnt\"]))\n\ndef plot_best_threshold(df, scores, y_train):\n    best_profit = -1000000000\n    best_threshold = 0\n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4)) \n    for t in np.linspace(0, 10, 100):\n        y_pred = scores &gt; t\n        tp = (y_pred == 0) & (y_train == 0)\n        # tp.sum()\n        fp = (y_pred == 0) & (y_train == 1)\n        # fp.sum()\n        profit = calculate_repaid_profit(df[tp]).sum() + calculate_default_profit(df[fp]).sum()\n        profit /= len(df)\n        ax.scatter(t, profit, color = \"steelblue\", s = 10)\n        if profit &gt; best_profit: \n            best_profit = profit\n            best_threshold = t\n\n\n    ax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\n    labs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net Profit\", title = f\"Best Profit Per Borrower ${best_profit:.2f} at best threshold t = {best_threshold:.3f}\")\n\nplot_best_threshold(df_train, X3_scores, y_train)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Evaluate My Model from the Bank’s Perspective",
    "text": "Evaluate My Model from the Bank’s Perspective\n\nOPTIMAL_THRESHOLD = 1.414\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test = pd.get_dummies(df_test)\ndf_test = df_test.dropna()\nX_test, y_test = preprocess(df_test, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\ntest_scores = linear_score(X_test, fit1.coef_)\ntest_scores = test_scores.iloc[:, 0]\n\ny_pred = test_scores &gt; OPTIMAL_THRESHOLD\ntest_tp = (y_pred == 0) & (y_test == 0)\ntest_tn = (y_pred == 1) & (y_test == 1)\ntest_profit = calculate_repaid_profit(df_test[test_tp]).sum() + calculate_default_profit(df_test[test_tn]).sum()\ntest_profit /= len(df_test)\nprint(f\"Optimal Profit is ${test_profit:.2f}\")\nplot_best_threshold(df_test, test_scores, y_test)\n\nOptimal Profit is $1102.18\n\n\n\n\n\n\n\n\n\n\ndf_test[\"risk_score\"] = test_scores\nage_risk_score = df_test.groupby(\"person_age\")[[\"person_age\", \"risk_score\"]].mean()\n\n\nsns.scatterplot(df_test, x = \"person_age\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_age_approved = approved[\"person_age\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_age_denied = denied[\"person_age\"].mean()\nprint(f\" Average age of approved borrower: {avg_age_approved} vs. Average age of denied borrower: {avg_age_denied}\")\n\n Average age of approved borrower: 27.87536718422157 vs. Average age of denied borrower: 26.935751295336786\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the average age of borrowers approved with our threshold is less than 1 year older than the average age of denied borrowers. As such, it appears that the people of varying age groups have a similar degree of access to credit under my proposed system.\n\ncount_medical_total =  df_test[df_test[\"loan_intent_MEDICAL\"]]\ncount_medical_approved = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_medical_default = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\n\nprint(f\"Percentage of medical seeking borrowers which were approved: {(len(count_medical_approved)/len(count_medical_total))*100:.2f}%\")\nprint(f\"Percentage of approved medical seeking borrowers who defaulted: {(len(count_medical_default)/len(count_medical_approved))*100:.2f}%\")\n\n\nPercentage of medical seeking borrowers which were approved: 77.54%\nPercentage of approved medical seeking borrowers who defaulted: 15.02%\n\n\n\ncount_venture_total =  df_test[df_test[\"loan_intent_VENTURE\"]]\ncount_venture_approved = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_venture_default = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of venture seeking borrowers which were approved: {(len(count_venture_approved)/len(count_venture_total))*100:.2f}%\")\nprint(f\"Percentage of approved venture seeking borrowers who defaulted: {(len(count_venture_default)/len(count_venture_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 89.94%\nPercentage of approved medical seeking borrowers who defaulted: 7.50%\n\n\n\ncount_education_total =  df_test[df_test[\"loan_intent_EDUCATION\"]]\ncount_education_approved = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_education_default = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of education seeking borrowers which were approved: {(len(count_education_approved)/len(count_education_total))*100:.2f}%\")\nprint(f\"Percentage of approved education seeking borrowers who defaulted: {(len(count_education_default)/len(count_education_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 88.78%\nPercentage of approved medical seeking borrowers who defaulted: 10.73%\n\n\n\n\nAnalysis:\nBased on the above data, borrowers seeking a loan for medical reasons have a more difficult time accessing credit under my proposed system. Only 77.54% of borrowers from the test set who were seeking medical loans were approved compared to venture at 89.94% and education borrowers at 88.78%. However, at the same time, the rate of default was higher amongst medical borrowers - 15.02% - compared to venture - 7.50% - and education - 10.73%.\n\nsns.scatterplot(df_test, x = \"person_income\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_income_approved = approved[\"person_income\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_income_denied = denied[\"person_income\"].mean()\nprint(f\" Average income of approved borrower: {avg_income_approved} vs. Average income of denied borrower: {avg_income_denied}\")\n\n Average income of approved borrower: 71671.01657574486 vs. Average income of denied borrower: 41555.21658031088\n\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the income of borrowers approved with our threshold is notably higher - ~$30,000 higher - than the income of denied borrowers. As such, it appears that the people of lower income groups may have a more difficult time accessing credit under this system."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nIn conclusion, this post examined decision theory in classification through both quantitative and ethical lenses, aiming to optimize a bank’s expected profit per borrower. The analysis began with visual exploration of the data, followed by fitting logistic regression models using various feature combinations. By computing linear risk scores, an initial optimal threshold of 1.414 was identified — yielding $1391.53 per borrower on the training data and $1102.18 on the test data. However, further analysis of the test set revealed an optimal threshold of 1.010, which maximized profit at $1347.19 per borrower, indicating that while the system maintained high profitability, it did not capture the test data’s full profit potential.\nAt the end of the study, I explored the fairness of this model. Fairness in general is a difficult concept to define, but is complicated to a high degree in the context of machine learning models which make predictions about people. A machine learning model can never achieve full fairness - making a decision about an outcome only after considering not only all features/traits of a person relevant to that outcome, but also the context-specificity of those features - as ML is fundamentally built on generalization from examples. So, instead the question of fairness in this post should not be understood as an absolute, but relative to the context at hand: is the model fair enough? Also, the notion of fairness depends on the position of the evaluator - what fairness means to me fundamentally as a person is different to someone whose primary goal is to develop a system which maximizes profits, and therefore makes decisions about borrowers with historical default rates in mind.\nAfter analyzing the outcome data, I noticed that the percent of approved borrowers seeking a medical loan was over 10% lower than that of education/venture seeking borrowers. However, at the same time, medical borrowers had a default rate which was 5% higher than the other two loan seeking groups. From this, I am inclined to ask whether, considering their higher rate of default, it is fair for medical borrowers to have a harder time accessing credit? On one hand, I want to say this is not fair, as even though the borrowers have a history of higher defaults, the value/importance of a medical loan - to potentially save ones life - is much higher than that of an education or venture loan seeker. But at the same time, I wonder whether a bank-loaning model is realistically possible or responsible to include this value in an automated decision making model: can a profit maximizing model actually objectively quantify the ethical/lived importance of a loan to a borrower? Ultimately, these observations highlight the inherent tension between profit optimization and equitable access to credit. While the model accurately identifies risk patterns based on historical data, it falls short in capturing broader social values, particularly for medical needs where the stakes may be much higher. This raises critical ethical questions: should decision systems rely solely on past performance, or must they also adjust to recognize the importance of timely, life-saving credit?"
  },
  {
    "objectID": "posts/Limitations of the Quantitative Approach to Bias and Fairness/index.html",
    "href": "posts/Limitations of the Quantitative Approach to Bias and Fairness/index.html",
    "title": "Limitations of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In his 2022 lecture, The Limits of the Quantitative Approach to Discrimination, Arvind Narayanan puts forth a provocative claim on the role of quantitative methods in the contemporary age: “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan 2022). However, this claim can only be fully appreciated in the broader context of his speech, including not only its evidence and subclaims, but also Narayanan’s own positionality. As Narayanan explains in his speech, the harmful effects of Quantitative Methods seen today are not due to any inherent flaw, but instead because we have misunderstood the role and value of numbers in human society. As a quantitative scholar himself, Narayanan can provide a distinct level of specificity to this claim – of insider insight – which helps illuminate the tangible realities which beg us to take seriously that – as simple as it may sound – numbers actually can lie.\nNarayanan highlights 7 key limitations of quantitative methods which he has observed in his time as a quantitative scientist: Choice of null hypothesis, Use of snapshot datasets, Use of data provided by company, Explaining away discrimination, Wrong locus of intervention, The objectivity illusion, and Performativity.\nThe objectivity illusion is the quantitative limitation which, in a way, is at the heart of all other limitations put forth in Narayanan’s essay. As such, it is at the heart of the logic which my essay puts forth. Acknowledging it necessarily entails complications related to snapshot data, explaining away discrimination, and even the performative policies which quantitative science has brought about. The objectivity illusion refers to the false belief that quantitative research is objective because numbers are objective. Quantitative research involves subjective decisions made by researchers at multiple steps throughout the process such as – as mentioned above – choice of dataset or data collection methods, how to frame research questions and findings, which variables to control and to test on, and what “counts” as statistical evidence of discrimination: “In a typical research paper I would say researchers make at least 10-20 subjective choices, each of which could substantially alter their conclusions” (Narayanan 2022). The objectivity illusion is also at the heart of the previously mentioned limitations of snapshot data sets. As D’ignazio and Klein explain, “data are not neutral or objective. They are products of unequal social relations, and this context is essential for conducting accurate, ethical analysis” (See Introduction in D’ignazio and Klein 2023). Contemporary theories in the humanities, while not directly engaging questions of the quantitative world, offer language which is helpful to clarify the objectivity illusion.\nIn his book Crossing and Dwellings, for example, Thomas Tweed draws from insights of his scholarship to encourage a dynamic and itinerant approach to religious theory. The reason I bring such a seemingly distant resource to this conversation is, well, because it may not be as distant as we think. In fact, the “meta-theory” of theory which Tweed puts forth is quite similar to the “adversarial collaboration” approach of interpreting data which Narayanan mentions as “admirable”, in that it “explicitly acknowledges that scientists have their biases, their pet theories, their favored and hoped for conclusions” which will ultimately bias – in conscious or unconscious ways – the methodological approach they bring to their research (Narayanan 2022). As Tweed writes, “it is precisely because we stand in a particular place that we are able to see, to know, to narrate” (Tweed 2008, 18). There is no independent access to the experiences or subjective states of another, only that which the scholar can ‘see’ – used by Tweed to represent the multi-sensory reception of the corporeal world before us (Tweed 17). And, it is from those sightings that a scholar is able to construct meaning using the “categories and criteria they inherit, revise, and create” (Tweed 2008, 18). In the quantitative world, such sightings are at particular risk of being incomplete not because it is inherently worse, but because it is just simply not possible yet – whether by nature or by the limitations of modern technology – to capture the depth and variety of human experience with numbers. It is in recognizing this reality that the first steps may be taken towards deconstructing the epistemic hierarchy which grips the quantitative world.\nIt is ultimately from the objectivity illusion that other limitations emerge. For example, take the Choice of null hypothesis. Quantitative communities have “settled on the absence of discrimination” as the default assumption. Choosing to see the world as impartial places the burden of proof on those alleging discrimination, and naively pretends that quantitative scholars – or really that anyone – has the capacity to live in a world void of human subjectivity. Such a choice not only ignores, but may even perpetuate discrimination as it brushes aside the possibility of systemic discrimination – which manifests in implicit, everyday forms and compound over time – and holds us back from implementing the critical interventions which may help address discriminatory realities. To standardize the absence of discrimination holds a massive normative significance in that it encourages us to be blind to inequality unless it presents itself clearly and ‘objectively’, and even then, retain a level of skepticism towards it until it can be quantitatively confirmed. Narayanan’s observations about the choice and the null hypothesis point us to consider the principles upon which we build quantitative methods, and particularly how quantitative methods and researchers operate in relation to power. Quantitative scholars who follow the framework of Data Feminism, for example, would argue that such considerations should manifest in challenges to unequal power structures and working toward justice (D’ignazio and Klein 2023, Introduction). However, even before such a principle can be acted on, scholars must first take seriously the existence of power structures, and the ways in which the data they evaluate grows within, exists in relation to, and is drawn from those contexts.\nFurthermore, most datasets upon which quantitative methods are employed are collected from a “single system at a single point in time,” and it is even rarer for people to go out and collect new data. Such snapshot datasets are thus only frames in a much larger picture which inevitably involves systemic, social, and structural trends, “forcing the researcher to ignore people’s broader circumstances and the past discrimination they may have experienced” (Narayanan 2022). If we assume that such data can provide sufficient view of discrimination, or a lack thereof, we are encouraged to believe discrimination happens in snapshots – in “discrete moments of time” and quantitatively observable ways – which is simply never how discrimination has operated. Other quantitative scholars affirm this limitation, such as D’ignazio and Klein who argue that “the process of converting life experience into data always necessarily entails a reduction of that experience” (D’ignazio and Klein 2023, Introduction).\nIt is these misapprehensions which ultimately lead to Explaining away discrimination. In evaluating statistical disparities, quantitative researchers often diminish/set aside evidence of discrimination through their choice of analytical variables. Specifically, in trying to make their research logically/interpretatively sound, they control for variables that are tied to social constructs like race, subsequently excluding many of the proximal ways in which discrimination may actually operate. For example, Narayanan cites the 2004 study by Bertrand and Mullainathan which sent fake resumes to employers to test if an applicant’s race had an impact on the likelihood of them getting an interview. The way they implemented this was by creating pairs of resumes which were identical except for the applicant’s name, which would either be “White-sounding” (Emily, Greg, etc.) or “Black-sounding” (Lakisha, Jamal, etc.) (Narayanan 2022). While the study did find that “White names” were 50% more likely to receive a callback, it also limits our understanding of what data may signal someone’s social identity to their name: “Literally every other variable that might signal race or be correlated with race [other than one’s name] is held constant between the two conditions” (Narayanan 2022). Thus, while the study may have revealed discrimination in practice, it may have also “drastically underestimated” the degree of discrimination found in hiring practices (Narayanan 2022).\nA 2016 paper by Mittelstadt et al. further highlights this limitation, asserting that “the outputs of algorithms also require interpretation.” As such, ‘objective’ correlations can come to reflect the interpreter’s “unconscious motivations, particular emotions, deliberate choices, socio-economic determinations, geographic or demographic influences” (Mittelstadt et al. 2016). This point is reiterated in Fairness and Machine Learning, in which Barocas, Hardt, and Narayanan himself discuss the complexities which come with building fair machine learning models. If systematic discrimination persists in the world through subtle and systemic channels, it can’t be wholly identified in data that excludes those channels: “In real datasets, most attributes tend to be proxies for demographic variables,” meaning it is both irresponsible and dangerous to essentialize demographic identity to a single element, such as name signifying race (Barocas, Hardt, and Narayanan 2023, 17).\nHowever, while on one hand arguing about the present harms of quantitative methods, it would be insufficient to characterize Narayanan’s lecture as an exposé. He isn’t just saying what is wrong with this system, but rather presents a call to action and reformed thinking about the quantitative world. Eliminating quantitative methods from the conversation is neither desirable nor attainable, for the limitations which are found today are “not inherent to quantitative work” (Narayanan 2022). A better way of engaging with the quantitative world is possible, and steps made toward it can be seen today. On the surface, the proposition of taking seriously the subjectivity of human experience – of both the researcher and the researched – is not technically sophisticated enough. On the surface, the idea of developing research methods to measure and interpret systemic forms of discrimination – methods which “dig deeper” into the assumptions and politics which inevitably shape our engagement with the world (Narayanan 2022) – may not seem quantitatively plausible. However, it is only once we realize that the epistemic role of quantitative methods in a human world can never be absolute, because it is one of many ways of knowing the world, that we may begin to shift from predicting the world into fully reflecting on, understanding, and progressing from the form it inhabits right now.\nThe 2019 study conducted by Obermeyer et al., Dissecting racial bias in an algorithm used to manage the health of populations, provides a critical example of this digging deeper, and how quantitative methods can be employed to uncover and address systemic biases in algorithmic decision-making processes, specifically in healthcare risk prediction. In this research, the authors examined a commercially deployed prediction algorithm intended to identify patients who could benefit from extra medical support, known as “high-risk care management” programs (HRCMPs) (Obermeyer et al. 2019). By exploring the data produced from one of these algorithms through quantitative methods, Obermeyer and colleagues revealed that the algorithm systematically underestimated the healthcare needs of Black patients. Specifically, they found that Black patients who had identical algorithmic risk scores - i.e. who were considered of equally high medical risk/eligible for HRCMP as their white counterparts - were considerably sicker on average. Specifically, they found that Black patients who were predicted as “very-high-risk” (at the point of being auto-identified from HRCMP enrollment) had 26% more chronic health conditions when compared to their White counterparts. After further searching, the authors identified that the algorithm predicts health care costs rather than illness, targeting patients with high costs as eligible for HRCMPs. However, in doing so the algorithm mistakenly equated lower healthcare expenditures by Black patients with lower healthcare needs, overlooking the structural inequalities that produce disparate costs across demographic groups.\nWhile Black patients may spend less on healthcare than white patients, it is not because they are less sick. Instead, the authors identified that such a disparity arises from “unequal access to care” which has been driven by historical relationships – such as Black patients having reduced trust in the doctors – or socioeconomic barriers – such as access to transportation – which proximally impact the extent to which Black patients engage the health care system (Obermeyer et al. 2019). If these patients are cared for less often, then they will ultimately spend less on health care, contributing to the predictive biases which the study identified.\nThis study provides an insightful application of the notion of calibration as detailed in Fairness and Machine Learning. Calibration, a quantitative fairness criterion, requires that risk scores accurately reflect outcomes equivalently across different demographic groups (Barocas et al. 61-62). In the context of this study, a calibrated model ensures that, conditional on risk score, “predictions do not favor Whites or Blacks anywhere in the risk distribution” (Obermeyer et al. 2019). Technically, the healthcare algorithm satisfied this calibration criterion when assigning risk scores based on healthcare expenditure. However, the study illuminated a crucial limitation: calibration alone does not account for systemic biases embedded within the outcomes themselves - in this case, healthcare costs rather than actual health condition or needs. While calibration seeks to produce proportional consistency, it can perpetuate deep-rooted inequities if the target outcome itself is unjustly biased. Relying on proxies like healthcare spending is problematic as it disproportionately reflects unequal access and historical discrimination rather than genuine patient health needs. In other words, even a technically fair model - one that accurately predicts on healthcare expenditures - can remain unjust if it systematically disadvantages groups facing structural inequality.\nThis study highlights the benefits of employing epistemically humble (to stay in line with Narayanan’s language) quantitative methods in analyses of discrimination. The authors’ evaluation allows them to reveal how a seemingly neutral algorithm actually perpetuated discrimination. By quantifying the precise and various extents of disparity, the researchers make the reality of discrimination tangible, clear, and actionable. Subsequently, their analysis guides readers through possible interventions – such as reformulating the algorithm itself, or changing the data it is ultimately fed – that could substantially mitigate racial disparities, thus promoting equality. Therefore, Obermeyer et al’s study not only underscores the value of quantitative methods in exposing hidden biases, but also emphasizes the need to critically evaluate and choose fairness criteria that align with equity and justice.\nNarayanan’s critique is not an outright dismissal of quantitative methods but rather a call to critically examine the assumptions and limitations that shape their applications. My analysis of Narayanan’s claim, contextualized through his concept of the objectivity illusion, has shown that the limitations he identifies are not inherent to quantitative methods, but arise from a failure to acknowledge the subjective decisions embedded in quantitative research processes. As this essay has argued, understanding that data is the product of contextually embedded relations and realities is key to dismantling the epistemic hierarchy that privileges quantitative approaches as neutral or objective.\n\n\n\n\nReferences\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism. MIT press.\n\n\nMittelstadt, Brent Daniel, Patrick Allo, Mariarosaria Taddeo, Sandra Wachter, and Luciano Floridi. 2016. “The Ethics of Algorithms: Mapping the Debate.” Big Data & Society 3 (2): 2053951716679679.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\n\nTweed, Thomas A. 2008. A Theory of Religion. Cambridge, MA; London, England: Harvard University Press. https://doi.org/doi:10.4159/9780674044517."
  },
  {
    "objectID": "posts/Double Descent/index.html",
    "href": "posts/Double Descent/index.html",
    "title": "Abstract:",
    "section": "",
    "text": "In this blog post, I explore how feature complexity/quantity affects the performance of linear regression models, particularly in the context of overparameterized optimization. I begin the post by analyzing why the standard solution to regression fails at the interpolation threshold - where the number of features begins to exceed number of data observations. I then implemented an overparameterized linear regression model using the Moore-Penrose pseudoinverse to fit data - linked here - and assessed its ability to learn on 1D data and corrupted image data. By performing a sweep of potential feature numbers - generated with random nonlinear feature maps - I generated training and testing error curves which help clarify double descent through visualization. Specifically, my results show that while performance worsens around the interpolation threshold, it quickly improves as feature count increases beyond data observations, with my lowest testing error occurring with 200 features on a synthetic dataset with 100 observations."
  },
  {
    "objectID": "posts/Double Descent/index.html#part-0",
    "href": "posts/Double Descent/index.html#part-0",
    "title": "Abstract:",
    "section": "Part 0",
    "text": "Part 0\nIn the provided equation for finding the optimal weight vector in unregularized least-squares linear regression, problems arise when p &gt; n because it disrupts the invertability of the matrix X.\nThis specific element of our equation - (XTX)-1 - only works if… well… XTX is invertible. However, when p &gt; n, the operation XTX produces a matrix which is - according to chapter 11 of our lectures - singular, meaning the columns of the matrix are linearly dependant. In order for a matrix to be invertible, its columns must be linearly independent. If XTX, then the whole closed-form solution breaks.\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom logistic import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)"
  },
  {
    "objectID": "posts/Double Descent/index.html#testing-model-on-simple-data",
    "href": "posts/Double Descent/index.html#testing-model-on-simple-data",
    "title": "Abstract:",
    "section": "Testing Model on Simple Data",
    "text": "Testing Model on Simple Data\nAfter implementing the MyLinearRegression model and OverParameterized Optimizer in my source code, I verify functionality by trying to fit it to some 1D data.\n\n# Generate nonlinear 1D data\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\n\n# Apply polynomial feature map to data\nphi = RandomFeatures(8)\nphi.fit(X)\nX_transformed = phi.transform(X)\n\n\n# Create and fit regression model\nMLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(MLR)\nopt.fit(X_transformed, y)\n\n\n# Predict model on same inputs\ny_preds = MLR.predict(X_transformed)\n\n# Plot it!\nplt.figure(figsize=(4,4))\n# need to convert to numpy bc that is what Matplot works with\nplt.scatter(X.numpy(), y.numpy(), label=\"Data\", color='gray')\nplt.plot(X.numpy(), y_preds.numpy(), label=\"Predictions\", color='blue')\nplt.legend()\nplt.title(\"Overparameterized Linear Regression\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.show()"
  },
  {
    "objectID": "posts/Double Descent/index.html#part-c-double-descent-in-image-corruption-detection",
    "href": "posts/Double Descent/index.html#part-c-double-descent-in-image-corruption-detection",
    "title": "Abstract:",
    "section": "Part C: Double Descent in Image Corruption Detection",
    "text": "Part C: Double Descent in Image Corruption Detection\nHere, I apply my model to a more complicated dataset!\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower, cmap='gray_r')\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1, cmap='gray_r')\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
  },
  {
    "objectID": "posts/Double Descent/index.html#my-task-assess-model-performance-on-image-corruption-prediction-as-vary-n_features-in-randomfeatures",
    "href": "posts/Double Descent/index.html#my-task-assess-model-performance-on-image-corruption-prediction-as-vary-n_features-in-randomfeatures",
    "title": "Abstract:",
    "section": "My Task: Assess model performance on image corruption prediction as vary n_features in RandomFeatures",
    "text": "My Task: Assess model performance on image corruption prediction as vary n_features in RandomFeatures\nSteps 1. Train model with different numbers of features 2. Compute training and test Mean Squared Errors (MSEs) 3. Plot Curves 4. plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍vertical line at interpolation threshold (point where number of features used exceeds number of training samples)\n\n\"\"\"\nThe first step is to train our model on a set of different feature numbers.\nSo, we perform a sweep over 200 different features numbers - based on provided\nreference plots - and keep track of training and testing errors for each number\nof features\n\"\"\"\nnum_features = list(range(1, 201))\ntraining_errors = []\ntesting_errors = []\n\nfor feats in num_features:\n    phi = RandomFeatures(n_features=feats, activation=square)\n    phi.fit(X_train)\n    X_train_features = phi.transform(X_train)\n    X_test_features  = phi.transform(X_test)\n\n    MLR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(MLR)\n    opt.fit(X_train_features, y_train)\n\n    MSE_train = MLR.loss(X_train_features, y_train).item()\n    MSE_test = MLR.loss(X_test_features, y_test).item()\n\n    training_errors.append(MSE_train)\n    testing_errors.append(MSE_test)\n\nmin_test_error = min(testing_errors)\nmin_test_error_index = testing_errors.index(min_test_error)\noptimal_num_features = num_features[min_test_error_index]\n\n\n\"\"\"\nHere, we plot the actual graphs that represent the above feature sweeps results.\nSince we know the interpolation threshold is the point where number of features\nexceeds number of training samples, we can plot our vertical line at the point where\nthe number of features equals the number of training samples.\n\"\"\"\n\nfig, axes = plt.subplots(1, 2, figsize=(14,5))\n\n# Training Plot\naxes[0].scatter(num_features, training_errors, color='gray')\n# Interp threshold is where num features &gt; num training samples\n# So, it is when num features = num training samples\naxes[0].axvline(x=len(X_train), color='black', linewidth=2)\naxes[0].set_yscale(\"log\")\naxes[0].set_title(\"Mean Squared Error (training)\")\naxes[0].set_xlabel(\"# of Features\")\naxes[0].set_xlim(0, 200)\naxes[0].grid(True)\n\naxes[1].scatter(num_features, testing_errors, color='red')\naxes[1].axvline(x=len(X_train), color='red', linewidth=2)\naxes[1].set_yscale(\"log\")\naxes[1].set_title(\"Mean Squared Error (testing)\")\naxes[1].set_xlabel(\"# of Features\")\naxes[1].set_xlim(0, 200)\naxes[1].grid(True)\n\n\n\n\n\n\n\n\n\n# And now, state the optimal number of features my exploration found\nprint(f\"Lowest Test Error = {min_test_error}, Occurring at {optimal_num_features} features\")\n\nLowest Test Error = 319.56678645266777, Occurring at 197 features\n\n\nWhat is Optimal? - Based on the results above, the lowest test error was ~287.03, which occurred at 200 features. The interpolation threshold was 100 features - since I trained on 100 data observations - meaning that the optimal number of features occurred above the interpolation threshold."
  },
  {
    "objectID": "posts/Double Descent/index.html#discussion",
    "href": "posts/Double Descent/index.html#discussion",
    "title": "Abstract:",
    "section": "Discussion:",
    "text": "Discussion:\nThis project gave me hands-on experience with the idea of overparameterized training. I was able to explore the complexities of matrix operations, and how the standard closed-form of linear regression fails when number of features exceeds observations due to the nature of matrix invertability. It was fascinating to see how at a point where the math seems to “break,” we are actually able to achieve algorithmic improvement once we try and push beyond those boundaries! By sweeping across increasing numbers of features, I was able to visualize how both training and testing errors behave as a model’s complexity grows. Most notably, my model achieved its best testing error - or really lowest testing error - when using 200 features, which was twice the number of training data observations I was using. This result confirmed that beyond the interpolation threshold, complexity can actually improve model performance, and reduce overfitting. This blog post ultimately helped me understand more about how machine learning algorithms can excel in high-parameter scenarios, even if the intuition of the math we knew begins to fall apart. What an powerful mystery to unfold!"
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html",
    "href": "posts/Implementing Logistic Regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, I implement logistic regression from scratch using PyTorch and apply it to both computer-generated and real-world data. I begin by developing a LogisticRegression class and GradientDescentOptimizer class that supports both vanilla gradient descent and momentum-based updates. Through a series of experiments, I analyzed the relationship of changing learning rates, momentum/beta rates, overfitting, and real-world datasets with the logistic regression algorithm. The experiments include generating 2D data for visualizing decision boundaries, a higher-dimensional overfitting case, and a real-world bot vs. user classification dataset from Kaggle. Each experiment is paired with visualizations and reflections to illustrate key concepts of the blog post.\n\nimport torch\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# Clarifying torch.mean dimensions\nX = torch.tensor([\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0]   \n])\n\nprint(torch.mean(X, dim = 0)) # Averages all elements in a column\nprint(torch.mean(X, dim = 1)) # Averages all elements in a row\n\ntensor([1., 2., 3.])\ntensor([2., 2., 2.])\n\n\n\n# Just confirmation that the pyfile contents work properly\n\nX = torch.tensor([\n    [1.0, 2.0, 1.0],\n    [2.0, 1.0, 1.0],\n    [0.0, 3.0, 1.0]   \n])\ny = torch.tensor([0.0, 1.0, 1.0])\n\nmodel = LogisticRegression()\nmodel.w = torch.tensor([0.1, -0.2, 0.0])\n\nprint(\"Scores:\", model.score(X))\nprint(\"Loss:\", model.loss(X, y))\nprint(\"Gradient:\", model.grad(X, y))\n\nScores: tensor([-0.3000,  0.0000, -0.6000])\nLoss: tensor(0.7617)\nGradient: tensor([-0.1915, -0.5286, -0.2400])\n\n\n\n# Define functions to create classification data, plot logistic classification data\n# and draw a line on classification plot\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_logistic_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nX, y = classification_data(noise = 0.5)\n\nThis function trains a logistic regression model on a given dataset using gradient descent (with optional momentum) and visualizes the training process. It tracks the logistic loss over 100 iterations and plots the loss curve to illustrate convergence results. If the input data has 2 features (plus a bias term), it also plots the decision boundary over the dataset.\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef logistic_loss_and_plot(X, y, alpha=0.1, beta=0.0, max_iters=100):\n    # Instantiate the model and optimizer\n    model = LogisticRegression()\n    model.score(X)  # ensure w is initialized\n    opt = GradientDescentOptimizer(model)\n\n    # Track loss over iterations\n    loss_vec = []\n\n    for _ in range(max_iters):\n        loss = model.loss(X, y)\n        loss_vec.append(loss.item())\n        opt.step(X, y, alpha=alpha, beta=beta)\n\n    print(f\"Final Loss: {loss_vec[-1]:.4f} after {max_iters} iterations.\")\n\n    # Plot loss per iteration\n    plt.figure(figsize=(6, 4))\n    plt.plot(loss_vec, color=\"slategrey\", linewidth=1.5)\n    plt.scatter(range(len(loss_vec)), loss_vec, color=\"slategrey\", edgecolors=\"black\", s=30)\n    plt.xlabel(\"Iteration\", fontsize=12)\n    plt.ylabel(\"Loss\", fontsize=12)\n    plt.title(f\"Final Loss: {loss_vec[-1]:.4f} after {len(loss_vec)} iterations.\", fontsize=12)\n    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n    # If data is 2D, also plot decision boundary\n    if X.shape[1] == 3:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.set(xlim=(-1, 2), ylim=(-1, 2))\n        plot_logistic_data(X, y, ax)\n\n        # Draw logistic decision boundary\n        draw_line(model.w, -1, 2, ax, color=\"black\")\n        ax.set_title(\"Decision Boundary (Logistic Regression)\")\n        plt.show()\n\n\n\nIn this experiment,I generate 2D classification data using the classification_data() function. The data is linearly separable with moderate noise. I train a logistic regression model using vanilla gradient descent and visualize the decision boundary and loss curve over 100 iterations. This aims to show that my logistic regression implementation correctly converges, decreases monotonically, and that a decision boundary can be visualized.\n\nX, y = classification_data(noise=0.5, p_dims=2)\nprint(\"Plot for beta = 0.0\")\nlogistic_loss_and_plot(X, y, alpha=0.1, beta=0.0)\n\nPlot for beta = 0.0\nFinal Loss: 0.3306 after 100 iterations."
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html#abstract",
    "href": "posts/Implementing Logistic Regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, I implement logistic regression from scratch using PyTorch and apply it to both computer-generated and real-world data. I begin by developing a LogisticRegression class and GradientDescentOptimizer class that supports both vanilla gradient descent and momentum-based updates. Through a series of experiments, I analyzed the relationship of changing learning rates, momentum/beta rates, overfitting, and real-world datasets with the logistic regression algorithm. The experiments include generating 2D data for visualizing decision boundaries, a higher-dimensional overfitting case, and a real-world bot vs. user classification dataset from Kaggle. Each experiment is paired with visualizations and reflections to illustrate key concepts of the blog post.\n\nimport torch\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# Clarifying torch.mean dimensions\nX = torch.tensor([\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0]   \n])\n\nprint(torch.mean(X, dim = 0)) # Averages all elements in a column\nprint(torch.mean(X, dim = 1)) # Averages all elements in a row\n\ntensor([1., 2., 3.])\ntensor([2., 2., 2.])\n\n\n\n# Just confirmation that the pyfile contents work properly\n\nX = torch.tensor([\n    [1.0, 2.0, 1.0],\n    [2.0, 1.0, 1.0],\n    [0.0, 3.0, 1.0]   \n])\ny = torch.tensor([0.0, 1.0, 1.0])\n\nmodel = LogisticRegression()\nmodel.w = torch.tensor([0.1, -0.2, 0.0])\n\nprint(\"Scores:\", model.score(X))\nprint(\"Loss:\", model.loss(X, y))\nprint(\"Gradient:\", model.grad(X, y))\n\nScores: tensor([-0.3000,  0.0000, -0.6000])\nLoss: tensor(0.7617)\nGradient: tensor([-0.1915, -0.5286, -0.2400])\n\n\n\n# Define functions to create classification data, plot logistic classification data\n# and draw a line on classification plot\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_logistic_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nX, y = classification_data(noise = 0.5)\n\nThis function trains a logistic regression model on a given dataset using gradient descent (with optional momentum) and visualizes the training process. It tracks the logistic loss over 100 iterations and plots the loss curve to illustrate convergence results. If the input data has 2 features (plus a bias term), it also plots the decision boundary over the dataset.\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef logistic_loss_and_plot(X, y, alpha=0.1, beta=0.0, max_iters=100):\n    # Instantiate the model and optimizer\n    model = LogisticRegression()\n    model.score(X)  # ensure w is initialized\n    opt = GradientDescentOptimizer(model)\n\n    # Track loss over iterations\n    loss_vec = []\n\n    for _ in range(max_iters):\n        loss = model.loss(X, y)\n        loss_vec.append(loss.item())\n        opt.step(X, y, alpha=alpha, beta=beta)\n\n    print(f\"Final Loss: {loss_vec[-1]:.4f} after {max_iters} iterations.\")\n\n    # Plot loss per iteration\n    plt.figure(figsize=(6, 4))\n    plt.plot(loss_vec, color=\"slategrey\", linewidth=1.5)\n    plt.scatter(range(len(loss_vec)), loss_vec, color=\"slategrey\", edgecolors=\"black\", s=30)\n    plt.xlabel(\"Iteration\", fontsize=12)\n    plt.ylabel(\"Loss\", fontsize=12)\n    plt.title(f\"Final Loss: {loss_vec[-1]:.4f} after {len(loss_vec)} iterations.\", fontsize=12)\n    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n    # If data is 2D, also plot decision boundary\n    if X.shape[1] == 3:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.set(xlim=(-1, 2), ylim=(-1, 2))\n        plot_logistic_data(X, y, ax)\n\n        # Draw logistic decision boundary\n        draw_line(model.w, -1, 2, ax, color=\"black\")\n        ax.set_title(\"Decision Boundary (Logistic Regression)\")\n        plt.show()\n\n\n\nIn this experiment,I generate 2D classification data using the classification_data() function. The data is linearly separable with moderate noise. I train a logistic regression model using vanilla gradient descent and visualize the decision boundary and loss curve over 100 iterations. This aims to show that my logistic regression implementation correctly converges, decreases monotonically, and that a decision boundary can be visualized.\n\nX, y = classification_data(noise=0.5, p_dims=2)\nprint(\"Plot for beta = 0.0\")\nlogistic_loss_and_plot(X, y, alpha=0.1, beta=0.0)\n\nPlot for beta = 0.0\nFinal Loss: 0.3306 after 100 iterations."
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html#experiment-2-gradient-descent-with-momentum",
    "href": "posts/Implementing Logistic Regression/index.html#experiment-2-gradient-descent-with-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Experiment 2: Gradient Descent with momentum",
    "text": "Experiment 2: Gradient Descent with momentum\nUsing the same dataset as Experiment 1, I compare vanilla gradient descent with momentum-based gradient descent. I train each model for the same number of iterations and plot the training loss over time. This aims to show how momentum accelerates convergence and leads to a lower loss being achieved earlier than vanilla gradient descent (as seen above).\n\nprint(\"Plot for beta = 0.9\")\nlogistic_loss_and_plot(X, y, alpha=0.1, beta=0.9)\n\nPlot for beta = 0.9\nFinal Loss: 0.1516 after 100 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 3: Overfitting in Higher Dimensional Space\nTo illustrate overfitting, I generate data where the number of features p &gt; n. I train the model on the training set with more features than datapoints until it achieves perfect accuracy, and then evaluate performance on the test set which has more datapoints than features. This experiment aims to show that while a model with a lot of features can lead to good training results, it can produce overfitting. Just because you achieve good loss does not mean you will have good accuracy on test data.\n\n# Fewer data points, more dimensions = overfitting risk\nX_train, y_train = classification_data(n_points=30, p_dims=100, noise=0.3)\nX_test, y_test = classification_data(n_points=300, p_dims=100, noise=0.3)\n\n# Plot training loss\nlogistic_loss_and_plot(X_train, y_train, alpha=0.1, beta=0.0)\n\nFinal Loss: 0.0933 after 100 iterations.\n\n\n\n\n\n\n\n\n\n\n# Recreate model for evaluation - logistic_loss_and_plot doesn't create global models\nmodel = LogisticRegression()\nmodel.score(X_train)\nopt = GradientDescentOptimizer(model)\n\nfor _ in range(100):\n    opt.step(X_train, y_train, alpha=0.1, beta=0.0)\n\ntrain_accuracy = (model.predict(X_train) == y_train)\ntest_accuracy = (model.predict(X_test) == y_test)\n\nprint(f\"Train Accuracy: {train_accuracy.float().mean():.4f}\")\nprint(f\"Test Accuracy:  {test_accuracy.float().mean():.4f}\")\n\nTrain Accuracy: 0.9667\nTest Accuracy:  0.8233\n\n\n\n\nIn this section, I analyze a real-world dataset containing features from user profiles on a Russian social media platform, VKontakte. The goal od this analysis is to train a model to predict whether a profile is a bot (target = 1) or a human (target = 0). I implement this using a logistic regression model with gradient descent, both with and without momentum. The analysis includes the following steps:\n\nData preprocessing: All feature values from the dataset must be converted to numeric values. I will drop column entries that are mostly missing, as well as data entries with absent values. Then I will split the dataset into training and test set using train-test split from scikit learn.\nFeature engineering\nTrain the Model: Fit the logistic regression model using gradient descent with and without momentum.\nVisualize Results: Plot the training loss data across gradient descent iterations for both approaches mentioned in step 3.\nCalculate the accuracy of the model on test data.\n\n\n\nStep 1: Data Loading and Preprocessing\nNotes on data provided on Kaggle\n\nNumerical features have NaNs preserved on purpose (i.e. these missing values might carry a meaning, such as inactivity)\nCategorical features: Any missing values are explicitly labeled ‘unknown’, and Boolean values are already converted to binary where applicable\n\nAs such, preprocessing should be optimized to only transform/pre-process entries where necessary, with these feature notes kept in mind. In other words, ‘unknown’ entries can be converted to 0 (not removed) and NaNs should be left in place/unchanged.\n\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Data Loading and Preprocessing\n\n# Load the data\ndf = pd.read_csv(\"bots_vs_users.csv\")\n\n# Replace 'Unknown' with 0 to indicate missing categorical value\ndf.replace(\"Unknown\", 0.0, inplace=True)\n\n# The only feature with string entry values is 'city', so we one-hot encode it\n# This creates a new column for each unique city value, with binary 0.0/1.0 value\n# Since there are only 4 distinct entries for this feature, this operation preferred to factorizing these values\ndf = pd.get_dummies(df, columns=[\"city\"], dtype=float)\n\n# Now, since we have possibly meaningful NaN values, i choose to replace them with -1\ndf = df.fillna(-1.0)\n\n# Drop the target column to create X and y\nX_df = df.drop(\"target\", axis=1)\n\n# Convert the feature matrix and target column to NumPy array for eventually use with PyTorch\n# Include astype(float) to ensure any possible type object entries are made into floats\nX_np = X_df.astype(float).values\ny = df[\"target\"].values\n\n# Scale the data to make sure all features on same scale\nscaler = StandardScaler()\nX = scaler.fit_transform(X_np)\n\n\n\nSplitting the Data: Train/Validation/Test Split\n\n# Split data into train (60%) and temp (40%) sets (To be turned to test and validation)\n\"\"\"\nI choose to pass stratify = y so that target distribution is maintained\nStratify makes sure that the distribution of entries in a certain column \nstays the same in the output data subset. In this case, we want the \ndistribution of bots and users to be the same. So, for example, say the \noriginal dataset had 50% bots and 50% users, us passing y to stratify\nwill ensure those proportions are maintained in X_train/y_train and \nX_temp/y_temp\n\"\"\"\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, train_size=0.6, random_state=802, stratify=y\n)\n\n# Split temp set evenly into validation (20%) and test (20%)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, train_size=0.5, random_state = 802, stratify=y_temp\n)\n\n# Convert these to PyTorch Tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\n\nX_validation = torch.tensor(X_val, dtype=torch.float32)\ny_validation = torch.tensor(y_val, dtype=torch.float32)\n\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n\n\n\nTraining Loop Function\n\ndef train_model(X_train, y_train, X_val, y_val, alpha=0.1, beta=0.0, num__iterations = 100):\n    LR = LogisticRegression()\n    opt = GradientDescentOptimizer(LR)\n    train_losses = []\n    val_losses = []\n\n    # Perform training iterations\n    for i in range(num__iterations):\n\n        # Since beta activates momentum in step, pass 0.0 unless otherwise indicated in function call\n        opt.step(X_train, y_train, alpha=alpha, beta=beta)\n\n        # Track the loss\n        # .item() extracts the Python scalar bc loss returns Tensor\n        train_loss = LR.loss(X_train, y_train).item()\n        val_loss = LR.loss(X_val, y_val).item()\n\n        # Append loss to arrays for plotting\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n    \n    return LR, train_losses, val_losses\n\n\n\nRun the Training Loop Function\n\n# Each function call below runs a training loop on our training/validation data.\n# The first call uses vanilla gradient descent (beta = 0.0)\n# The second call uses gradient descent with momentum (0.9)\n\nvanilla_LR, vanilla_train_loss, vanilla_validation_loss = train_model(\n    X_train, y_train, X_validation, y_validation, alpha=0.05, beta=0.0, num__iterations=100 \n)\n\nmomentum_LR, momentum_train_loss, momentum_validation_loss = train_model(\n    X_train, y_train, X_validation, y_validation, alpha=0.05, beta=0.9, num__iterations=100 \n)\n\n\n\nPlot Loss Curves\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\n\n# Plot 4 loss combinations\n\nplt.plot(vanilla_train_loss, \n         label=\"Vanilla Gradient Descent Training Loss\", \n         linestyle=\"-\", \n         color = \"red\")\n\nplt.plot(vanilla_validation_loss, \n         label=\"Vanilla Gradient Descent Validation Loss\", \n         linestyle=\"-\", \n         color = \"purple\")\n\nplt.plot(momentum_train_loss, \n         label=\"Gradient Descent W/ Momentum Training Loss\", \n         linestyle=\"--\", \n         color = \"red\")\n\nplt.plot(momentum_validation_loss, \n         label=\"Gradient Descent W/ Momentum Validation Loss\", \n         linestyle=\"--\", \n         color = \"purple\")\n\n# Give plot a title\nplt.title(\"Training and Validation Loss Over 100 Iterations\")\n\n# X-axis title\nplt.xlabel(\"# Iterations\")\n\n# y-label\nplt.ylabel(\"Loss\")\n\nplt.legend()\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTest Data Accuracy and Loss\n\ndef test_model(model, X_test, y_test):\n    preds = model.predict(X_test)\n    accuracy = (preds == y_test).float().mean().item()\n    loss = model.loss(X_test, y_test).item()\n\n    return accuracy, loss\n\n\nvanilla_accuracy, vanilla_loss = test_model(vanilla_LR, X_test, y_test)\nmomentum_accuracy, momentum_loss = test_model(momentum_LR, X_test, y_test)\n\nprint(f\"Vanilla Gradient Descent: \\nAccuracy = {vanilla_accuracy:.4f}; \\nLoss = {vanilla_loss:.2f}\\n\")\nprint(f\"Gradient Descent W/ Momentum: \\nAccuracy = {momentum_accuracy:.4f}; \\nLoss = {momentum_loss:.2f}\")\n\nVanilla Gradient Descent: \nAccuracy = 0.8996; \nLoss = 0.63\n\nGradient Descent W/ Momentum: \nAccuracy = 0.9387; \nLoss = 0.47\n\n\nExperiment 4 Conclusion: As we can see from the printed results, Vanilla gradient descent produces relatively high classification accuracy, but with a higher loss. This means the vanilla algorithm is less calibrated. As we transition to gradient descent with momentum, we achieve a higher accuracy AND lower loss, suggesting that keeping a record of prior model training steps can help improve future results!"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#abstract",
    "href": "posts/Classifying Palmers Penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract:",
    "text": "Abstract:\nThis blog post is an introductory exploration into understanding the process and procedures of ML classifications and predictions. Using the Palmers Penguin dataset, I was able to identify a set three features - Culmen Length (mm), Culmen Depth (mm), and Clutch Completion (T/F) - which produced the highest Logistic Regression (LR) cross validation score across all feature combinations. From this, I tested a variety of potential classification models - such as SVC, Random Forest, and Decision Tree, alongside the original LR model - adjusting parameter options such as gamma and max-depth identifying the LR model as that which performed best at classifying the training data. Then, with an “optimal” feature set and classification set identified - insofar as my experimentation indicated it to be optimal - I evaluated my model by splitting it our over the qualitative feature of my feature set, and showed that it only misidentified 1 penguin in the test data using a confusion matrix.\n\n# Imports\nimport pandas as pd\nimport numpy as np\n\n# Access data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "href": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "title": "Classifying Palmer Penguins",
    "section": "Our Data: A First Look",
    "text": "Our Data: A First Look\nUsing the preprocessing code provided by Prof. Phil as a basis, I copied the training data, and subsequently remove/revise columns of our dataset to make it easily useable for visualization. Specifically, I leave the Species column in so that I can group our data points in a more accessible manner - given the skills in Pandas/Seaborn I currently have\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create DataFrame for visualization\nX_train_visualize = train.copy()\nX_train_visualize[\"Species\"] = X_train_visualize[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nX_train_visualize[\"species_label\"] = le.fit_transform(X_train_visualize[\"Species\"])\nX_train_visualize = X_train_visualize.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Island\", \"Stage\"], axis = 1)\nX_train_visualize = X_train_visualize[X_train_visualize[\"Sex\"] != \".\"]\nX_train_visualize = X_train_visualize.dropna()\n\nNow, let’s take a look at our DataFrame\n\nX_train_visualize\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nspecies_label\n\n\n\n\n0\nChinstrap\nYes\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\n1\n\n\n1\nChinstrap\nYes\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\n1\n\n\n2\nGentoo\nYes\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\n2\n\n\n3\nGentoo\nYes\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\n2\n\n\n4\nChinstrap\nYes\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nGentoo\nYes\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\n2\n\n\n271\nAdelie\nNo\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\n0\n\n\n272\nAdelie\nYes\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\n0\n\n\n273\nAdelie\nYes\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\n0\n\n\n274\nChinstrap\nYes\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\n1\n\n\n\n\n256 rows × 10 columns\n\n\n\nWith our DataFrame on hand, we now move forward to part 1 of the blog post: Exploration of the data\n\n# Import Seaborn & matplotlib for visualizations\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 1: Pair Plotting Penguin Quantitative Features",
    "text": "Figure 1: Pair Plotting Penguin Quantitative Features\nUsing Seaborn’s pairplot function, I can plot all quantitative feature-pair scatterplots. Though I can’t add a title directly to a seaborn pairplot, since it is built on top of matplotlib, I can access plot attributes/modules such as figure, which holds all plot elements and can be used to add a title. See here for more info on the figure module: https://matplotlib.org/stable/api/figure_api.html#module-matplotlib.figure\n\npairplot = sns.pairplot(X_train_visualize[[\"Culmen Length (mm)\", \n                                \"Culmen Depth (mm)\", \n                                \"Flipper Length (mm)\", \n                                \"Body Mass (g)\", \n                                \"Delta 15 N (o/oo)\", \n                                \"Delta 13 C (o/oo)\", \n                                \"Species\"]], \n                                hue = \"Species\", \n                                palette = \"tab10\",\n                                corner=True)\n\n\npairplot.figure.suptitle(\"Pairplot of All Quantitative Penguin Features and Corresponding Distributions\",\n                         fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\nDiscussion:\nAbove we have used the Seaborn pairplot function to plot pairwise relationships between all quantitative features in our penguins training data set. Using Pandas fancy indexing, I have selected only the quantitative data in order to A.) limit the number of pairs plotted, as including qualitative data could make an excessively large figure, and B.) when I tried with qualitative data, data-points became so clustered around the limited values for each of those features - i.e. they would cluster around True and False in linear blobs - which made it very hard to distinguish between entries. While this visualization doesn’t necessarily offer a clear choice for optimal features - those features that would be “best” to fit a model on - it can at least offer some insight into where the penguin features experience overlap across species. If you are working with larger data sets, this may be a helpful first step in allowing the Data Scientist to get a “functional sense” of which entries may be productive to focus processing on, and eliminate early on unhelpful feature combinations which could speed up cross-validation/feature selection later down the line."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 2: Looking for Potential Biases in our Data Set",
    "text": "Figure 2: Looking for Potential Biases in our Data Set\nGiven that the above figure provides a pretty comprehensive look at our feature pairs, I thought it would be helpful to turn towards penguin counts to see if there might be biases in our data set. In other words, are there equal data points for each penguin species we are classifying/predicting?\n\nsns.histplot(X_train_visualize, x = \"Species\", hue = \"Species\", shrink = .75)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWhile this may seem to be a fairly simple visualization, I’d argue that it is one of the most insightful. By showing the count distribution of our penguin species, we begin to see where biases lie in our data set. In the case of the penguins, this means seeing which species are more or less present in the training dataset. If there is a dramatic difference between our different species, this means that we might run the risk of building a prediction model which performs unevenly across new samples. For example, it may be more poorly trained on Chinstrap penguins since there is a far lower count - and thus fewer data points to fit a model on - which could lead to incorrect predictions on future Chinstrap Penguins whose features fall outside the decision boundaries of our model. As such, the we should be wary of the difference between Gentoo and Adelie counts to Chinstrap, and as such should maybe consider features in which Chinstrap penguins have a higher standard deviation - such as Culmen Length - relative to that of Adelie and Gentoo."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 3: Summary Table",
    "text": "Figure 3: Summary Table\nWith both of previous figures in mind, it might now be helpful to get quantified information about how our feature relate across species and to other features.\n\nX_train_visualize.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]].aggregate([\"std\", \"mean\"])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n2.685713\n38.961111\n1.218430\n18.380556\n6.652184\n190.527778\n462.850335\n3722.916667\n0.428454\n8.861431\n0.567815\n-25.808814\n\n\nChinstrap\n3.456257\n48.771429\n1.137935\n18.346429\n7.366033\n195.821429\n410.148997\n3739.732143\n0.370290\n9.331004\n0.224608\n-24.567075\n\n\nGentoo\n2.783242\n47.133696\n1.016336\n14.926087\n6.061715\n216.739130\n498.976123\n5057.336957\n0.282566\n8.252573\n0.561689\n-26.145754\n\n\n\n\n\n\n\n\nDiscussion:\nThe above summary table shows the standard deviation and mean of all quantitative features in our data set, across the three species of penguins. This is again offers important insight into the breadth of our data points - how much variation we might find in a feature for a specific penguin species - and how each species’ feature sets might relate to one another - that is how close is the average culmen length for an Adelie penguin vs. a Gentoo penguin. Features which a higher standard deviation indicates that the data set might cover a wider/more representative portion of the species population, which could mean stronger predictions, but it could also mean more overlap with other species on that feature. This is pretty relative to the mean of those features, of course, but nonetheless offers an important reminder that there isn’t one “ideal” standard deviation. Instead, we must always consider it as a statistical value relative to the goal of our model and the context of our dataset. Additionally, features with a similar mean across species could indicate overlap in the data set, meaning it may not be the most optimal feature to fit our model on.\nFrom this data - which is visualized by the corresponding distributions on the diagonal of Figure 1 - we can see there are some sections of our data with significant overlap across species. This means it would not be best to build a model on these feature combinations."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 4 (After the Fact)",
    "text": "Figure 4 (After the Fact)\nHaving noticed that a qualitative feature was used to build our model, I wanted to see if there was some way that I could have visualized the quantitative data before building the model to see which feature - Sex or Clutch Completion - would be more helpful than the other.\n\nfig, ax = plt.subplots(1, 2, figsize = (9, 5))\n\np1 = sns.countplot(X_train_visualize, x = \"Clutch Completion\", hue = \"Species\", ax = ax[0])\np1 = sns.countplot(X_train_visualize, x = \"Sex\", hue = \"Species\", ax = ax[1], legend=False)\n\n\n\n\n\n\n\n\n\nDiscussion: (Extra - not part of original three visualizations)\nAfter completing the blog post, I returned to the data exploration/visualization. I was curious about how - considering the fact that clutch completion made it to our “optimal” feature set for classification - we could/would be able to predict if a qualitative feature would be part of our model’s feature set? So, while I tried a variety of different plotting functions, this was the best or really most useful visualization I could come up with, a simple count plot. Considering the fact that Clutch Completion made it to the classifier, I don’t see anything that would indicate it to be a strong distinguishing feature across the data set. All penguins have far more successful clutch completions (Yes entries) than not (No entries). Ultimately, this shows that the power of our model isn’t in single feature classifications, but in being able to compute the relationship between features across our data set."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "href": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Building The Model",
    "text": "Building The Model\nMoving now to part two, we need to actually build our model using features from the data set\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nNow that I have pre-processed the data, it’s time to determine which features will be used to build our model. Here we take a bit of a long approach, iterating through a variety of feature combinations and seeing which set has the best cross-validation score when fit by a Logistic Regression model.\nBefore processing the data, I decided to scale the quantitative features in order to speed up the computations. This process is result of receiving consistent warnings about max_iterations, and code remaining too slow even when the max_iter parameter on my LogisticRegression() constructor was set to 10,000. The warning produced this link: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\nAs such, I have included the StandardScaler() object and scaled/transformed my data as per the instructions on the linked page.\n\n# Cross Validations\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n# All feature columns\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', \n                  'Culmen Depth (mm)', \n                  'Flipper Length (mm)', \n                  'Body Mass (g)', \n                  'Delta 15 N (o/oo)', \n                  'Delta 13 C (o/oo)']\n\n# Initialize array for optimal columns and variable to store cross-validation\n# score for the optimal columns\noptimal_columns = []\nbest_LR_cv_score = 0\n\n# Scale data to facilitate a clean cross-validation process\nscaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_train_scaled[['Culmen Length (mm)', \n                'Culmen Depth (mm)', \n                'Flipper Length (mm)', \n                'Body Mass (g)', \n                'Delta 15 N (o/oo)', \n                'Delta 13 C (o/oo)']] = scaler.fit_transform(X_train_scaled[['Culmen Length (mm)', \n                                                                              'Culmen Depth (mm)', \n                                                                              'Flipper Length (mm)', \n                                                                              'Body Mass (g)', \n                                                                              'Delta 15 N (o/oo)', \n                                                                              'Delta 13 C (o/oo)']])\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    LR = LogisticRegression()\n    cv_scores_LR = cross_val_score(LR, X_train_scaled, y_train, cv=5)\n    cv_scores_mean = cv_scores_LR.mean()\n    if cv_scores_mean &gt; best_LR_cv_score:\n      best_LR_cv_score = cv_scores_mean\n      optimal_columns = cols\n\nprint(f\"The highest Cross-Validation Score for Logistic Regression was {best_LR_cv_score} \\nIt was modeled on the following features: \\n{optimal_columns}\")\n\n\nThe highest Cross-Validation Score for Logistic Regression was 0.996078431372549 \nIt was modeled on the following features: \n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nHaving identified our “optimal columns” - i.e. those with the best cross-validation score - I now explore how those features score when fit with different models.\n\n# Cross Validation for SVC\nfrom sklearn.svm import SVC\n# Define range of gamma values (as suggested in blog post assignment)\ngamma_values = 10.0**np.arange(-10, 10)\n\nbest_svc_cv_score = 0\noptimal_gamma = 0.0\n\nfor gamma in gamma_values:\n    svc = SVC(gamma = gamma)\n    cv_scores_svc = cross_val_score(svc, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_svc.mean()\n    if cv_scores_mean &gt; best_svc_cv_score:\n        best_svc_cv_score = cv_scores_mean\n        optimal_gamma = gamma\n        \n\nprint(f\"The highest Cross-Validation Score for SVC was {best_svc_cv_score} \\nIt was calculated with a gamma of {optimal_gamma}\")\n\nThe highest Cross-Validation Score for SVC was 0.9491704374057315 \nIt was calculated with a gamma of 0.1\n\n\n\n# Cross Validation for Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_DT_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    DT = DecisionTreeClassifier(max_depth = depth)\n    cv_scores_DT = cross_val_score(DT, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_DT.mean()\n    if cv_scores_mean &gt; best_DT_cv_score:\n        best_DT_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Decision Tree was {best_DT_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Decision Tree was 0.9452488687782805 \nIt was calculated with a max depth of 9\n\n\n\n# Cross Validation for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_RF_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    RF = RandomForestClassifier(max_depth = depth)\n    cv_scores_RF = cross_val_score(RF, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_RF.mean()\n    if cv_scores_mean &gt; best_RF_cv_score:\n        best_RF_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Random Forest was {best_RF_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Random Forest was 0.9491704374057315 \nIt was calculated with a max depth of 9"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "href": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluate",
    "text": "Evaluate\nWith our features chosen as well as our model - Logistic Regression - it is time to evaluate and test our model to see how well it performed\n\nPlotting Decision Regions\n\n# Preprocessing: Scale my data, leaving qualitative features alone\nscaler = StandardScaler()\noptimal_X_train_scaled = X_train[optimal_columns].copy()\noptimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n# Swap qualitative and quantitative to fit function provided on assignment\nplottable_opt_x_train_scaled = optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]]\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nLR = LogisticRegression()\nLR.fit(plottable_opt_x_train_scaled, y_train)\nplot_regions(LR, plottable_opt_x_train_scaled, y_train)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "href": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Testing & Confusion Matrix",
    "text": "Testing & Confusion Matrix\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Preprocess\nscaler = StandardScaler()\nX_test = X_test[optimal_columns].copy()\nX_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n\n# Produce a Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(optimal_X_train_scaled, y_train)\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 1, 10,  0],\n       [ 0,  0, 26]])\n\n\n\n# Print what each value in the Confusion Matrix represents in the context of our data\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "href": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Final Discussion:",
    "text": "Final Discussion:\nThis blog post taught me that, in a way, constructing predictive models is not a purely objective science. While the operations and computations encountered along the way are fundamentally based on logic, the problem of prediction is deeply shaped by the context of the data and the data scientist. At the end of the day the limitations of hardware, software, and our implementations of the algorithmic world cannot - or at least have not yet - account for all the variability in the physical world: The realm of computational classification and prediction is but a finite set of instructions to put together a puzzle of infinite possibilities! Even though the model I coded scored well on the test data, my capacity to correctly predict future penguin species is still constrained: 1 penguin incorrectly predicted still means the model is not perfect. Furthermore, it was trained on much fewer Chinstrap penguins than Adelie or Gentoo, which means I might run a greater risk of mis-predicting Chinstrap penguins down the line. Even more, though it scored well on the test data, that is still only a fraction of penguins that there are out there to predict: there is not necessarily a guarantee that my model will continue to predict at such a high level. This isn’t to take away from the performance of the model, or the power of prediction, but more a matter of respecting the dynamism of the physical world. If I have taken anything away from the blog post, it’s that we must look to build models in such a way that respects this truth."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Limitations of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\n\n\n\n\n\n\nJiffy Lesica\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2: Design and Impact of Automated Decision Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nJiffy Lesica\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\n\n\n\n\n\n\nJiffy Lesica\n\n\n\n\n\n\nNo matching items"
  }
]