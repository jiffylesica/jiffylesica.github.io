[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Jiffy and this is my blog for CS451!"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#abstract",
    "href": "posts/Classifying Palmers Penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract:",
    "text": "Abstract:\nThis blog post is an introductory exploration into understanding the process and procedures of ML classifications and predictions. Using the Palmers Penguin dataset, I was able to identify a set three features - Culmen Length (mm), Culmen Depth (mm), and Clutch Completion (T/F) - which produced the highest Logistic Regression (LR) cross validation score across all feature combinations. From this, I tested a variety of potential classification models - such as SVC, Random Forest, and Decision Tree, alongside the original LR model - adjusting parameter options such as gamma and max-depth identifying the LR model as that which performed best at classifying the training data. Then, with an “optimal” feature set and classification set identified - insofar as my experimentation indicated it to be optimal - I evaluated my model by splitting it our over the qualitative feature of my feature set, and showed that it only misidentified 1 penguin in the test data using a confusion matrix.\n\n# Imports\nimport pandas as pd\nimport numpy as np\n\n# Access data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "href": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "title": "Classifying Palmer Penguins",
    "section": "Our Data: A First Look",
    "text": "Our Data: A First Look\nUsing the preprocessing code provided by Prof. Phil as a basis, I copied the training data, and subsequently remove/revise columns of our dataset to make it easily useable for visualization. Specifically, I leave the Species column in so that I can group our data points in a more accessible manner - given the skills in Pandas/Seaborn I currently have\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create DataFrame for visualization\nX_train_visualize = train.copy()\nX_train_visualize[\"Species\"] = X_train_visualize[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nX_train_visualize[\"species_label\"] = le.fit_transform(X_train_visualize[\"Species\"])\nX_train_visualize = X_train_visualize.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Island\", \"Stage\"], axis = 1)\nX_train_visualize = X_train_visualize[X_train_visualize[\"Sex\"] != \".\"]\nX_train_visualize = X_train_visualize.dropna()\n\nNow, let’s take a look at our DataFrame\n\nX_train_visualize\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nspecies_label\n\n\n\n\n0\nChinstrap\nYes\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\n1\n\n\n1\nChinstrap\nYes\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\n1\n\n\n2\nGentoo\nYes\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\n2\n\n\n3\nGentoo\nYes\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\n2\n\n\n4\nChinstrap\nYes\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nGentoo\nYes\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\n2\n\n\n271\nAdelie\nNo\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\n0\n\n\n272\nAdelie\nYes\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\n0\n\n\n273\nAdelie\nYes\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\n0\n\n\n274\nChinstrap\nYes\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\n1\n\n\n\n\n256 rows × 10 columns\n\n\n\nWith our DataFrame on hand, we now move forward to part 1 of the blog post: Exploration of the data\n\n# Import Seaborn & matplotlib for visualizations\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 1: Pair Plotting Penguin Quantitative Features",
    "text": "Figure 1: Pair Plotting Penguin Quantitative Features\nUsing Seaborn’s pairplot function, I can plot all quantitative feature-pair scatterplots. Though I can’t add a title directly to a seaborn pairplot, since it is built on top of matplotlib, I can access plot attributes/modules such as figure, which holds all plot elements and can be used to add a title. See here for more info on the figure module: https://matplotlib.org/stable/api/figure_api.html#module-matplotlib.figure\n\npairplot = sns.pairplot(X_train_visualize[[\"Culmen Length (mm)\", \n                                \"Culmen Depth (mm)\", \n                                \"Flipper Length (mm)\", \n                                \"Body Mass (g)\", \n                                \"Delta 15 N (o/oo)\", \n                                \"Delta 13 C (o/oo)\", \n                                \"Species\"]], \n                                hue = \"Species\", \n                                palette = \"tab10\",\n                                corner=True)\n\n\npairplot.figure.suptitle(\"Pairplot of All Quantitative Penguin Features and Corresponding Distributions\",\n                         fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\nDiscussion:\nAbove we have used the Seaborn pairplot function to plot pairwise relationships between all quantitative features in our penguins training data set. Using Pandas fancy indexing, I have selected only the quantitative data in order to A.) limit the number of pairs plotted, as including qualitative data could make an excessively large figure, and B.) when I tried with qualitative data, data-points became so clustered around the limited values for each of those features - i.e. they would cluster around True and False in linear blobs - which made it very hard to distinguish between entries. While this visualization doesn’t necessarily offer a clear choice for optimal features - those features that would be “best” to fit a model on - it can at least offer some insight into where the penguin features experience overlap across species. If you are working with larger data sets, this may be a helpful first step in allowing the Data Scientist to get a “functional sense” of which entries may be productive to focus processing on, and eliminate early on unhelpful feature combinations which could speed up cross-validation/feature selection later down the line."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 2: Looking for Potential Biases in our Data Set",
    "text": "Figure 2: Looking for Potential Biases in our Data Set\nGiven that the above figure provides a pretty comprehensive look at our feature pairs, I thought it would be helpful to turn towards penguin counts to see if there might be biases in our data set. In other words, are there equal data points for each penguin species we are classifying/predicting?\n\nsns.histplot(X_train_visualize, x = \"Species\", hue = \"Species\", shrink = .75)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWhile this may seem to be a fairly simple visualization, I’d argue that it is one of the most insightful. By showing the count distribution of our penguin species, we begin to see where biases lie in our data set. In the case of the penguins, this means seeing which species are more or less present in the training dataset. If there is a dramatic difference between our different species, this means that we might run the risk of building a prediction model which performs unevenly across new samples. For example, it may be more poorly trained on Chinstrap penguins since there is a far lower count - and thus fewer data points to fit a model on - which could lead to incorrect predictions on future Chinstrap Penguins whose features fall outside the decision boundaries of our model. As such, the we should be wary of the difference between Gentoo and Adelie counts to Chinstrap, and as such should maybe consider features in which Chinstrap penguins have a higher standard deviation - such as Culmen Length - relative to that of Adelie and Gentoo."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 3: Summary Table",
    "text": "Figure 3: Summary Table\nWith both of previous figures in mind, it might now be helpful to get quantified information about how our feature relate across species and to other features.\n\nX_train_visualize.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]].aggregate([\"std\", \"mean\"])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n2.685713\n38.961111\n1.218430\n18.380556\n6.652184\n190.527778\n462.850335\n3722.916667\n0.428454\n8.861431\n0.567815\n-25.808814\n\n\nChinstrap\n3.456257\n48.771429\n1.137935\n18.346429\n7.366033\n195.821429\n410.148997\n3739.732143\n0.370290\n9.331004\n0.224608\n-24.567075\n\n\nGentoo\n2.783242\n47.133696\n1.016336\n14.926087\n6.061715\n216.739130\n498.976123\n5057.336957\n0.282566\n8.252573\n0.561689\n-26.145754\n\n\n\n\n\n\n\n\nDiscussion:\nThe above summary table shows the standard deviation and mean of all quantitative features in our data set, across the three species of penguins. This is again offers important insight into the breadth of our data points - how much variation we might find in a feature for a specific penguin species - and how each species’ feature sets might relate to one another - that is how close is the average culmen length for an Adelie penguin vs. a Gentoo penguin. Features which a higher standard deviation indicates that the data set might cover a wider/more representative portion of the species population, which could mean stronger predictions, but it could also mean more overlap with other species on that feature. This is pretty relative to the mean of those features, of course, but nonetheless offers an important reminder that there isn’t one “ideal” standard deviation. Instead, we must always consider it as a statistical value relative to the goal of our model and the context of our dataset. Additionally, features with a similar mean across species could indicate overlap in the data set, meaning it may not be the most optimal feature to fit our model on.\nFrom this data - which is visualized by the corresponding distributions on the diagonal of Figure 1 - we can see there are some sections of our data with significant overlap across species. This means it would not be best to build a model on these feature combinations."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 4 (After the Fact)",
    "text": "Figure 4 (After the Fact)\nHaving noticed that a qualitative feature was used to build our model, I wanted to see if there was some way that I could have visualized the quantitative data before building the model to see which feature - Sex or Clutch Completion - would be more helpful than the other.\n\nfig, ax = plt.subplots(1, 2, figsize = (9, 5))\n\np1 = sns.countplot(X_train_visualize, x = \"Clutch Completion\", hue = \"Species\", ax = ax[0])\np1 = sns.countplot(X_train_visualize, x = \"Sex\", hue = \"Species\", ax = ax[1], legend=False)\n\n\n\n\n\n\n\n\n\nDiscussion: (Extra - not part of original three visualizations)\nAfter completing the blog post, I returned to the data exploration/visualization. I was curious about how - considering the fact that clutch completion made it to our “optimal” feature set for classification - we could/would be able to predict if a qualitative feature would be part of our model’s feature set? So, while I tried a variety of different plotting functions, this was the best or really most useful visualization I could come up with, a simple count plot. Considering the fact that Clutch Completion made it to the classifier, I don’t see anything that would indicate it to be a strong distinguishing feature across the data set. All penguins have far more successful clutch completions (Yes entries) than not (No entries). Ultimately, this shows that the power of our model isn’t in single feature classifications, but in being able to compute the relationship between features across our data set."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "href": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Building The Model",
    "text": "Building The Model\nMoving now to part two, we need to actually build our model using features from the data set\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nNow that I have pre-processed the data, it’s time to determine which features will be used to build our model. Here we take a bit of a long approach, iterating through a variety of feature combinations and seeing which set has the best cross-validation score when fit by a Logistic Regression model.\nBefore processing the data, I decided to scale the quantitative features in order to speed up the computations. This process is result of receiving consistent warnings about max_iterations, and code remaining too slow even when the max_iter parameter on my LogisticRegression() constructor was set to 10,000. The warning produced this link: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\nAs such, I have included the StandardScaler() object and scaled/transformed my data as per the instructions on the linked page.\n\n# Cross Validations\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n# All feature columns\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', \n                  'Culmen Depth (mm)', \n                  'Flipper Length (mm)', \n                  'Body Mass (g)', \n                  'Delta 15 N (o/oo)', \n                  'Delta 13 C (o/oo)']\n\n# Initialize array for optimal columns and variable to store cross-validation\n# score for the optimal columns\noptimal_columns = []\nbest_LR_cv_score = 0\n\n# Scale data to facilitate a clean cross-validation process\nscaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_train_scaled[['Culmen Length (mm)', \n                'Culmen Depth (mm)', \n                'Flipper Length (mm)', \n                'Body Mass (g)', \n                'Delta 15 N (o/oo)', \n                'Delta 13 C (o/oo)']] = scaler.fit_transform(X_train_scaled[['Culmen Length (mm)', \n                                                                              'Culmen Depth (mm)', \n                                                                              'Flipper Length (mm)', \n                                                                              'Body Mass (g)', \n                                                                              'Delta 15 N (o/oo)', \n                                                                              'Delta 13 C (o/oo)']])\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    LR = LogisticRegression()\n    cv_scores_LR = cross_val_score(LR, X_train_scaled, y_train, cv=5)\n    cv_scores_mean = cv_scores_LR.mean()\n    if cv_scores_mean &gt; best_LR_cv_score:\n      best_LR_cv_score = cv_scores_mean\n      optimal_columns = cols\n\nprint(f\"The highest Cross-Validation Score for Logistic Regression was {best_LR_cv_score} \\nIt was modeled on the following features: \\n{optimal_columns}\")\n\n\nThe highest Cross-Validation Score for Logistic Regression was 0.996078431372549 \nIt was modeled on the following features: \n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nHaving identified our “optimal columns” - i.e. those with the best cross-validation score - I now explore how those features score when fit with different models.\n\n# Cross Validation for SVC\nfrom sklearn.svm import SVC\n# Define range of gamma values (as suggested in blog post assignment)\ngamma_values = 10.0**np.arange(-10, 10)\n\nbest_svc_cv_score = 0\noptimal_gamma = 0.0\n\nfor gamma in gamma_values:\n    svc = SVC(gamma = gamma)\n    cv_scores_svc = cross_val_score(svc, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_svc.mean()\n    if cv_scores_mean &gt; best_svc_cv_score:\n        best_svc_cv_score = cv_scores_mean\n        optimal_gamma = gamma\n        \n\nprint(f\"The highest Cross-Validation Score for SVC was {best_svc_cv_score} \\nIt was calculated with a gamma of {optimal_gamma}\")\n\nThe highest Cross-Validation Score for SVC was 0.9491704374057315 \nIt was calculated with a gamma of 0.1\n\n\n\n# Cross Validation for Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_DT_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    DT = DecisionTreeClassifier(max_depth = depth)\n    cv_scores_DT = cross_val_score(DT, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_DT.mean()\n    if cv_scores_mean &gt; best_DT_cv_score:\n        best_DT_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Decision Tree was {best_DT_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Decision Tree was 0.9452488687782805 \nIt was calculated with a max depth of 9\n\n\n\n# Cross Validation for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_RF_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    RF = RandomForestClassifier(max_depth = depth)\n    cv_scores_RF = cross_val_score(RF, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_RF.mean()\n    if cv_scores_mean &gt; best_RF_cv_score:\n        best_RF_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Random Forest was {best_RF_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Random Forest was 0.9491704374057315 \nIt was calculated with a max depth of 9"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "href": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluate",
    "text": "Evaluate\nWith our features chosen as well as our model - Logistic Regression - it is time to evaluate and test our model to see how well it performed\n\nPlotting Decision Regions\n\n# Preprocessing: Scale my data, leaving qualitative features alone\nscaler = StandardScaler()\noptimal_X_train_scaled = X_train[optimal_columns].copy()\noptimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n# Swap qualitative and quantitative to fit function provided on assignment\nplottable_opt_x_train_scaled = optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]]\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nLR = LogisticRegression()\nLR.fit(plottable_opt_x_train_scaled, y_train)\nplot_regions(LR, plottable_opt_x_train_scaled, y_train)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "href": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Testing & Confusion Matrix",
    "text": "Testing & Confusion Matrix\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Preprocess\nscaler = StandardScaler()\nX_test = X_test[optimal_columns].copy()\nX_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n\n# Produce a Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(optimal_X_train_scaled, y_train)\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 1, 10,  0],\n       [ 0,  0, 26]])\n\n\n\n# Print what each value in the Confusion Matrix represents in the context of our data\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "href": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Final Discussion:",
    "text": "Final Discussion:\nThis blog post taught me that, in a way, constructing predictive models is not a purely objective science. While the operations and computations encountered along the way are fundamentally based on logic, the problem of prediction is deeply shaped by the context of the data and the data scientist. At the end of the day the limitations of hardware, software, and our implementations of the algorithmic world cannot - or at least have not yet - account for all the variability in the physical world: The realm of computational classification and prediction is but a finite set of instructions to put together a puzzle of infinite possibilities! Even though the model I coded scored well on the test data, my capacity to correctly predict future penguin species is still constrained: 1 penguin incorrectly predicted still means the model is not perfect. Furthermore, it was trained on much fewer Chinstrap penguins than Adelie or Gentoo, which means I might run a greater risk of mis-predicting Chinstrap penguins down the line. Even more, though it scored well on the test data, that is still only a fraction of penguins that there are out there to predict: there is not necessarily a guarantee that my model will continue to predict at such a high level. This isn’t to take away from the performance of the model, or the power of prediction, but more a matter of respecting the dynamism of the physical world. If I have taken anything away from the blog post, it’s that we must look to build models in such a way that respects this truth."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Introduction:",
    "text": "Introduction:\nThe purpose of this blog post is to explore the quantitative and ethical depths of decision theory in classification. The specific goal of the assignment was to try and identify a scoring function and threshold which would optimize expected profit per borrower for a bank who is deciding who to give loans to, using any of the features contained in our data set and any method. I started by exploring the data visually, trying to understand how different feature columns related to each other, and how bivariate feature combinations may correlate to loan status - i.e. whether a borrower repaid their loan or defaulted. After this, I fit a logistic regression classification onto the data set using varying feature combinations to see if there were any particular combinations which optimized initial classification accuracy. Once a model was fit, I used the model weights to calculate linear risk scores for each of the data entries/borrowers, and used these scores to identify a threshold risk score (borrowers lower than the threshold were approved, and above the threshold denied) which maximized profit per borrower for the bank. I found that the optimal threshold score was 1.414, which produced a profit per borrower of $1391.53 on the training data. Using this score as a threshold on the test data, I identified a maximized profit of $1102.18 per borrower. While this was lower than the training value, that is expected. However, upon processing through the test data to find its specific optimizing threshold, I found that my proposed threshold of 1.414 was not maximizing profit to its full potential on the test data. Instead, maximum profits of $1347.19 was found at a threshold of 1.010. So, while the proposed system kept profit high, it did not achieve its highest potential on the test data.\n\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 1: Explore the Data",
    "text": "Part 1: Explore the Data\nRemember, 1 means a person defaulted on loan, and 0 means they repaid in full\n\n# Import seaborn, our visualization library\nimport seaborn as sns\n\n# Create a copy of our dataset, as since we use df_train later in this blogpost we want to prevent\n# any errors from being passed on to these visualizations due to the scope of df_train\nfor_viz = df_train.copy()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 1:",
    "text": "Figure 1:\nFirst, we want to understand how loan intent varies with different features, specifically borrower age, length of employment, and homeownership status. Not only with this offer us qualitative insights, but also help us understand the distribution of borrowers we are encountering, and what patterns may emerge in the data.\n\nsns.displot(data = for_viz, x = \"person_age\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe are dealing with, for the most part, younger borrowers. The vast majority of borrowers across intents are 40 or younger, and above 40 years of age we see a steep drop off in borrowers. We see an especially high number of young borrowers in the data set seeking a loan for education."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 2:",
    "text": "Figure 2:\n\nsns.displot(data = for_viz, x = \"person_emp_length\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe see similar shapes in these visualizations as in that comparing borrower age and loan intent, but the context of course gives it a different meaning. Across the board, we see that the majority of borrowers have been employed for under 20 years. Again, the highest count of borrowers is found in the subset of those seeking education loans. After 20 years of employment, loan seekers effectively disappear from view. The lowest number of loan seekers are found for home improvement loans."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 3:",
    "text": "Figure 3:\n\nsns.displot(data = for_viz, x = \"person_home_ownership\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nIn nearly all of the visualizations, the highest number of borrowers seeking a loan are also renters. From highest to lowest counts, the borrowers’ home ownership status is RENT, MORTGAGE, and OWN. This makes sense intuitively as home ownership is likely a proxy for financial secure individuals who may be less likely to seek a loan as a form of financial support. The only place we do not see this trend is in the Home Improvement visualization. Here, the plurality of borrowers are mortgaging their home. This also makes sense, however, because those who have mortgages are likely “newer” homeowners who are undergoing more renovations/home improvements than those who have owned a home for a long time."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 4:",
    "text": "Figure 4:\nNext, I want to see the relationship between loan interest rates and loans as a percent of a borrowers income, and how this relationship may shape loan status - i.e. whether a borrower repays or defaults. Additionally, it will be helpful to compare visualization methods to see which Seaborn plots are more effective for such a large dataset\n\nsns.jointplot(for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\", kind = \"kde\", height=6)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\")\n\n\n\n\n\n\n\n\n\nDiscussion:\nAs seen above, using a scatterplot to visualize such a large set of data points is not a very effective approach. There is a great deal of overlap across our points, meaning that classification trends may be getting lost behind other data points. When looking at the joint plot, we first note from the edge plots that there are far more repaying than defaulting borrowers in our data set. Additionally, a higher distribution of borrowers repay loans they have a low interest rate, and their loan is a lower percent of their income. However, the reality remains that - based on the inner JointGrid - that there is still a great deal of overlap between borrowers of both loan status in relation to these two variables. Feature selection could not be done here purely through visualization."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 2: Building a Model",
    "text": "Part 2: Building a Model\n\n# First, establish the target column. This won't change so I initialize it here.\nTARGET_COL = \"loan_status\"\n\n# Even before processing the data, we perform some preliminary changes to the DataFrame to ensure\n# future processing does not interfere with indexing, such as dropping entries with empty values,\n# and converting qualitative features to one-hot encoded dummies\ndf_train = pd.get_dummies(df_train)\ndf_train = df_train.dropna()\n\n\n# Create a preprocessing function that will scale data and \n# create an X_train and y_train based on chosen feature columns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\ndef preprocess(df, quant_cols, qual_cols, target_col):\n\n    df_new = pd.DataFrame()\n    df_new[quant_cols] = df[quant_cols]\n    df_new[qual_cols] = df[qual_cols]\n    df_new_target = df[target_col]\n\n    scaler = StandardScaler()\n    \n    df_new[quant_cols] = scaler.fit_transform(df_new[quant_cols])\n    df_new[qual_cols] = df_new[qual_cols]\n    \n    return df_new, df_new_target\n\n\nExperiment with features/models\n\n# Model imports\nfrom sklearn.linear_model import LogisticRegression\n\n\n# First, I will test a model on all quantitative/qualitative features in the dataset\n\nX1_quant_cols = [\"person_age\", \n                    \"person_income\", \n                    \"person_emp_length\",\n                    \"loan_int_rate\", \n                    \"loan_amnt\", \n                    \"loan_percent_income\",\n                    \"cb_person_default_on_file_N\",\n                    \"cb_person_default_on_file_Y\",\n                    \"cb_person_cred_hist_length\"]\n\nX1_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\",\n                    \"loan_intent_VENTURE\",\n                    \"loan_intent_EDUCATION\",\n                    \"loan_intent_MEDICAL\",\n                    \"loan_intent_HOMEIMPROVEMENT\",\n                    \"loan_intent_PERSONAL\",\n                    \"loan_intent_DEBTCONSOLIDATION\", ]\n\n\nX1_train, y_train = preprocess(df_train, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\nLR1 = LogisticRegression()\nfit1 = LR1.fit(X1_train, y_train)\nLR1.score(X1_train, y_train)\n\n0.8498712184048544\n\n\n\n# Having fit and scored a model on all feature columns, we now explore smaller feature combinations,\n# looking to see if any particular combination maximizes classification accuracy\nX2_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"person_emp_length\"]\n\nX2_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX2_train, y_train = preprocess(df_train, X2_quant_cols, X2_qual_cols, TARGET_COL)\n\nLR2 = LogisticRegression()\nfit2 = LR2.fit(X2_train, y_train)\nLR2.score(X2_train, y_train)\n\n0.8468153839437726\n\n\n\n# Similar to above block but for alternative feature combination\n\nX3_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"loan_amnt\"]\n\nX3_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX3_train, y_train = preprocess(df_train, X3_quant_cols, X3_qual_cols, TARGET_COL)\n\nLR3 = LogisticRegression()\nfit3 = LR3.fit(X3_train, y_train)\nLR3.score(X3_train, y_train)\n\n0.844632645043\n\n\n\n# Now that we have fit a Logistic Regression model on our data, we have access to the\n# Model weights via the .coef_ attribute.\n# However, we to transform this array into a column because otherwise it is just a row,\n# complicating necessary computations in the future\nprint(fit1.coef_.T)\n\n[[-0.0423979 ]\n [ 0.04612073]\n [-0.02431996]\n [ 1.03609245]\n [-0.57955282]\n [ 1.3239725 ]\n [-0.0143236 ]\n [ 0.0143236 ]\n [-0.00851694]\n [-0.27565869]\n [ 0.46232549]\n [-1.75917877]\n [-0.87364387]\n [-0.6441703 ]\n [-0.02154856]\n [ 0.24354326]\n [-0.46649075]\n [ 0.15978062]]"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 3: Find a Threshold",
    "text": "Part 3: Find a Threshold\n\n# Define a function that calculates the linear scores of our model\n# By calculating the cross product between our predictors and model weights\ndef linear_score(X, w):\n    return X@w.T\n\nX1_scores = linear_score(X1_train, fit1.coef_)\nX1_scores = X1_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX2_scores = linear_score(X2_train, fit2.coef_)\nX2_scores = X2_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX3_scores = linear_score(X3_train, fit3.coef_)\nX3_scores = X3_scores.iloc[:, 0] # All rows and first column to convert to Series\n\n\n# Plot the scores of our linear score function to see score distribution\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(X3_scores, bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\n\n# Now that we know our score distributions, we can select a range of possible thresholds\n# and define a function to identify the threshold which maximizes profit\ndef calculate_repaid_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"]) / 100) ** 10) - df[\"loan_amnt\"])\n\ndef calculate_default_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"] / 100)) ** 3) - (1.7 * df[\"loan_amnt\"]))\n\ndef plot_best_threshold(df, scores, y_train):\n    best_profit = -1000000000\n    best_threshold = 0\n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4)) \n    for t in np.linspace(0, 10, 100):\n        y_pred = scores &gt; t\n        tp = (y_pred == 0) & (y_train == 0)\n        # tp.sum()\n        fp = (y_pred == 0) & (y_train == 1)\n        # fp.sum()\n        profit = calculate_repaid_profit(df[tp]).sum() + calculate_default_profit(df[fp]).sum()\n        profit /= len(df)\n        ax.scatter(t, profit, color = \"steelblue\", s = 10)\n        if profit &gt; best_profit: \n            best_profit = profit\n            best_threshold = t\n\n\n    ax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\n    labs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net Profit\", title = f\"Best Profit Per Borrower ${best_profit:.2f} at best threshold t = {best_threshold:.3f}\")\n\nplot_best_threshold(df_train, X3_scores, y_train)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Evaluate My Model from the Bank’s Perspective",
    "text": "Evaluate My Model from the Bank’s Perspective\n\nOPTIMAL_THRESHOLD = 1.414\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test = pd.get_dummies(df_test)\ndf_test = df_test.dropna()\nX_test, y_test = preprocess(df_test, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\ntest_scores = linear_score(X_test, fit1.coef_)\ntest_scores = test_scores.iloc[:, 0]\n\ny_pred = test_scores &gt; OPTIMAL_THRESHOLD\ntest_tp = (y_pred == 0) & (y_test == 0)\ntest_tn = (y_pred == 1) & (y_test == 1)\ntest_profit = calculate_repaid_profit(df_test[test_tp]).sum() + calculate_default_profit(df_test[test_tn]).sum()\ntest_profit /= len(df_test)\nprint(f\"Optimal Profit is ${test_profit:.2f}\")\nplot_best_threshold(df_test, test_scores, y_test)\n\nOptimal Profit is $1102.18\n\n\n\n\n\n\n\n\n\n\ndf_test[\"risk_score\"] = test_scores\nage_risk_score = df_test.groupby(\"person_age\")[[\"person_age\", \"risk_score\"]].mean()\n\n\nsns.scatterplot(df_test, x = \"person_age\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_age_approved = approved[\"person_age\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_age_denied = denied[\"person_age\"].mean()\nprint(f\" Average age of approved borrower: {avg_age_approved} vs. Average age of denied borrower: {avg_age_denied}\")\n\n Average age of approved borrower: 27.87536718422157 vs. Average age of denied borrower: 26.935751295336786\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the average age of borrowers approved with our threshold is less than 1 year older than the average age of denied borrowers. As such, it appears that the people of varying age groups have a similar degree of access to credit under my proposed system.\n\ncount_medical_total =  df_test[df_test[\"loan_intent_MEDICAL\"]]\ncount_medical_approved = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_medical_default = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\n\nprint(f\"Percentage of medical seeking borrowers which were approved: {(len(count_medical_approved)/len(count_medical_total))*100:.2f}%\")\nprint(f\"Percentage of approved medical seeking borrowers who defaulted: {(len(count_medical_default)/len(count_medical_approved))*100:.2f}%\")\n\n\nPercentage of medical seeking borrowers which were approved: 77.54%\nPercentage of approved medical seeking borrowers who defaulted: 15.02%\n\n\n\ncount_venture_total =  df_test[df_test[\"loan_intent_VENTURE\"]]\ncount_venture_approved = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_venture_default = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of venture seeking borrowers which were approved: {(len(count_venture_approved)/len(count_venture_total))*100:.2f}%\")\nprint(f\"Percentage of approved venture seeking borrowers who defaulted: {(len(count_venture_default)/len(count_venture_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 89.94%\nPercentage of approved medical seeking borrowers who defaulted: 7.50%\n\n\n\ncount_education_total =  df_test[df_test[\"loan_intent_EDUCATION\"]]\ncount_education_approved = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_education_default = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of education seeking borrowers which were approved: {(len(count_education_approved)/len(count_education_total))*100:.2f}%\")\nprint(f\"Percentage of approved education seeking borrowers who defaulted: {(len(count_education_default)/len(count_education_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 88.78%\nPercentage of approved medical seeking borrowers who defaulted: 10.73%\n\n\n\n\nAnalysis:\nBased on the above data, borrowers seeking a loan for medical reasons have a more difficult time accessing credit under my proposed system. Only 77.54% of borrowers from the test set who were seeking medical loans were approved compared to venture at 89.94% and education borrowers at 88.78%. However, at the same time, the rate of default was higher amongst medical borrowers - 15.02% - compared to venture - 7.50% - and education - 10.73%.\n\nsns.scatterplot(df_test, x = \"person_income\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_income_approved = approved[\"person_income\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_income_denied = denied[\"person_income\"].mean()\nprint(f\" Average income of approved borrower: {avg_income_approved} vs. Average income of denied borrower: {avg_income_denied}\")\n\n Average income of approved borrower: 71671.01657574486 vs. Average income of denied borrower: 41555.21658031088\n\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the income of borrowers approved with our threshold is notably higher - ~$30,000 higher - than the income of denied borrowers. As such, it appears that the people of lower income groups may have a more difficult time accessing credit under this system."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nIn conclusion, this post examined decision theory in classification through both quantitative and ethical lenses, aiming to optimize a bank’s expected profit per borrower. The analysis began with visual exploration of the data, followed by fitting logistic regression models using various feature combinations. By computing linear risk scores, an initial optimal threshold of 1.414 was identified — yielding $1391.53 per borrower on the training data and $1102.18 on the test data. However, further analysis of the test set revealed an optimal threshold of 1.010, which maximized profit at $1347.19 per borrower, indicating that while the system maintained high profitability, it did not capture the test data’s full profit potential.\nAt the end of the study, I explored the fairness of this model. Fairness in general is a difficult concept to define, but is complicated to a high degree in the context of machine learning models which make predictions about people. A machine learning model can never achieve full fairness - making a decision about an outcome only after considering not only all features/traits of a person relevant to that outcome, but also the context-specificity of those features - as ML is fundamentally built on generalization from examples. So, instead the question of fairness in this post should not be understood as an absolute, but relative to the context at hand: is the model fair enough? Also, the notion of fairness depends on the position of the evaluator - what fairness means to me fundamentally as a person is different to someone whose primary goal is to develop a system which maximizes profits, and therefore makes decisions about borrowers with historical default rates in mind.\nAfter analyzing the outcome data, I noticed that the percent of approved borrowers seeking a medical loan was over 10% lower than that of education/venture seeking borrowers. However, at the same time, medical borrowers had a default rate which was 5% higher than the other two loan seeking groups. From this, I am inclined to ask whether, considering their higher rate of default, it is fair for medical borrowers to have a harder time accessing credit? On one hand, I want to say this is not fair, as even though the borrowers have a history of higher defaults, the value/importance of a medical loan - to potentially save ones life - is much higher than that of an education or venture loan seeker. But at the same time, I wonder whether a bank-loaning model is realistically possible or responsible to include this value in an automated decision making model: can a profit maximizing model actually objectively quantify the ethical/lived importance of a loan to a borrower? Ultimately, these observations highlight the inherent tension between profit optimization and equitable access to credit. While the model accurately identifies risk patterns based on historical data, it falls short in capturing broader social values, particularly for medical needs where the stakes may be much higher. This raises critical ethical questions: should decision systems rely solely on past performance, or must they also adjust to recognize the importance of timely, life-saving credit?"
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Goals: 1. Train a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including gender, and 2. Perform a bias audit of our algorithm to determine whether it displays gender bias.\nIn this blog post, I trained a machine learning classifier on PUMS data from the state of Connecticut to predict an individual’s employment status based on demographic features. After preparing the data with the folktables library, I experimented with a various classifier models, ultimately deciding on a Random Forest model whose performance I evaluated using metrics like accuracy, Positive Prediction Value, and False Positive/Negative Rates. I then conducted a bias audit by examining false positive rates (FPR), false negative rates (FNR), and positive predictive value (PPV) across the gender groups available in the data - Male and Female. I then plotted feasible FNRs & FPRs to visualize the relationship of error rates between groups, and understand how much one group’s error rate would have to change to match the others. Our results showed notable differences in error rates and PPVs between groups, highlighting areas where the model may inadvertently misclassify women more frequently.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"CT\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000191\n1\n1\n302\n1\n9\n1013097\n40\n90\n...\n82\n39\n38\n81\n0\n38\n0\n0\n36\n34\n\n\n1\nP\n2018GQ0000285\n1\n1\n101\n1\n9\n1013097\n70\n18\n...\n69\n134\n7\n70\n6\n68\n141\n68\n132\n145\n\n\n2\nP\n2018GQ0000328\n1\n1\n1101\n1\n9\n1013097\n17\n54\n...\n37\n35\n16\n16\n0\n17\n18\n19\n18\n16\n\n\n3\nP\n2018GQ0000360\n1\n1\n905\n1\n9\n1013097\n47\n18\n...\n46\n48\n4\n90\n87\n84\n90\n3\n47\n48\n\n\n4\nP\n2018GQ0000428\n1\n1\n903\n1\n9\n1013097\n35\n96\n...\n32\n35\n36\n71\n36\n3\n37\n2\n2\n35\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n90\n16.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n2\n1\n6.0\n\n\n1\n18\n16.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n17.0\n5\n17\n1\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n3\n18\n19.0\n5\n17\n2\nNaN\n3\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n4\n96\n16.0\n2\n16\n1\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n1.0\n2\n1\n6.0"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#my-version",
    "href": "posts/Auditing Bias/index.html#my-version",
    "title": "Auditing Bias",
    "section": "My Version",
    "text": "My Version\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n\nfeatures_to_use1 = [f for f in possible_features if f not in [\"ESR\", \"SEX\"]]\n\n\nEmploymentProblemSex = BasicProblem(\n    features=features_to_use1,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblemSex.df_to_numpy(acs_data)\n\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nConvert Folktables data back to Pandas DataFrame for ease of descriptive analysis\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use1)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\n1. How many individuals are in the data?\n\nprint(f\"There are {df.shape[0]} individuals in the data for the state of CT\")\n\nThere are 29029 individuals in the data for the state of CT\n\n\n\n\n2. How Many Individuals Have a Target Label == 1 (i.e. How Many Individuals are Employed)?\n\nprint(f\"Of the {df.shape[0]} in the data set, {(df['label'].mean() * 100):.2f}% - or {df['label'].sum()} individuals - are employed\")\n\nOf the 29029 in the data set, 48.43% - or 14060 individuals - are employed\n\n\n\n\n3. Of the Employed Individuals, How Many are Male (1) and How Many are Female (2)?\n\nemployed_by_group = df.groupby(\"group\")[\"label\"].sum()\nprint(f\"Of the 14060 employed individuals, {employed_by_group.iloc[0]} are male and {employed_by_group.iloc[1]} are female\")\n\nOf the 14060 employed individuals, 7169 are male and 6891 are female\n\n\n\n\n4. In Each Group, What Proportion of Individuals Have Target Label Equal to 1 (i.e. Are Employed)?\n\nproportion_employed_by_group = df.groupby(\"group\")[\"label\"].mean()\nprint(f\"According to the Data,{proportion_employed_by_group.iloc[0]*100: .2f}% of male individuals are employed and{proportion_employed_by_group.iloc[1]*100: .2f}% female individuals are employed\")\n\nAccording to the Data, 51.17% of male individuals are employed and 45.88% female individuals are employed\n\n\n\n\n5. Intersectional Trends\n\n# Since RAC1P Has Values &gt; 2, we must filter to only 1.0 and 2.0\ndf_filtered = df[(df[\"RAC1P\"] == 1.0) | (df[\"RAC1P\"] == 2.0)].copy()\n# Now, we convert RAC1P values to ints to match type\ndf_filtered[\"RAC1P\"] = df_filtered[\"RAC1P\"].astype(int)\n# Since I want to use categorical labels, and not just numbered labels, we use Pandas map function on a series\n# https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html\ndf_filtered[\"group\"] = df_filtered[\"group\"].map({1 : \"Male\", 2 : \"Female\"})\ndf_filtered[\"RAC1P\"] = df_filtered[\"RAC1P\"].map({1 : \"White\", 2 : \"Black\"})\nproportion_employed_by_group = df_filtered.groupby([\"group\", \"RAC1P\"])[\"label\"].mean()\n\ngroup   RAC1P\nFemale  Black    0.466667\n        White    0.464758\nMale    Black    0.371805\n        White    0.535252\nName: label, dtype: float64\n\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Using seaborn barplot uses mean as default, which represents the proportional insights we are looking for\nsns.barplot(df_filtered, x = \"group\", y = \"label\", hue = \"RAC1P\", width=.8, gap=.2)\nplt.title(\"Intersectional Trends\")\n\nText(0.5, 1.0, 'Intersectional Trends')\n\n\n\n\n\n\n\n\n\nTraining my model with different classifiers\n\n# Include alternative classifiers from sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#logistic-regression",
    "href": "posts/Auditing Bias/index.html#logistic-regression",
    "title": "Auditing Bias",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n# Logistic Regression Classifier (playing with polynomial features)\n# Discovered we can ass PolynomialFeatures to pipeline!\nfrom sklearn.preprocessing import PolynomialFeatures\n# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\nmodel_sex_LR = make_pipeline(PolynomialFeatures(degree = 2), StandardScaler(), LogisticRegression(max_iter=1000))\nmodel_sex_LR.fit(X_train.copy(), y_train)\n\ny_hat_LR = model_sex_LR.predict(X_test)\n\n# Calculate Overall Values\nTP_LR = ((y_hat_LR == 1) & (y_test == 1)).sum()\nFP_LR = ((y_hat_LR == 1) & (y_test == 0)).sum()\nTN_LR = ((y_hat_LR == 0) & (y_test == 0)).sum()\nFN_LR = ((y_hat_LR == 0) & (y_test == 1)).sum()\n\nPPV_LR = TP_LR / (y_hat_LR == 1).sum()\nFPR_LR = FP_LR / (TP_LR + FP_LR)\nFNR_LR = FN_LR / (TP_LR + FN_LR)\n\nprint(f\"Accuracy for Logistic Regression:{((y_hat_LR == y_test).mean())*100: .2f}%\")\nprint(f\"PPV for Logistic Regression:{PPV_LR*100: .2f}%\")\nprint(f\"Overall FPR for LR:{FPR_LR*100: .2f}%\")\nprint(f\"Overall FNR for LR:{FNR_LR*100: .2f}%\")\n\nAccuracy for Logistic Regression: 82.57%\nPPV for Logistic Regression: 79.65%\nOverall FPR for LR: 20.35%\nOverall FNR for LR: 12.74%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_LR = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_LR[\"group\"] = group_test\ndf_test_LR[\"label\"] = y_test\ndf_test_LR[\"predicted_value\"] = y_hat_LR\ndf_test_LR[\"correct_prediction\"] = df_test_LR[\"predicted_value\"] == df_test_LR[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_LR[\"correct_prediction\"] = df_test_LR[\"predicted_value\"] == df_test_LR[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n \\n {df_test_LR.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_LR[\"true_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 1)\ndf_test_LR[\"false_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_LR = df_test_LR.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_LR = df_test_LR.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_LR / (TP_per_group_LR + FP_per_group_LR)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_LR[\"false_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n \\n {df_test_LR.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_LR[\"false_negative\"] = (df_test_LR[\"predicted_value\"] == 0) & (df_test_LR[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n \\n {df_test_LR.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_LR = df_test_LR.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_LR = df_test_LR.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_LR = (predicted_positives_per_group_LR / per_group_total_LR) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_LR}%\")\n\n\n\nAccuracy By Group: \n \n group\n1    84.209040\n2    81.011296\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    83.887734\n2    75.639764\ndtype: float64\n\nFPR By Group: \n \n group\n1     8.757062\n2    13.313609\nName: false_positive, dtype: float64\n\nFNR By Group: \n \n group\n1    7.033898\n2    5.675094\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    54.350282\n2    54.653039\nName: predicted_value, dtype: float64%\n\n\n\nDiscussion:\nBy implementing the PolynomialFeatures preprocessing function/module, I was able to add polynomial feature adjustments directly into the model pipeline. By calculating all polynomial combinations of my features with a degree of 2, I achieved my best accuracy of 82.57%."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#svc",
    "href": "posts/Auditing Bias/index.html#svc",
    "title": "Auditing Bias",
    "section": "SVC",
    "text": "SVC\n\n# SVC Classifier\nmodel_sex_SVC = make_pipeline(StandardScaler(), SVC(C = 3.0))\nmodel_sex_SVC.fit(X_train, y_train)\n\ny_hat_SVC = model_sex_SVC.predict(X_test)\n\n# Calculate Overall Values\nTP_SVC = ((y_hat_SVC == 1) & (y_test == 1)).sum()\nFP_SVC = ((y_hat_SVC == 1) & (y_test == 0)).sum()\nTN_SVC = ((y_hat_SVC == 0) & (y_test == 0)).sum()\nFN_SVC = ((y_hat_SVC == 0) & (y_test == 1)).sum()\n\nPPV_SVC = TP_SVC / (y_hat_SVC == 1).sum()\nFPR_SVC = FP_SVC / (TP_SVC + FP_SVC)\nFNR_SVC = FN_SVC / (TP_SVC + FN_SVC)\n\nprint(f\"Accuracy for SVC:{((y_hat_SVC == y_test).mean())*100: .2f}%\")\nprint(f\"PPV for SVC:{PPV_SVC*100: .2f}%\")\nprint(f\"Overall FPR for SVC:{FPR_SVC*100: .2f}%\")\nprint(f\"Overall FNR for SVC:{FNR_SVC*100: .2f}%\")\n\nAccuracy for SVC: 82.41%\nPPV for SVC: 78.79%\nOverall FPR for SVC: 21.21%\nOverall FNR for SVC: 11.55%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_SVC = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_SVC[\"group\"] = group_test\ndf_test_SVC[\"label\"] = y_test\ndf_test_SVC[\"predicted_value\"] = y_hat_SVC\ndf_test_SVC[\"correct_prediction\"] = df_test_SVC[\"predicted_value\"] == df_test_SVC[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_SVC[\"correct_prediction\"] = df_test_SVC[\"predicted_value\"] == df_test_SVC[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n {df_test_SVC.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_SVC[\"true_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 1)\ndf_test_SVC[\"false_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_SVC = df_test_SVC.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_SVC = df_test_SVC.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_SVC / (TP_per_group_SVC + FP_per_group_SVC)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_SVC[\"false_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n {df_test_SVC.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_SVC[\"false_negative\"] = (df_test_SVC[\"predicted_value\"] == 0) & (df_test_SVC[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n {df_test_SVC.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_SVC = df_test_SVC.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_SVC = df_test_SVC.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_SVC = (predicted_positives_per_group_SVC / per_group_total_SVC) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_SVC}\")\n\n\nAccuracy By Group: \n group\n1    84.406780\n2    80.500269\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    83.324860\n2    74.508864\ndtype: float64\n\nFPR By Group: \n group\n1     9.265537\n2    14.308768\nName: false_positive, dtype: float64\n\nFNR By Group: \n group\n1    6.327684\n2    5.190963\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    55.564972\n2    56.132329\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nFrom what I could find on sklearn and online (this Medium article: https://medium.com/@myselfaman12345/c-and-gamma-in-svm-e6cee48626be) C - to put it generally - indicates how much we want to avoid misclassification on our training data. A low C value leads to low training error, and high C allows for more error on training data. However, minimizing C - and therefore training classification error - too far seems to lead to over fitting. As I dropped the C value below 1.0 (the default sklearn value) I found my classification accuracy begin to drop. Setting it too high led to the same effect. It seems that the “optimal” C value is context-dependent. Interestingly, my accuracy was very similary at a C value of 0.8 (82.35%) and 3.0 (82.41%). The default C value of 1.0 resulted in an 83.32% accuracy.\nExtract predicitions on all test sets modeled with each classifier"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#decision-tree",
    "href": "posts/Auditing Bias/index.html#decision-tree",
    "title": "Auditing Bias",
    "section": "Decision Tree",
    "text": "Decision Tree\n\n# Decision Tree Classifier\nmodel_sex_DT = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=12, min_samples_split=8))\nmodel_sex_DT.fit(X_train, y_train)\n\ny_hat_DT = model_sex_DT.predict(X_test)\n\n# Calculate Overall Values\nTP_DT = ((y_hat_DT == 1) & (y_test == 1)).sum()\nFP_DT = ((y_hat_DT == 1) & (y_test == 0)).sum()\nTN_DT = ((y_hat_DT == 0) & (y_test == 0)).sum()\nFN_DT = ((y_hat_DT == 0) & (y_test == 1)).sum()\n\nPPV_DT = TP_DT / (y_hat_DT == 1).sum()\nFPR_DT = FP_DT / (TP_DT + FP_DT)\nFNR_DT = FN_DT / (TP_DT + FN_DT)\n\nprint(f\"Accuracy for Decision Tree:{((y_hat_DT == y_test).mean())*100: .2f}%\")\n# No Parameters: 77.28%, With Parameters: 83.12% \nprint(f\"PPV for Decision Tree:{PPV_DT*100: .2f}%\")\nprint(f\"Overall FPR for Decision Tree:{FPR_DT*100: .2f}%\")\nprint(f\"Overall FNR for Decision Tree:{FNR_DT*100: .2f}%\")\n\nAccuracy for Decision Tree: 83.11%\nPPV for Decision Tree: 80.41%\nOverall FPR for Decision Tree: 19.59%\nOverall FNR for Decision Tree: 12.68%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_DT = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_DT[\"group\"] = group_test\ndf_test_DT[\"label\"] = y_test\ndf_test_DT[\"predicted_value\"] = y_hat_DT\ndf_test_DT[\"correct_prediction\"] = df_test_DT[\"predicted_value\"] == df_test_DT[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_DT[\"correct_prediction\"] = df_test_DT[\"predicted_value\"] == df_test_DT[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n {df_test_DT.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_DT[\"true_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 1)\ndf_test_DT[\"false_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_DT = df_test_DT.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_DT = df_test_DT.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_DT / (TP_per_group_DT + FP_per_group_DT)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_DT[\"false_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n {df_test_DT.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_DT[\"false_negative\"] = (df_test_DT[\"predicted_value\"] == 0) & (df_test_DT[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n {df_test_DT.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_DT = df_test_DT.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_DT = df_test_DT.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_DT = (predicted_positives_per_group_DT / per_group_total_DT) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_DT}\")\n\n\nAccuracy By Group: \n group\n1    85.084746\n2    81.226466\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    85.039370\n2    76.041667\ndtype: float64\n\nFPR By Group: \n group\n1     8.050847\n2    12.990855\nName: false_positive, dtype: float64\n\nFNR By Group: \n group\n1    6.864407\n2    5.782679\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    53.813559\n2    54.222700\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nThe Decision Tree Classifier achieved it’s highest accuracy of 83.12% with a max-depth of 12 and a min_samples_split - the minimum number of samples required to split an internal node - of 8. This classifier also ran the fasted of all the classifiers tested with parameters."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#random-forest",
    "href": "posts/Auditing Bias/index.html#random-forest",
    "title": "Auditing Bias",
    "section": "Random Forest",
    "text": "Random Forest\n\n# Random Forest Classifier\nmodel_sex_RF = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=400, max_depth=16))\nmodel_sex_RF.fit(X_train, y_train)\n\ny_hat_RF = model_sex_RF.predict(X_test)\n\n# Calculate Overall Values\nTP_RF = ((y_hat_RF == 1) & (y_test == 1)).sum()\nFP_RF = ((y_hat_RF == 1) & (y_test == 0)).sum()\nTN_RF = ((y_hat_RF == 0) & (y_test == 0)).sum()\nFN_RF = ((y_hat_RF == 0) & (y_test == 1)).sum()\n\nPPV_RF = TP_RF / (y_hat_RF == 1).sum()\nFPR_RF = FP_RF / (TP_RF + FP_RF)\nFNR_RF = FN_RF / (TP_RF + FN_RF)\n\nprint(f\"Accuracy for Random Forest:{((y_hat_RF == y_test).mean())*100: .2f}%\")\n# Without Parameters: 80.92%, With Parameters: 83.74%\nprint(f\"Overall PPV for Random Forest:{PPV_RF*100: .2f}%\")\nprint(f\"Overall FPR for Random Forest:{FPR_RF*100: .2f}%\")\nprint(f\"Overall FNR for Random Forest:{FNR_RF*100: .2f}%\")\n\n\nAccuracy for Random Forest: 83.70%\nOverall PPV for Random Forest: 80.81%\nOverall FPR for Random Forest: 19.19%\nOverall FNR for Random Forest: 11.82%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_RF = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_RF[\"group\"] = group_test\ndf_test_RF[\"label\"] = y_test\ndf_test_RF[\"predicted_value\"] = y_hat_RF\ndf_test_RF[\"correct_prediction\"] = df_test_RF[\"predicted_value\"] == df_test_RF[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_RF[\"correct_prediction\"] = df_test_RF[\"predicted_value\"] == df_test_RF[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n \\n {df_test_RF.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_RF[\"true_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_RF = df_test_RF.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_RF = df_test_RF.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_RF / (TP_per_group_RF + FP_per_group_RF)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n \\n {df_test_RF.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_RF[\"false_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n \\n {df_test_RF.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_RF = df_test_RF.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_RF = df_test_RF.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_RF = (predicted_positives_per_group_RF / per_group_total_RF) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_RF}\")\n\n\nAccuracy By Group: \n \n group\n1    85.706215\n2    81.791286\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    85.356957\n2    76.496784\ndtype: float64\n\nFPR By Group: \n \n group\n1     7.937853\n2    12.775686\nName: false_positive, dtype: float64\n\nFNR By Group: \n \n group\n1    6.355932\n2    5.433029\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    54.209040\n2    54.357181\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nWhile increasing max_depth to 16 itself led to the largest increase in classifier accuracy, adding more estimators - n_estimators set to 400 - to the model led to a further increase in accuracy from 83.55% to 83.74%. Random Forest achieved the highest prediction accuracy of all models tested."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#bias-measures",
    "href": "posts/Auditing Bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures:",
    "text": "Bias Measures:\n\nA model is consider well calibrated if it reflects the same likelihood of positive prediction irrespective an individuals’ group membership. In other words, the model should be free from predictive bias. It appears that none of the models are well-calibrated as across all of them, the positive prediction rate of male employment is several percentage points higher than female employment. More specifically, the PPV ranges from anywhere between ~8%-9% across all of the models.\nAcross the board, again, the models do not satisfy error rate balance. The False Positive Rate (FPR) is higher for females across all four, whereas the False Negative Rate (FNR) is higher for males across all four models. Even further, the margin of difference between groups for FPR is much greater than that for FNR across all Models. The FPR difference between groups is at its lowest ~4.56% in our LR model, whereas the largest difference in FNRs is ~1.36%, also found in our LR model.\nThe best performing fairness metric across our models is statistical parity, with no models differing more than .7% in acceptance rate (i.e. employment prediction) between male and females."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#plotting-feasible-fnr-and-fpr-rates",
    "href": "posts/Auditing Bias/index.html#plotting-feasible-fnr-and-fpr-rates",
    "title": "Auditing Bias",
    "section": "Plotting Feasible FNR and FPR Rates",
    "text": "Plotting Feasible FNR and FPR Rates\nUsing Random Forest Model because it had the best accuracy per group\n\nfrom matplotlib import pyplot as plt\n\nprevalence = df_test_RF.groupby(\"group\")[\"label\"].mean()\np_male = prevalence.loc[1]\np_female = prevalence.loc[2]\n\n# Recalculating FNR here to make continuity more clear/avoid using variables from other cells\n\n# Calculate true positives and false positives by group\ndf_test_RF[\"true_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0)\ndf_test_RF[\"false_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"true_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_RF = df_test_RF.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_RF = df_test_RF.groupby(\"group\")[\"false_positive\"].sum()\nFN_per_group_RF = df_test_RF.groupby(\"group\")[\"false_negative\"].sum()\nTN_per_group_RF = df_test_RF.groupby(\"group\")[\"true_negative\"].sum()\n\nPPV_per_group = TP_per_group_RF / (TP_per_group_RF + FP_per_group_RF)\nPPV_male = PPV_per_group.loc[1]\nPPV_female = PPV_per_group.loc[2]\n\n# Calculate FNR\nFNR = FN_per_group_RF / (FN_per_group_RF + TP_per_group_RF)\nFNR_male = FNR.loc[1]\nFNR_female = FNR.loc[2]\n\n# Calculate FPR\nFPR_male = (p_male / (1 - p_male)) * ((1 - PPV_male) / PPV_male) * (1 - FNR_male)\nFPR_female = (p_female / (1 - p_female)) * ((1 - PPV_female) / PPV_female) * (1 - FNR_female)\n\nfeasible_FNR_range = np.linspace(0, 1, 100)\n\nFPR_male_line = (p_male / (1 - p_male)) * ((1 - PPV_female) / PPV_female) * (1 - feasible_FNR_range)\nFPR_female_line = (p_female / (1 - p_female)) * ((1 - PPV_female) / PPV_female) * (1 - feasible_FNR_range)\n\nplt.figure(figsize=(8, 5))\n\nplt.plot(FNR_male, FPR_male, 'o', color='orange', label='Male')\nplt.plot(FNR_female, FPR_female, 'o', color='black', label='Female')\n\nplt.plot(feasible_FNR_range, FPR_male_line, '-', color='orange')\nplt.plot(feasible_FNR_range, FPR_female_line, '-', color='black')\n\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible FNRs and FPRs\")\nplt.legend()\n\n0.8535695674830641\n0.7649678377041069\n\n\n\n\n\n\n\n\n\nTuning the classifier threshold: Looking at the feasibility curves for males and females, if we set the threshold so that both groups have an FPR of 0.15, the male curve corresponds to an FNR of 0.25, while the female curve corresponds to an FNR of 0.40. Thus, to achieve the same FPR across groups, we must allow the female group’s FNR to increase from 0.25 to 0.40, a difference of 15%."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#concluding-discussion",
    "href": "posts/Auditing Bias/index.html#concluding-discussion",
    "title": "Auditing Bias",
    "section": "Concluding Discussion:",
    "text": "Concluding Discussion:\nWhat groups of people could stand to benefit?\nThis model predicts whether an individual is employed, so it could benefit recruiting agencies or HR departments seeking to identify jobseekers or allocate training resources - specifically understanding what feature groups may be searching for a job/have more possible unemployed candidates available to interview.\nImpact of deploying model?\nFrom the bias audit, we see differences in false positive and false negative rates across groups. If deployed widely, these disparities could systematically disadvantage certain populations—for instance, if a higher false negative rate leads to fewer recognized as “employed,” those individuals might miss job opportunities or loans. Conversely, higher false positives could unfairly label individuals as employed when they are not, whereas false negatives could mean that hiring/search resources are allocated to the wrong demographic areas, disadvantaging those who don’t fit the template of an employed/unemployed individual. Even more, the differences in the bias audit reveal the way in which employment data is a proxy for historical prejudice - and in my case, specifically gender-based hiring prejudice - which could lead to reinforced stereotypes of who is “employable” and who is not.\nDoes the Model Display Bias?\nFrom the bias audit (examining accuracy, PPV, FNR, and FPR by group), we see that the model’s performance differs between males and females for several fairness criterion. For example, it appears that none of the models are well-calibrated as across all we see the positive prediction rate of male employment is several percentage points higher than female employment. More specifically, the PPV ranges from anywhere between ~8%-9% across all of the models. There is also error rate imbalance: the False Positive Rate (FPR) is higher for females across all four, whereas the False Negative Rate (FNR) is higher for males across all four models. Even further, the margin of difference between groups for FPR is much greater than that for FNR across all Models. The FPR difference between groups is at its lowest ~4.56% in our LR model, whereas the largest difference in FNRs is ~1.36%, also found in our LR model. This could lead to prejudicial assumptions which have harmful effects on fair hiring practices. For instance, if one group has a statistically higher false negative rate, that group may be systematically overlooked for certain opportunities.\nFurther Concerns?\nThe model analyzed here is quite complex. Specifically, it involves the correlation/scoring of a large set of features, and classifier/modeling methods that are not very accessible. In other words, it isn’t a very transparent model, and that can lead to a lot of mistrust about how the decisions are being made and why. I also want to know more about where the data is being collected in a state, what areas/communities it is taken from, and how representative or random are those samples. To address these issues, we could continue ollect more diverse data, and promote education and transparency initiatives about Machine Learning. Furthermore, it is always important to include human oversight as a “second set of eyes” in decisions made about another person’s life from data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog Post 2: Design and Impact of Automated Decision Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]