[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Jiffy and this is my blog for CS451!"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#abstract",
    "href": "posts/Classifying Palmers Penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract:",
    "text": "Abstract:\nThis blog post is an introductory exploration into understanding the process and procedures of ML classifications and predictions. Using the Palmers Penguin dataset, I was able to identify a set three features - Culmen Length (mm), Culmen Depth (mm), and Clutch Completion (T/F) - which produced the highest Logistic Regression (LR) cross validation score across all feature combinations. From this, I tested a variety of potential classification models - such as SVC, Random Forest, and Decision Tree, alongside the original LR model - adjusting parameter options such as gamma and max-depth identifying the LR model as that which performed best at classifying the training data. Then, with an “optimal” feature set and classification set identified - insofar as my experimentation indicated it to be optimal - I evaluated my model by splitting it our over the qualitative feature of my feature set, and showed that it only misidentified 1 penguin in the test data using a confusion matrix.\n\n# Imports\nimport pandas as pd\nimport numpy as np\n\n# Access data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "href": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "title": "Classifying Palmer Penguins",
    "section": "Our Data: A First Look",
    "text": "Our Data: A First Look\nUsing the preprocessing code provided by Prof. Phil as a basis, I copied the training data, and subsequently remove/revise columns of our dataset to make it easily useable for visualization. Specifically, I leave the Species column in so that I can group our data points in a more accessible manner - given the skills in Pandas/Seaborn I currently have\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create DataFrame for visualization\nX_train_visualize = train.copy()\nX_train_visualize[\"Species\"] = X_train_visualize[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nX_train_visualize[\"species_label\"] = le.fit_transform(X_train_visualize[\"Species\"])\nX_train_visualize = X_train_visualize.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Island\", \"Stage\"], axis = 1)\nX_train_visualize = X_train_visualize[X_train_visualize[\"Sex\"] != \".\"]\nX_train_visualize = X_train_visualize.dropna()\n\nNow, let’s take a look at our DataFrame\n\nX_train_visualize\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nspecies_label\n\n\n\n\n0\nChinstrap\nYes\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\n1\n\n\n1\nChinstrap\nYes\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\n1\n\n\n2\nGentoo\nYes\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\n2\n\n\n3\nGentoo\nYes\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\n2\n\n\n4\nChinstrap\nYes\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nGentoo\nYes\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\n2\n\n\n271\nAdelie\nNo\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\n0\n\n\n272\nAdelie\nYes\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\n0\n\n\n273\nAdelie\nYes\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\n0\n\n\n274\nChinstrap\nYes\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\n1\n\n\n\n\n256 rows × 10 columns\n\n\n\nWith our DataFrame on hand, we now move forward to part 1 of the blog post: Exploration of the data\n\n# Import Seaborn & matplotlib for visualizations\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 1: Pair Plotting Penguin Quantitative Features",
    "text": "Figure 1: Pair Plotting Penguin Quantitative Features\nUsing Seaborn’s pairplot function, I can plot all quantitative feature-pair scatterplots. Though I can’t add a title directly to a seaborn pairplot, since it is built on top of matplotlib, I can access plot attributes/modules such as figure, which holds all plot elements and can be used to add a title. See here for more info on the figure module: https://matplotlib.org/stable/api/figure_api.html#module-matplotlib.figure\n\npairplot = sns.pairplot(X_train_visualize[[\"Culmen Length (mm)\", \n                                \"Culmen Depth (mm)\", \n                                \"Flipper Length (mm)\", \n                                \"Body Mass (g)\", \n                                \"Delta 15 N (o/oo)\", \n                                \"Delta 13 C (o/oo)\", \n                                \"Species\"]], \n                                hue = \"Species\", \n                                palette = \"tab10\",\n                                corner=True)\n\n\npairplot.figure.suptitle(\"Pairplot of All Quantitative Penguin Features and Corresponding Distributions\",\n                         fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\nDiscussion:\nAbove we have used the Seaborn pairplot function to plot pairwise relationships between all quantitative features in our penguins training data set. Using Pandas fancy indexing, I have selected only the quantitative data in order to A.) limit the number of pairs plotted, as including qualitative data could make an excessively large figure, and B.) when I tried with qualitative data, data-points became so clustered around the limited values for each of those features - i.e. they would cluster around True and False in linear blobs - which made it very hard to distinguish between entries. While this visualization doesn’t necessarily offer a clear choice for optimal features - those features that would be “best” to fit a model on - it can at least offer some insight into where the penguin features experience overlap across species. If you are working with larger data sets, this may be a helpful first step in allowing the Data Scientist to get a “functional sense” of which entries may be productive to focus processing on, and eliminate early on unhelpful feature combinations which could speed up cross-validation/feature selection later down the line."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 2: Looking for Potential Biases in our Data Set",
    "text": "Figure 2: Looking for Potential Biases in our Data Set\nGiven that the above figure provides a pretty comprehensive look at our feature pairs, I thought it would be helpful to turn towards penguin counts to see if there might be biases in our data set. In other words, are there equal data points for each penguin species we are classifying/predicting?\n\nsns.histplot(X_train_visualize, x = \"Species\", hue = \"Species\", shrink = .75)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWhile this may seem to be a fairly simple visualization, I’d argue that it is one of the most insightful. By showing the count distribution of our penguin species, we begin to see where biases lie in our data set. In the case of the penguins, this means seeing which species are more or less present in the training dataset. If there is a dramatic difference between our different species, this means that we might run the risk of building a prediction model which performs unevenly across new samples. For example, it may be more poorly trained on Chinstrap penguins since there is a far lower count - and thus fewer data points to fit a model on - which could lead to incorrect predictions on future Chinstrap Penguins whose features fall outside the decision boundaries of our model. As such, the we should be wary of the difference between Gentoo and Adelie counts to Chinstrap, and as such should maybe consider features in which Chinstrap penguins have a higher standard deviation - such as Culmen Length - relative to that of Adelie and Gentoo."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 3: Summary Table",
    "text": "Figure 3: Summary Table\nWith both of previous figures in mind, it might now be helpful to get quantified information about how our feature relate across species and to other features.\n\nX_train_visualize.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]].aggregate([\"std\", \"mean\"])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n2.685713\n38.961111\n1.218430\n18.380556\n6.652184\n190.527778\n462.850335\n3722.916667\n0.428454\n8.861431\n0.567815\n-25.808814\n\n\nChinstrap\n3.456257\n48.771429\n1.137935\n18.346429\n7.366033\n195.821429\n410.148997\n3739.732143\n0.370290\n9.331004\n0.224608\n-24.567075\n\n\nGentoo\n2.783242\n47.133696\n1.016336\n14.926087\n6.061715\n216.739130\n498.976123\n5057.336957\n0.282566\n8.252573\n0.561689\n-26.145754\n\n\n\n\n\n\n\n\nDiscussion:\nThe above summary table shows the standard deviation and mean of all quantitative features in our data set, across the three species of penguins. This is again offers important insight into the breadth of our data points - how much variation we might find in a feature for a specific penguin species - and how each species’ feature sets might relate to one another - that is how close is the average culmen length for an Adelie penguin vs. a Gentoo penguin. Features which a higher standard deviation indicates that the data set might cover a wider/more representative portion of the species population, which could mean stronger predictions, but it could also mean more overlap with other species on that feature. This is pretty relative to the mean of those features, of course, but nonetheless offers an important reminder that there isn’t one “ideal” standard deviation. Instead, we must always consider it as a statistical value relative to the goal of our model and the context of our dataset. Additionally, features with a similar mean across species could indicate overlap in the data set, meaning it may not be the most optimal feature to fit our model on.\nFrom this data - which is visualized by the corresponding distributions on the diagonal of Figure 1 - we can see there are some sections of our data with significant overlap across species. This means it would not be best to build a model on these feature combinations."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 4 (After the Fact)",
    "text": "Figure 4 (After the Fact)\nHaving noticed that a qualitative feature was used to build our model, I wanted to see if there was some way that I could have visualized the quantitative data before building the model to see which feature - Sex or Clutch Completion - would be more helpful than the other.\n\nfig, ax = plt.subplots(1, 2, figsize = (9, 5))\n\np1 = sns.countplot(X_train_visualize, x = \"Clutch Completion\", hue = \"Species\", ax = ax[0])\np1 = sns.countplot(X_train_visualize, x = \"Sex\", hue = \"Species\", ax = ax[1], legend=False)\n\n\n\n\n\n\n\n\n\nDiscussion: (Extra - not part of original three visualizations)\nAfter completing the blog post, I returned to the data exploration/visualization. I was curious about how - considering the fact that clutch completion made it to our “optimal” feature set for classification - we could/would be able to predict if a qualitative feature would be part of our model’s feature set? So, while I tried a variety of different plotting functions, this was the best or really most useful visualization I could come up with, a simple count plot. Considering the fact that Clutch Completion made it to the classifier, I don’t see anything that would indicate it to be a strong distinguishing feature across the data set. All penguins have far more successful clutch completions (Yes entries) than not (No entries). Ultimately, this shows that the power of our model isn’t in single feature classifications, but in being able to compute the relationship between features across our data set."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "href": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Building The Model",
    "text": "Building The Model\nMoving now to part two, we need to actually build our model using features from the data set\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nNow that I have pre-processed the data, it’s time to determine which features will be used to build our model. Here we take a bit of a long approach, iterating through a variety of feature combinations and seeing which set has the best cross-validation score when fit by a Logistic Regression model.\nBefore processing the data, I decided to scale the quantitative features in order to speed up the computations. This process is result of receiving consistent warnings about max_iterations, and code remaining too slow even when the max_iter parameter on my LogisticRegression() constructor was set to 10,000. The warning produced this link: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\nAs such, I have included the StandardScaler() object and scaled/transformed my data as per the instructions on the linked page.\n\n# Cross Validations\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n# All feature columns\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', \n                  'Culmen Depth (mm)', \n                  'Flipper Length (mm)', \n                  'Body Mass (g)', \n                  'Delta 15 N (o/oo)', \n                  'Delta 13 C (o/oo)']\n\n# Initialize array for optimal columns and variable to store cross-validation\n# score for the optimal columns\noptimal_columns = []\nbest_LR_cv_score = 0\n\n# Scale data to facilitate a clean cross-validation process\nscaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_train_scaled[['Culmen Length (mm)', \n                'Culmen Depth (mm)', \n                'Flipper Length (mm)', \n                'Body Mass (g)', \n                'Delta 15 N (o/oo)', \n                'Delta 13 C (o/oo)']] = scaler.fit_transform(X_train_scaled[['Culmen Length (mm)', \n                                                                              'Culmen Depth (mm)', \n                                                                              'Flipper Length (mm)', \n                                                                              'Body Mass (g)', \n                                                                              'Delta 15 N (o/oo)', \n                                                                              'Delta 13 C (o/oo)']])\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    LR = LogisticRegression()\n    cv_scores_LR = cross_val_score(LR, X_train_scaled, y_train, cv=5)\n    cv_scores_mean = cv_scores_LR.mean()\n    if cv_scores_mean &gt; best_LR_cv_score:\n      best_LR_cv_score = cv_scores_mean\n      optimal_columns = cols\n\nprint(f\"The highest Cross-Validation Score for Logistic Regression was {best_LR_cv_score} \\nIt was modeled on the following features: \\n{optimal_columns}\")\n\n\nThe highest Cross-Validation Score for Logistic Regression was 0.996078431372549 \nIt was modeled on the following features: \n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nHaving identified our “optimal columns” - i.e. those with the best cross-validation score - I now explore how those features score when fit with different models.\n\n# Cross Validation for SVC\nfrom sklearn.svm import SVC\n# Define range of gamma values (as suggested in blog post assignment)\ngamma_values = 10.0**np.arange(-10, 10)\n\nbest_svc_cv_score = 0\noptimal_gamma = 0.0\n\nfor gamma in gamma_values:\n    svc = SVC(gamma = gamma)\n    cv_scores_svc = cross_val_score(svc, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_svc.mean()\n    if cv_scores_mean &gt; best_svc_cv_score:\n        best_svc_cv_score = cv_scores_mean\n        optimal_gamma = gamma\n        \n\nprint(f\"The highest Cross-Validation Score for SVC was {best_svc_cv_score} \\nIt was calculated with a gamma of {optimal_gamma}\")\n\nThe highest Cross-Validation Score for SVC was 0.9491704374057315 \nIt was calculated with a gamma of 0.1\n\n\n\n# Cross Validation for Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_DT_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    DT = DecisionTreeClassifier(max_depth = depth)\n    cv_scores_DT = cross_val_score(DT, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_DT.mean()\n    if cv_scores_mean &gt; best_DT_cv_score:\n        best_DT_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Decision Tree was {best_DT_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Decision Tree was 0.9452488687782805 \nIt was calculated with a max depth of 9\n\n\n\n# Cross Validation for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_RF_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    RF = RandomForestClassifier(max_depth = depth)\n    cv_scores_RF = cross_val_score(RF, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_RF.mean()\n    if cv_scores_mean &gt; best_RF_cv_score:\n        best_RF_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Random Forest was {best_RF_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Random Forest was 0.9491704374057315 \nIt was calculated with a max depth of 9"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "href": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluate",
    "text": "Evaluate\nWith our features chosen as well as our model - Logistic Regression - it is time to evaluate and test our model to see how well it performed\n\nPlotting Decision Regions\n\n# Preprocessing: Scale my data, leaving qualitative features alone\nscaler = StandardScaler()\noptimal_X_train_scaled = X_train[optimal_columns].copy()\noptimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n# Swap qualitative and quantitative to fit function provided on assignment\nplottable_opt_x_train_scaled = optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]]\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nLR = LogisticRegression()\nLR.fit(plottable_opt_x_train_scaled, y_train)\nplot_regions(LR, plottable_opt_x_train_scaled, y_train)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "href": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Testing & Confusion Matrix",
    "text": "Testing & Confusion Matrix\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Preprocess\nscaler = StandardScaler()\nX_test = X_test[optimal_columns].copy()\nX_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n\n# Produce a Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(optimal_X_train_scaled, y_train)\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 1, 10,  0],\n       [ 0,  0, 26]])\n\n\n\n# Print what each value in the Confusion Matrix represents in the context of our data\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "href": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Final Discussion:",
    "text": "Final Discussion:\nThis blog post taught me that, in a way, constructing predictive models is not a purely objective science. While the operations and computations encountered along the way are fundamentally based on logic, the problem of prediction is deeply shaped by the context of the data and the data scientist. At the end of the day the limitations of hardware, software, and our implementations of the algorithmic world cannot - or at least have not yet - account for all the variability in the physical world: The realm of computational classification and prediction is but a finite set of instructions to put together a puzzle of infinite possibilities! Even though the model I coded scored well on the test data, my capacity to correctly predict future penguin species is still constrained: 1 penguin incorrectly predicted still means the model is not perfect. Furthermore, it was trained on much fewer Chinstrap penguins than Adelie or Gentoo, which means I might run a greater risk of mis-predicting Chinstrap penguins down the line. Even more, though it scored well on the test data, that is still only a fraction of penguins that there are out there to predict: there is not necessarily a guarantee that my model will continue to predict at such a high level. This isn’t to take away from the performance of the model, or the power of prediction, but more a matter of respecting the dynamism of the physical world. If I have taken anything away from the blog post, it’s that we must look to build models in such a way that respects this truth."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]