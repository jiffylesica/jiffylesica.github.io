[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Jiffy and this is my blog for CS451!"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html",
    "href": "posts/Implementing Perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "Link to Perceptron Implementation: Perceptron pyfile"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#abstract",
    "href": "posts/Implementing Perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "Abstract:",
    "text": "Abstract:\nThis blog post demonstrates an implementation and evaluation of the perceptron algorithm. It walks through the coding of a perceptron class, which inherits and extends functionality of a LinearModel class. This implementation includes the calculation of data sample score, and gradient descent updates to a weight vector. Following this implementation, the blog post validates the source code by performing experiments on both linearly separable and non-separable datasets. Additionally, it explores the effect of modifying the algorithm to allow for minibatch updates, and analyzes runtime complexities, offering a comprehensive view of how these factors impact convergence and overall performance.\n\nimport torch\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nIn the perceptron.py source code, I have implemented a Perceptron class. Of not is the gradient descent function - grad() - which performs gradient descent on the data contained in our instance of the Perceptron model:\nStep 1: Score Computation - The function computes a score for each data point in X by performing matrix multiplication between the feature matrix - X - and the weight vector - self.w.\nscore = X @ self.w\nStep 2: Update labels - since the perceptron update requires labels of -1 or 1, the function converts the binary labels y (currently 0 or 1) to these values.\ny_ = 2 * y - 1\nStep 3: Identify misclassifications - for each data point, the function checks if the product of the new label - -1 or 1 - and the data points score is negative. If it is negative, this means that there is a misclassification. Since the perceptron algorithm thresholds scores at 0 - i.e. positive scores are classified as positive predictions, and negative scores as negative predictions - a negative product between target value and score indicates a misalignment between the two.\nmisclassified = 1.0 * (y_ * score &lt; 0).float()\nStep 4: Reshaping for Multiplication - both our target value vector and misclassified vector are reshaped into column vectors to allow for multiplication in the perceptron update (technical code, not essential to math of function)\nStep 5: Compute the update - the update term - which is returned to the step function which actually updates the perceptron weight vector - is calculated as the mean over all datapoints of X which have been evaluated for misclassification, allowing us to subtract the gradient in the optimizer’s step function.\nreturn torch.mean((- (misclassified * y_) * X), dim=0)\nPut together, this results in the full grad() function:\ndef grad(self, X, y):\n        score = X @ self.w\n        y_ = 2 * y - 1\n        misclassified = 1.0 * (y_ * score &lt; 0).float()\n        # Had to reshape both to [300,1] tensors to multiply\n        # https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html\n        misclassified = misclassified.reshape(misclassified.shape[0], 1)\n        y_ = y_.reshape(y_.shape[0], 1)\n\n\n        # Return update\n        return torch.mean((- (misclassified * y_) * X), dim = 0)\n\nTensor shape exploration\nFirst, I set up small tensor examples to visualize their shapes throughout the empirical risk minimization process, and how we can reshape a tensor to ensure proper dimensions for matrix multiplication.\n\n# Code which helps visualize the dimensionality of tensors used in grad function\ntest_X = torch.tensor([\n    [2.0, -1.0, 0.5],\n    [1.0, -2.0, 1.5],\n    [3.0, 0.0, -0.5]\n])\nprint(\"test_X\")\nprint(test_X)\nprint(test_X.shape, end=\"\\n\\n\")\n\ntest_w = torch.tensor([0.5, 0.5, 0.5]) \nprint(\"test_w\")\nprint(test_w)\nprint(test_w.shape, end=\"\\n\\n\")\n\ntest_y = torch.tensor([1.0, 1.0, 0.0]) \nprint(\"test_y\")\nprint(test_y)\nprint(test_y.shape, end=\"\\n\\n\")\n\nscores = test_X @ test_y\nprint(\"scores\")\nprint(scores)\nprint(scores.shape, end=\"\\n\\n\")\n\ntest_y_ = 2 * test_y - 1  \nprint(\"test_y_\")\nprint(test_y_)\nprint(test_y_.shape, end=\"\\n\\n\")\n\nmisclassified = 1.0 * (test_y_ * scores &lt; 0).float()\nprint(\"misclassified\")\nprint(misclassified)\nprint(misclassified.shape, end=\"\\n\\n\")\n\nout_tensor = - (misclassified * test_y_) * test_X\nprint(\"out_tensor\")\nprint(out_tensor)\nprint(out_tensor.shape, end=\"\\n\\n\")\n\nmean_test = torch.mean(out_tensor, dim=0) # Seems dim = 0 is column-wise, dim = 1 is row\nprint(\"mean_test\")\nprint(mean_test)\nprint(mean_test.shape)\n\ntest_X\ntensor([[ 2.0000, -1.0000,  0.5000],\n        [ 1.0000, -2.0000,  1.5000],\n        [ 3.0000,  0.0000, -0.5000]])\ntorch.Size([3, 3])\n\ntest_w\ntensor([0.5000, 0.5000, 0.5000])\ntorch.Size([3])\n\ntest_y\ntensor([1., 1., 0.])\ntorch.Size([3])\n\nscores\ntensor([ 1., -1.,  3.])\ntorch.Size([3])\n\ntest_y_\ntensor([ 1.,  1., -1.])\ntorch.Size([3])\n\nmisclassified\ntensor([0., 1., 1.])\ntorch.Size([3])\n\nout_tensor\ntensor([[-0.0000,  1.0000,  0.5000],\n        [-0.0000,  2.0000,  1.5000],\n        [-0.0000, -0.0000, -0.5000]])\ntorch.Size([3, 3])\n\nmean_test\ntensor([0.0000, 1.0000, 0.5000])\ntorch.Size([3])\n\n\n\n\nHelper Functions\nBelow are a series of helper functions which will facilitate clean code/effective abstraction through this blog post.\n\nperceptron_data: creates set of labeled data points\nplot_perceptron_data: visualizes labeled date points using matplotlib’s pyplot\ndraw_line: plots decision boundary of a given weight vector (used in plots of data points after gradient descent). This function comes from Professor Phil Chodrow’s lecture notes for his Machine Learning course at Middlebury College.\n\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2): \n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nPerceptron Training Loop and Visualization\nThe below function - perceptron_loss_and_plot - trains the perceptron using the perceptron update rule. It logs the loss of the current weight vector after each iteration, and visualizes the final decision boundary (if the input data is 2-dimensional).\n\ndef perceptron_loss_and_plot(X, y, k_choice = 5):\n    # Generate 2d data\n\n    # Instantiate the model and optimizer\n    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n\n    # Training loop\n    loss_vec = []\n    # Set max iterations to avoid infinite loop in nonseparable cases\n    max_iterations = 1000 \n    iteration = 0\n    loss = p.loss(X, y)\n\n    while loss &gt; 0 and iteration &lt; max_iterations:\n        loss = p.loss(X, y)\n        loss_vec.append(loss.item())\n        k = k_choice\n        idx = torch.randperm(X.size(0))[:k]\n        X_pass = X[idx, :]\n        y_pass = y[idx]\n        opt.step(X_pass, y_pass)\n        iteration += 1\n\n    print(\"Final Loss:\", loss.item(), \"after\", iteration, \"iterations.\")\n\n    # Plot loss per point\n    plt.figure()\n    plt.plot(loss_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n    plt.xlabel(\"Perceptron Iteration (Updates Only)\")\n    plt.ylabel(\"loss\")\n    \n    if X.shape[1] &lt;= 3:\n        # Data with separating line\n        fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        plot_perceptron_data(X, y, ax)\n        draw_line(p.w, -1, 2, ax, color = \"black\")\n        plt.show()"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#experiments",
    "href": "posts/Implementing Perceptron/index.html#experiments",
    "title": "Implementing Perceptron",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Generate a linearly separable, 2D dataset, and show that the perceptron algorithm converges to weight vector tha describes a separating line (i.e. achieves loss of 0)\n\n'''\nMaintaining low noise - and thus low standard deviation of data - produces linearly separable data\n'''\n\nX, y = perceptron_data(n_points=300, noise=0.2, p_dims=2)\nperceptron_loss_and_plot(X, y)\n\nFinal Loss: 0.0 after 180 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow visualization comes from Prof. Phil Chodrow’s Machine Learning Course, Lecture 7 - Introduction to Classification: The Perceptron (Subsection: A Complete Run)\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\nn= X.shape[0]\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0 and current_ax &lt; 6:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nExperiment 2: Generate a non-linearly separable, 2D dataset, and show that the perceptron algorithm does not converge on a final weight vector (i.e. does not achieve loss of 0). Instead, it runs max iterations without achieving perfect accuracy.\n\n'''\nBy increasing the noise parameter, we increase the standard deviation of the data points of a class.\nThis leads to more overlap across elements with different target values, increasing the likelihood\nof non-linearly separable data\n'''\nX, y = perceptron_data(n_points=300, noise=0.4, p_dims=2)\nperceptron_loss_and_plot(X, y)\n\nFinal Loss: 0.03333333507180214 after 1000 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 3: Show that the perceptron algorithm can run on data with at least 5 features, and visualize the evolution of the loss score over the raining period.\n\n'''\nAdjusting p_dims parameters allows us to modify number of features in data. \np_dims = 5 corresponds to 5 features.\n'''\nX, y = perceptron_data(n_points=300, noise=0.4, p_dims=5)\nperceptron_loss_and_plot(X, y)\n\nFinal Loss: 0.0 after 46 iterations.\n\n\n\n\n\n\n\n\n\nNote: This 5-feature data is linearly separable as the loss reaches 0.0!"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#minibatch-perceptron-experiments",
    "href": "posts/Implementing Perceptron/index.html#minibatch-perceptron-experiments",
    "title": "Implementing Perceptron",
    "section": "Minibatch Perceptron Experiments:",
    "text": "Minibatch Perceptron Experiments:\n\nX, y = perceptron_data(n_points=300, noise=0.2, p_dims=2)\n\n\nMinibatch Experiment 1: Test with a single sample to show that minibatch behaves like standard perceptron\n\nperceptron_loss_and_plot(X, y, k_choice=1)\n\nFinal Loss: 0.0 after 24 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen passing a batch size of 1 sample to the minibatch version of our algorithm, our implementation still behaves like a standard perceptron, performing gradient descent over one data point at a time.\n\n\nMinibatch Experiment 1: Test with a multiple samples - a small batch - to show that minibatch can still find separating line on 2D data\n\nperceptron_loss_and_plot(X, y, k_choice=10)\n\nFinal Loss: 0.0 after 9 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEven though each step of the perceptron algorithm performs computations on multiple data points, the minibatch algorithm can still find a separating line on linearly separable data, as shown by it achieving a final loss of 0.0\n\n\nMinibatch Experiment 1: Test with full dataset as batch - size of the batch is the number of samples in dataset - to show that minibatch can still converge even when data isn’t perfectly separable.\n\nX, y = perceptron_data(n_points=300, noise=0.4, p_dims=2)\nperceptron_loss_and_plot(X, y, k_choice=X.shape[0])\n\nFinal Loss: 0.02666666731238365 after 1000 iterations."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#runtime-complexity-of-standard-perceptron-single-iteration",
    "href": "posts/Implementing Perceptron/index.html#runtime-complexity-of-standard-perceptron-single-iteration",
    "title": "Implementing Perceptron",
    "section": "Runtime complexity of standard perceptron single iteration",
    "text": "Runtime complexity of standard perceptron single iteration\nA single iteration of the standard perceptron algorithm performs operations on only a single data point. So, runtime complexity will be dependent on the number of features which it is matrix multiplied with during score calculation and the update of the weight vector. Thus, for an input with k features, the standard perceptron single iteration runtime is O(k)."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#runtime-of-minibatch-perceptron",
    "href": "posts/Implementing Perceptron/index.html#runtime-of-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Runtime of Minibatch perceptron",
    "text": "Runtime of Minibatch perceptron\nThe runtime of the Minibatch perceptron has an upper bound complexity limit of O(n x k), where n is the number of samples operated on in an iteration, and k is again the number of features. Since we have shown in our experiments above that the minibatch can still converge when the batch size equals the number of samples in a data set of length n, n x p is the upper bounded complexity of a single iteration of the Minbatch algorithm."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#conclusion",
    "href": "posts/Implementing Perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn summary, the experiments confirm that the perceptron algorithm correctly converges on linearly separable data while revealing its limitations on non-linearly separable datasets. The analysis of runtime complexities shows that a standard perceptron iteration operates in O(k) time, while a minibatch implementation of the perceptron algorithm has an upper complexity bound of O(n × k). This exploration provides insights into how feature/weight-vector optimization can enhance our ability to classify data."
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Goals: 1. Train a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including gender, and 2. Perform a bias audit of our algorithm to determine whether it displays gender bias.\nIn this blog post, I trained a machine learning classifier on PUMS data from the state of Connecticut to predict an individual’s employment status based on demographic features. After preparing the data with the folktables library, I experimented with a various classifier models, ultimately deciding on a Random Forest model whose performance I evaluated using metrics like accuracy, Positive Prediction Value, and False Positive/Negative Rates. I then conducted a bias audit by examining false positive rates (FPR), false negative rates (FNR), and positive predictive value (PPV) across the gender groups available in the data - Male and Female. I then plotted feasible FNRs & FPRs to visualize the relationship of error rates between groups, and understand how much one group’s error rate would have to change to match the others. Our results showed notable differences in error rates and PPVs between groups, highlighting areas where the model may inadvertently misclassify women more frequently.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"CT\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000191\n1\n1\n302\n1\n9\n1013097\n40\n90\n...\n82\n39\n38\n81\n0\n38\n0\n0\n36\n34\n\n\n1\nP\n2018GQ0000285\n1\n1\n101\n1\n9\n1013097\n70\n18\n...\n69\n134\n7\n70\n6\n68\n141\n68\n132\n145\n\n\n2\nP\n2018GQ0000328\n1\n1\n1101\n1\n9\n1013097\n17\n54\n...\n37\n35\n16\n16\n0\n17\n18\n19\n18\n16\n\n\n3\nP\n2018GQ0000360\n1\n1\n905\n1\n9\n1013097\n47\n18\n...\n46\n48\n4\n90\n87\n84\n90\n3\n47\n48\n\n\n4\nP\n2018GQ0000428\n1\n1\n903\n1\n9\n1013097\n35\n96\n...\n32\n35\n36\n71\n36\n3\n37\n2\n2\n35\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n90\n16.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n2\n1\n6.0\n\n\n1\n18\n16.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n17.0\n5\n17\n1\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n3\n18\n19.0\n5\n17\n2\nNaN\n3\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n4\n96\n16.0\n2\n16\n1\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n1.0\n2\n1\n6.0"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#my-version",
    "href": "posts/Auditing Bias/index.html#my-version",
    "title": "Auditing Bias",
    "section": "My Version",
    "text": "My Version\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n\nfeatures_to_use1 = [f for f in possible_features if f not in [\"ESR\", \"SEX\"]]\n\n\nEmploymentProblemSex = BasicProblem(\n    features=features_to_use1,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblemSex.df_to_numpy(acs_data)\n\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nConvert Folktables data back to Pandas DataFrame for ease of descriptive analysis\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use1)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\n1. How many individuals are in the data?\n\nprint(f\"There are {df.shape[0]} individuals in the data for the state of CT\")\n\nThere are 29029 individuals in the data for the state of CT\n\n\n\n\n2. How Many Individuals Have a Target Label == 1 (i.e. How Many Individuals are Employed)?\n\nprint(f\"Of the {df.shape[0]} in the data set, {(df['label'].mean() * 100):.2f}% - or {df['label'].sum()} individuals - are employed\")\n\nOf the 29029 in the data set, 48.43% - or 14060 individuals - are employed\n\n\n\n\n3. Of the Employed Individuals, How Many are Male (1) and How Many are Female (2)?\n\nemployed_by_group = df.groupby(\"group\")[\"label\"].sum()\nprint(f\"Of the 14060 employed individuals, {employed_by_group.iloc[0]} are male and {employed_by_group.iloc[1]} are female\")\n\nOf the 14060 employed individuals, 7169 are male and 6891 are female\n\n\n\n\n4. In Each Group, What Proportion of Individuals Have Target Label Equal to 1 (i.e. Are Employed)?\n\nproportion_employed_by_group = df.groupby(\"group\")[\"label\"].mean()\nprint(f\"According to the Data,{proportion_employed_by_group.iloc[0]*100: .2f}% of male individuals are employed and{proportion_employed_by_group.iloc[1]*100: .2f}% female individuals are employed\")\n\nAccording to the Data, 51.17% of male individuals are employed and 45.88% female individuals are employed\n\n\n\n\n5. Intersectional Trends\n\n# Since RAC1P Has Values &gt; 2, we must filter to only 1.0 and 2.0\ndf_filtered = df[(df[\"RAC1P\"] == 1.0) | (df[\"RAC1P\"] == 2.0)].copy()\n# Now, we convert RAC1P values to ints to match type\ndf_filtered[\"RAC1P\"] = df_filtered[\"RAC1P\"].astype(int)\n# Since I want to use categorical labels, and not just numbered labels, we use Pandas map function on a series\n# https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html\ndf_filtered[\"group\"] = df_filtered[\"group\"].map({1 : \"Male\", 2 : \"Female\"})\ndf_filtered[\"RAC1P\"] = df_filtered[\"RAC1P\"].map({1 : \"White\", 2 : \"Black\"})\nproportion_employed_by_group = df_filtered.groupby([\"group\", \"RAC1P\"])[\"label\"].mean()\n\ngroup   RAC1P\nFemale  Black    0.466667\n        White    0.464758\nMale    Black    0.371805\n        White    0.535252\nName: label, dtype: float64\n\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Using seaborn barplot uses mean as default, which represents the proportional insights we are looking for\nsns.barplot(df_filtered, x = \"group\", y = \"label\", hue = \"RAC1P\", width=.8, gap=.2)\nplt.title(\"Intersectional Trends\")\n\nText(0.5, 1.0, 'Intersectional Trends')\n\n\n\n\n\n\n\n\n\nTraining my model with different classifiers\n\n# Include alternative classifiers from sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#logistic-regression",
    "href": "posts/Auditing Bias/index.html#logistic-regression",
    "title": "Auditing Bias",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n# Logistic Regression Classifier (playing with polynomial features)\n# Discovered we can ass PolynomialFeatures to pipeline!\nfrom sklearn.preprocessing import PolynomialFeatures\n# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\nmodel_sex_LR = make_pipeline(PolynomialFeatures(degree = 2), StandardScaler(), LogisticRegression(max_iter=1000))\nmodel_sex_LR.fit(X_train.copy(), y_train)\n\ny_hat_LR = model_sex_LR.predict(X_test)\n\n# Calculate Overall Values\nTP_LR = ((y_hat_LR == 1) & (y_test == 1)).sum()\nFP_LR = ((y_hat_LR == 1) & (y_test == 0)).sum()\nTN_LR = ((y_hat_LR == 0) & (y_test == 0)).sum()\nFN_LR = ((y_hat_LR == 0) & (y_test == 1)).sum()\n\nPPV_LR = TP_LR / (y_hat_LR == 1).sum()\nFPR_LR = FP_LR / (TP_LR + FP_LR)\nFNR_LR = FN_LR / (TP_LR + FN_LR)\n\nprint(f\"Accuracy for Logistic Regression:{((y_hat_LR == y_test).mean())*100: .2f}%\")\nprint(f\"PPV for Logistic Regression:{PPV_LR*100: .2f}%\")\nprint(f\"Overall FPR for LR:{FPR_LR*100: .2f}%\")\nprint(f\"Overall FNR for LR:{FNR_LR*100: .2f}%\")\n\nAccuracy for Logistic Regression: 82.57%\nPPV for Logistic Regression: 79.65%\nOverall FPR for LR: 20.35%\nOverall FNR for LR: 12.74%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_LR = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_LR[\"group\"] = group_test\ndf_test_LR[\"label\"] = y_test\ndf_test_LR[\"predicted_value\"] = y_hat_LR\ndf_test_LR[\"correct_prediction\"] = df_test_LR[\"predicted_value\"] == df_test_LR[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_LR[\"correct_prediction\"] = df_test_LR[\"predicted_value\"] == df_test_LR[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n \\n {df_test_LR.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_LR[\"true_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 1)\ndf_test_LR[\"false_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_LR = df_test_LR.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_LR = df_test_LR.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_LR / (TP_per_group_LR + FP_per_group_LR)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_LR[\"false_positive\"] = (df_test_LR[\"predicted_value\"] == 1) & (df_test_LR[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n \\n {df_test_LR.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_LR[\"false_negative\"] = (df_test_LR[\"predicted_value\"] == 0) & (df_test_LR[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n \\n {df_test_LR.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_LR = df_test_LR.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_LR = df_test_LR.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_LR = (predicted_positives_per_group_LR / per_group_total_LR) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_LR}%\")\n\n\n\nAccuracy By Group: \n \n group\n1    84.209040\n2    81.011296\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    83.887734\n2    75.639764\ndtype: float64\n\nFPR By Group: \n \n group\n1     8.757062\n2    13.313609\nName: false_positive, dtype: float64\n\nFNR By Group: \n \n group\n1    7.033898\n2    5.675094\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    54.350282\n2    54.653039\nName: predicted_value, dtype: float64%\n\n\n\nDiscussion:\nBy implementing the PolynomialFeatures preprocessing function/module, I was able to add polynomial feature adjustments directly into the model pipeline. By calculating all polynomial combinations of my features with a degree of 2, I achieved my best accuracy of 82.57%."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#svc",
    "href": "posts/Auditing Bias/index.html#svc",
    "title": "Auditing Bias",
    "section": "SVC",
    "text": "SVC\n\n# SVC Classifier\nmodel_sex_SVC = make_pipeline(StandardScaler(), SVC(C = 3.0))\nmodel_sex_SVC.fit(X_train, y_train)\n\ny_hat_SVC = model_sex_SVC.predict(X_test)\n\n# Calculate Overall Values\nTP_SVC = ((y_hat_SVC == 1) & (y_test == 1)).sum()\nFP_SVC = ((y_hat_SVC == 1) & (y_test == 0)).sum()\nTN_SVC = ((y_hat_SVC == 0) & (y_test == 0)).sum()\nFN_SVC = ((y_hat_SVC == 0) & (y_test == 1)).sum()\n\nPPV_SVC = TP_SVC / (y_hat_SVC == 1).sum()\nFPR_SVC = FP_SVC / (TP_SVC + FP_SVC)\nFNR_SVC = FN_SVC / (TP_SVC + FN_SVC)\n\nprint(f\"Accuracy for SVC:{((y_hat_SVC == y_test).mean())*100: .2f}%\")\nprint(f\"PPV for SVC:{PPV_SVC*100: .2f}%\")\nprint(f\"Overall FPR for SVC:{FPR_SVC*100: .2f}%\")\nprint(f\"Overall FNR for SVC:{FNR_SVC*100: .2f}%\")\n\nAccuracy for SVC: 82.41%\nPPV for SVC: 78.79%\nOverall FPR for SVC: 21.21%\nOverall FNR for SVC: 11.55%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_SVC = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_SVC[\"group\"] = group_test\ndf_test_SVC[\"label\"] = y_test\ndf_test_SVC[\"predicted_value\"] = y_hat_SVC\ndf_test_SVC[\"correct_prediction\"] = df_test_SVC[\"predicted_value\"] == df_test_SVC[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_SVC[\"correct_prediction\"] = df_test_SVC[\"predicted_value\"] == df_test_SVC[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n {df_test_SVC.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_SVC[\"true_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 1)\ndf_test_SVC[\"false_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_SVC = df_test_SVC.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_SVC = df_test_SVC.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_SVC / (TP_per_group_SVC + FP_per_group_SVC)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_SVC[\"false_positive\"] = (df_test_SVC[\"predicted_value\"] == 1) & (df_test_SVC[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n {df_test_SVC.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_SVC[\"false_negative\"] = (df_test_SVC[\"predicted_value\"] == 0) & (df_test_SVC[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n {df_test_SVC.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_SVC = df_test_SVC.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_SVC = df_test_SVC.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_SVC = (predicted_positives_per_group_SVC / per_group_total_SVC) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_SVC}\")\n\n\nAccuracy By Group: \n group\n1    84.406780\n2    80.500269\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    83.324860\n2    74.508864\ndtype: float64\n\nFPR By Group: \n group\n1     9.265537\n2    14.308768\nName: false_positive, dtype: float64\n\nFNR By Group: \n group\n1    6.327684\n2    5.190963\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    55.564972\n2    56.132329\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nFrom what I could find on sklearn and online (this Medium article: https://medium.com/@myselfaman12345/c-and-gamma-in-svm-e6cee48626be) C - to put it generally - indicates how much we want to avoid misclassification on our training data. A low C value leads to low training error, and high C allows for more error on training data. However, minimizing C - and therefore training classification error - too far seems to lead to over fitting. As I dropped the C value below 1.0 (the default sklearn value) I found my classification accuracy begin to drop. Setting it too high led to the same effect. It seems that the “optimal” C value is context-dependent. Interestingly, my accuracy was very similary at a C value of 0.8 (82.35%) and 3.0 (82.41%). The default C value of 1.0 resulted in an 83.32% accuracy.\nExtract predicitions on all test sets modeled with each classifier"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#decision-tree",
    "href": "posts/Auditing Bias/index.html#decision-tree",
    "title": "Auditing Bias",
    "section": "Decision Tree",
    "text": "Decision Tree\n\n# Decision Tree Classifier\nmodel_sex_DT = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=12, min_samples_split=8))\nmodel_sex_DT.fit(X_train, y_train)\n\ny_hat_DT = model_sex_DT.predict(X_test)\n\n# Calculate Overall Values\nTP_DT = ((y_hat_DT == 1) & (y_test == 1)).sum()\nFP_DT = ((y_hat_DT == 1) & (y_test == 0)).sum()\nTN_DT = ((y_hat_DT == 0) & (y_test == 0)).sum()\nFN_DT = ((y_hat_DT == 0) & (y_test == 1)).sum()\n\nPPV_DT = TP_DT / (y_hat_DT == 1).sum()\nFPR_DT = FP_DT / (TP_DT + FP_DT)\nFNR_DT = FN_DT / (TP_DT + FN_DT)\n\nprint(f\"Accuracy for Decision Tree:{((y_hat_DT == y_test).mean())*100: .2f}%\")\n# No Parameters: 77.28%, With Parameters: 83.12% \nprint(f\"PPV for Decision Tree:{PPV_DT*100: .2f}%\")\nprint(f\"Overall FPR for Decision Tree:{FPR_DT*100: .2f}%\")\nprint(f\"Overall FNR for Decision Tree:{FNR_DT*100: .2f}%\")\n\nAccuracy for Decision Tree: 83.11%\nPPV for Decision Tree: 80.41%\nOverall FPR for Decision Tree: 19.59%\nOverall FNR for Decision Tree: 12.68%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_DT = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_DT[\"group\"] = group_test\ndf_test_DT[\"label\"] = y_test\ndf_test_DT[\"predicted_value\"] = y_hat_DT\ndf_test_DT[\"correct_prediction\"] = df_test_DT[\"predicted_value\"] == df_test_DT[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_DT[\"correct_prediction\"] = df_test_DT[\"predicted_value\"] == df_test_DT[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n {df_test_DT.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_DT[\"true_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 1)\ndf_test_DT[\"false_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_DT = df_test_DT.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_DT = df_test_DT.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_DT / (TP_per_group_DT + FP_per_group_DT)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_DT[\"false_positive\"] = (df_test_DT[\"predicted_value\"] == 1) & (df_test_DT[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n {df_test_DT.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_DT[\"false_negative\"] = (df_test_DT[\"predicted_value\"] == 0) & (df_test_DT[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n {df_test_DT.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_DT = df_test_DT.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_DT = df_test_DT.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_DT = (predicted_positives_per_group_DT / per_group_total_DT) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_DT}\")\n\n\nAccuracy By Group: \n group\n1    85.084746\n2    81.226466\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    85.039370\n2    76.041667\ndtype: float64\n\nFPR By Group: \n group\n1     8.050847\n2    12.990855\nName: false_positive, dtype: float64\n\nFNR By Group: \n group\n1    6.864407\n2    5.782679\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    53.813559\n2    54.222700\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nThe Decision Tree Classifier achieved it’s highest accuracy of 83.12% with a max-depth of 12 and a min_samples_split - the minimum number of samples required to split an internal node - of 8. This classifier also ran the fasted of all the classifiers tested with parameters."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#random-forest",
    "href": "posts/Auditing Bias/index.html#random-forest",
    "title": "Auditing Bias",
    "section": "Random Forest",
    "text": "Random Forest\n\n# Random Forest Classifier\nmodel_sex_RF = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=400, max_depth=16))\nmodel_sex_RF.fit(X_train, y_train)\n\ny_hat_RF = model_sex_RF.predict(X_test)\n\n# Calculate Overall Values\nTP_RF = ((y_hat_RF == 1) & (y_test == 1)).sum()\nFP_RF = ((y_hat_RF == 1) & (y_test == 0)).sum()\nTN_RF = ((y_hat_RF == 0) & (y_test == 0)).sum()\nFN_RF = ((y_hat_RF == 0) & (y_test == 1)).sum()\n\nPPV_RF = TP_RF / (y_hat_RF == 1).sum()\nFPR_RF = FP_RF / (TP_RF + FP_RF)\nFNR_RF = FN_RF / (TP_RF + FN_RF)\n\nprint(f\"Accuracy for Random Forest:{((y_hat_RF == y_test).mean())*100: .2f}%\")\n# Without Parameters: 80.92%, With Parameters: 83.74%\nprint(f\"Overall PPV for Random Forest:{PPV_RF*100: .2f}%\")\nprint(f\"Overall FPR for Random Forest:{FPR_RF*100: .2f}%\")\nprint(f\"Overall FNR for Random Forest:{FNR_RF*100: .2f}%\")\n\n\nAccuracy for Random Forest: 83.70%\nOverall PPV for Random Forest: 80.81%\nOverall FPR for Random Forest: 19.19%\nOverall FNR for Random Forest: 11.82%\n\n\n\n# Breaking down by subgroups is much easier if we use a Dataframe\ndf_test_RF = pd.DataFrame(X_test, columns = features_to_use1)\ndf_test_RF[\"group\"] = group_test\ndf_test_RF[\"label\"] = y_test\ndf_test_RF[\"predicted_value\"] = y_hat_RF\ndf_test_RF[\"correct_prediction\"] = df_test_RF[\"predicted_value\"] == df_test_RF[\"label\"]\n\n# Calculate correct prediction by group\ndf_test_RF[\"correct_prediction\"] = df_test_RF[\"predicted_value\"] == df_test_RF[\"label\"]\nprint(f\"\\nAccuracy By Group: \\n \\n {df_test_RF.groupby(['group'])['correct_prediction'].mean()*100}\")\n\n# Calculate true positives and false positives by group\ndf_test_RF[\"true_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_RF = df_test_RF.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_RF = df_test_RF.groupby(\"group\")[\"false_positive\"].sum()\nPPV_per_group = TP_per_group_RF / (TP_per_group_RF + FP_per_group_RF)\nprint(f\"\\nPPV By Group: \\n {PPV_per_group * 100}\")\n\n# Calculate false positives by group\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0) \nprint(f\"\\nFPR By Group: \\n \\n {df_test_RF.groupby(['group'])['false_positive'].mean()*100}\")\n\n# Calculate false negatives by group\ndf_test_RF[\"false_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 1) \nprint(f\"\\nFNR By Group: \\n \\n {df_test_RF.groupby(['group'])['false_negative'].mean()*100}\")\n\n# Compute for statistical parity by group\n\n# Total individuals per group\nper_group_total_RF = df_test_RF.groupby(\"group\")[\"predicted_value\"].count()\n\n# Total predicted positives per group\npredicted_positives_per_group_RF = df_test_RF.groupby(\"group\")[\"predicted_value\"].sum()\n\n# Calculate acceptance rate\nstatistical_parity_by_group_RF = (predicted_positives_per_group_RF / per_group_total_RF) * 100\nprint(f\"\\nAcceptance Rate (Employment) By Group: \\n {statistical_parity_by_group_RF}\")\n\n\nAccuracy By Group: \n \n group\n1    85.706215\n2    81.791286\nName: correct_prediction, dtype: float64\n\nPPV By Group: \n group\n1    85.356957\n2    76.496784\ndtype: float64\n\nFPR By Group: \n \n group\n1     7.937853\n2    12.775686\nName: false_positive, dtype: float64\n\nFNR By Group: \n \n group\n1    6.355932\n2    5.433029\nName: false_negative, dtype: float64\n\nAcceptance Rate (Employment) By Group: \n group\n1    54.209040\n2    54.357181\nName: predicted_value, dtype: float64\n\n\n\nDiscussion:\nWhile increasing max_depth to 16 itself led to the largest increase in classifier accuracy, adding more estimators - n_estimators set to 400 - to the model led to a further increase in accuracy from 83.55% to 83.74%. Random Forest achieved the highest prediction accuracy of all models tested."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#bias-measures",
    "href": "posts/Auditing Bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures:",
    "text": "Bias Measures:\n\nA model is consider well calibrated if it reflects the same likelihood of positive prediction irrespective an individuals’ group membership. In other words, the model should be free from predictive bias. It appears that none of the models are well-calibrated as across all of them, the positive prediction rate of male employment is several percentage points higher than female employment. More specifically, the PPV ranges from anywhere between ~8%-9% across all of the models.\nAcross the board, again, the models do not satisfy error rate balance. The False Positive Rate (FPR) is higher for females across all four, whereas the False Negative Rate (FNR) is higher for males across all four models. Even further, the margin of difference between groups for FPR is much greater than that for FNR across all Models. The FPR difference between groups is at its lowest ~4.56% in our LR model, whereas the largest difference in FNRs is ~1.36%, also found in our LR model.\nThe best performing fairness metric across our models is statistical parity, with no models differing more than .7% in acceptance rate (i.e. employment prediction) between male and females."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#plotting-feasible-fnr-and-fpr-rates",
    "href": "posts/Auditing Bias/index.html#plotting-feasible-fnr-and-fpr-rates",
    "title": "Auditing Bias",
    "section": "Plotting Feasible FNR and FPR Rates",
    "text": "Plotting Feasible FNR and FPR Rates\nUsing Random Forest Model because it had the best accuracy per group\n\nfrom matplotlib import pyplot as plt\n\nprevalence = df_test_RF.groupby(\"group\")[\"label\"].mean()\np_male = prevalence.loc[1]\np_female = prevalence.loc[2]\n\n# Recalculating FNR here to make continuity more clear/avoid using variables from other cells\n\n# Calculate true positives and false positives by group\ndf_test_RF[\"true_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"false_positive\"] = (df_test_RF[\"predicted_value\"] == 1) & (df_test_RF[\"label\"] == 0)\ndf_test_RF[\"false_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 1)\ndf_test_RF[\"true_negative\"] = (df_test_RF[\"predicted_value\"] == 0) & (df_test_RF[\"label\"] == 0)\n\n# Calculate PPV by group\nTP_per_group_RF = df_test_RF.groupby(\"group\")[\"true_positive\"].sum()\nFP_per_group_RF = df_test_RF.groupby(\"group\")[\"false_positive\"].sum()\nFN_per_group_RF = df_test_RF.groupby(\"group\")[\"false_negative\"].sum()\nTN_per_group_RF = df_test_RF.groupby(\"group\")[\"true_negative\"].sum()\n\nPPV_per_group = TP_per_group_RF / (TP_per_group_RF + FP_per_group_RF)\nPPV_male = PPV_per_group.loc[1]\nPPV_female = PPV_per_group.loc[2]\n\n# Calculate FNR\nFNR = FN_per_group_RF / (FN_per_group_RF + TP_per_group_RF)\nFNR_male = FNR.loc[1]\nFNR_female = FNR.loc[2]\n\n# Calculate FPR\nFPR_male = (p_male / (1 - p_male)) * ((1 - PPV_male) / PPV_male) * (1 - FNR_male)\nFPR_female = (p_female / (1 - p_female)) * ((1 - PPV_female) / PPV_female) * (1 - FNR_female)\n\nfeasible_FNR_range = np.linspace(0, 1, 100)\n\nFPR_male_line = (p_male / (1 - p_male)) * ((1 - PPV_female) / PPV_female) * (1 - feasible_FNR_range)\nFPR_female_line = (p_female / (1 - p_female)) * ((1 - PPV_female) / PPV_female) * (1 - feasible_FNR_range)\n\nplt.figure(figsize=(8, 5))\n\nplt.plot(FNR_male, FPR_male, 'o', color='orange', label='Male')\nplt.plot(FNR_female, FPR_female, 'o', color='black', label='Female')\n\nplt.plot(feasible_FNR_range, FPR_male_line, '-', color='orange')\nplt.plot(feasible_FNR_range, FPR_female_line, '-', color='black')\n\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible FNRs and FPRs\")\nplt.legend()\n\n0.8535695674830641\n0.7649678377041069\n\n\n\n\n\n\n\n\n\nTuning the classifier threshold: Looking at the feasibility curves for males and females, if we set the threshold so that both groups have an FPR of 0.15, the male curve corresponds to an FNR of 0.25, while the female curve corresponds to an FNR of 0.40. Thus, to achieve the same FPR across groups, we must allow the female group’s FNR to increase from 0.25 to 0.40, a difference of 15%."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#concluding-discussion",
    "href": "posts/Auditing Bias/index.html#concluding-discussion",
    "title": "Auditing Bias",
    "section": "Concluding Discussion:",
    "text": "Concluding Discussion:\nWhat groups of people could stand to benefit?\nThis model predicts whether an individual is employed, so it could benefit recruiting agencies or HR departments seeking to identify jobseekers or allocate training resources - specifically understanding what feature groups may be searching for a job/have more possible unemployed candidates available to interview.\nImpact of deploying model?\nFrom the bias audit, we see differences in false positive and false negative rates across groups. If deployed widely, these disparities could systematically disadvantage certain populations—for instance, if a higher false negative rate leads to fewer recognized as “employed,” those individuals might miss job opportunities or loans. Conversely, higher false positives could unfairly label individuals as employed when they are not, whereas false negatives could mean that hiring/search resources are allocated to the wrong demographic areas, disadvantaging those who don’t fit the template of an employed/unemployed individual. Even more, the differences in the bias audit reveal the way in which employment data is a proxy for historical prejudice - and in my case, specifically gender-based hiring prejudice - which could lead to reinforced stereotypes of who is “employable” and who is not.\nDoes the Model Display Bias?\nFrom the bias audit (examining accuracy, PPV, FNR, and FPR by group), we see that the model’s performance differs between males and females for several fairness criterion. For example, it appears that none of the models are well-calibrated as across all we see the positive prediction rate of male employment is several percentage points higher than female employment. More specifically, the PPV ranges from anywhere between ~8%-9% across all of the models. There is also error rate imbalance: the False Positive Rate (FPR) is higher for females across all four, whereas the False Negative Rate (FNR) is higher for males across all four models. Even further, the margin of difference between groups for FPR is much greater than that for FNR across all Models. The FPR difference between groups is at its lowest ~4.56% in our LR model, whereas the largest difference in FNRs is ~1.36%, also found in our LR model. This could lead to prejudicial assumptions which have harmful effects on fair hiring practices. For instance, if one group has a statistically higher false negative rate, that group may be systematically overlooked for certain opportunities.\nFurther Concerns?\nThe model analyzed here is quite complex. Specifically, it involves the correlation/scoring of a large set of features, and classifier/modeling methods that are not very accessible. In other words, it isn’t a very transparent model, and that can lead to a lot of mistrust about how the decisions are being made and why. I also want to know more about where the data is being collected in a state, what areas/communities it is taken from, and how representative or random are those samples. To address these issues, we could continue ollect more diverse data, and promote education and transparency initiatives about Machine Learning. Furthermore, it is always important to include human oversight as a “second set of eyes” in decisions made about another person’s life from data."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Introduction:",
    "text": "Introduction:\nThe purpose of this blog post is to explore the quantitative and ethical depths of decision theory in classification. The specific goal of the assignment was to try and identify a scoring function and threshold which would optimize expected profit per borrower for a bank who is deciding who to give loans to, using any of the features contained in our data set and any method. I started by exploring the data visually, trying to understand how different feature columns related to each other, and how bivariate feature combinations may correlate to loan status - i.e. whether a borrower repaid their loan or defaulted. After this, I fit a logistic regression classification onto the data set using varying feature combinations to see if there were any particular combinations which optimized initial classification accuracy. Once a model was fit, I used the model weights to calculate linear risk scores for each of the data entries/borrowers, and used these scores to identify a threshold risk score (borrowers lower than the threshold were approved, and above the threshold denied) which maximized profit per borrower for the bank. I found that the optimal threshold score was 1.414, which produced a profit per borrower of $1391.53 on the training data. Using this score as a threshold on the test data, I identified a maximized profit of $1102.18 per borrower. While this was lower than the training value, that is expected. However, upon processing through the test data to find its specific optimizing threshold, I found that my proposed threshold of 1.414 was not maximizing profit to its full potential on the test data. Instead, maximum profits of $1347.19 was found at a threshold of 1.010. So, while the proposed system kept profit high, it did not achieve its highest potential on the test data.\n\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 1: Explore the Data",
    "text": "Part 1: Explore the Data\nRemember, 1 means a person defaulted on loan, and 0 means they repaid in full\n\n# Import seaborn, our visualization library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a copy of our dataset, as since we use df_train later in this blogpost we want to prevent\n# any errors from being passed on to these visualizations due to the scope of df_train\nfor_viz = df_train.copy()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#exploring-borrower-features-with-facetgrid",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#exploring-borrower-features-with-facetgrid",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Exploring Borrower Features with FacetGrid",
    "text": "Exploring Borrower Features with FacetGrid\nUsing Seaborn’s FacetGrid, I plot the distribution of borrower characteristics across different loan intents. This allows us to compare how features like age, employment length, and home ownership vary by loan purpose.\n\nfeatures = [\"person_age\", \"person_emp_length\", \"person_home_ownership\"]\n\n\nfor feature in features:\n    g = sns.FacetGrid(data=for_viz, col=\"loan_intent\", hue=\"loan_intent\", col_wrap=3)\n    g.map(sns.histplot, feature, bins=20, alpha=0.7)\n    g.set_titles(col_template=\"{col_name} loans\")\n    g.figure.subplots_adjust(top=0.85)\n    g.figure.suptitle(f\"Distribution of {feature.replace('_', ' ').title()} by Loan Intent\", fontsize=14)\n    g.add_legend()\n    plt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 1:",
    "text": "Figure 1:\nFirst, we want to understand how loan intent varies with different features, specifically borrower age, length of employment, and homeownership status. Not only with this offer us qualitative insights, but also help us understand the distribution of borrowers we are encountering, and what patterns may emerge in the data.\n\nsns.displot(data = for_viz, x = \"person_age\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe are dealing with, for the most part, younger borrowers. The vast majority of borrowers across intents are 40 or younger, and above 40 years of age we see a steep drop off in borrowers. We see an especially high number of young borrowers in the data set seeking a loan for education."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 2:",
    "text": "Figure 2:\n\nsns.displot(data = for_viz, x = \"person_emp_length\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe see similar shapes in these visualizations as in that comparing borrower age and loan intent, but the context of course gives it a different meaning. Across the board, we see that the majority of borrowers have been employed for under 20 years. Again, the highest count of borrowers is found in the subset of those seeking education loans. After 20 years of employment, loan seekers effectively disappear from view. The lowest number of loan seekers are found for home improvement loans."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 3:",
    "text": "Figure 3:\n\nsns.displot(data = for_viz, x = \"person_home_ownership\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nIn nearly all of the visualizations, the highest number of borrowers seeking a loan are also renters. From highest to lowest counts, the borrowers’ home ownership status is RENT, MORTGAGE, and OWN. This makes sense intuitively as home ownership is likely a proxy for financial secure individuals who may be less likely to seek a loan as a form of financial support. The only place we do not see this trend is in the Home Improvement visualization. Here, the plurality of borrowers are mortgaging their home. This also makes sense, however, because those who have mortgages are likely “newer” homeowners who are undergoing more renovations/home improvements than those who have owned a home for a long time."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 4:",
    "text": "Figure 4:\nNext, I want to see the relationship between loan interest rates and loans as a percent of a borrowers income, and how this relationship may shape loan status - i.e. whether a borrower repays or defaults. Additionally, it will be helpful to compare visualization methods to see which Seaborn plots are more effective for such a large dataset\n\nsns.jointplot(for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\", kind = \"kde\", height=6)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\")\n\n\n\n\n\n\n\n\n\nDiscussion:\nAs seen above, using a scatterplot to visualize such a large set of data points is not a very effective approach. There is a great deal of overlap across our points, meaning that classification trends may be getting lost behind other data points. When looking at the joint plot, we first note from the edge plots that there are far more repaying than defaulting borrowers in our data set. Additionally, a higher distribution of borrowers repay loans they have a low interest rate, and their loan is a lower percent of their income. However, the reality remains that - based on the inner JointGrid - that there is still a great deal of overlap between borrowers of both loan status in relation to these two variables. Feature selection could not be done here purely through visualization."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 2: Building a Model",
    "text": "Part 2: Building a Model\n\n# First, establish the target column. This won't change so I initialize it here.\nTARGET_COL = \"loan_status\"\n\n# Even before processing the data, we perform some preliminary changes to the DataFrame to ensure\n# future processing does not interfere with indexing, such as dropping entries with empty values,\n# and converting qualitative features to one-hot encoded dummies\ndf_train = pd.get_dummies(df_train)\ndf_train = df_train.dropna()\n\n\n# Create a preprocessing function that will scale data and \n# create an X_train and y_train based on chosen feature columns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\ndef preprocess(df, quant_cols, qual_cols, target_col):\n\n    df_new = pd.DataFrame()\n    df_new[quant_cols] = df[quant_cols]\n    df_new[qual_cols] = df[qual_cols]\n    df_new_target = df[target_col]\n\n    scaler = StandardScaler()\n    \n    df_new[quant_cols] = scaler.fit_transform(df_new[quant_cols])\n    df_new[qual_cols] = df_new[qual_cols]\n    \n    return df_new, df_new_target\n\n\nExperiment with features/models\n\n# Model imports\nfrom sklearn.linear_model import LogisticRegression\n\n\n# First, I will test a model on all quantitative/qualitative features in the dataset\n\nX1_quant_cols = [\"person_age\", \n                    \"person_income\", \n                    \"person_emp_length\",\n                    \"loan_int_rate\", \n                    \"loan_amnt\", \n                    \"loan_percent_income\",\n                    \"cb_person_default_on_file_N\",\n                    \"cb_person_default_on_file_Y\",\n                    \"cb_person_cred_hist_length\"]\n\nX1_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\",\n                    \"loan_intent_VENTURE\",\n                    \"loan_intent_EDUCATION\",\n                    \"loan_intent_MEDICAL\",\n                    \"loan_intent_HOMEIMPROVEMENT\",\n                    \"loan_intent_PERSONAL\",\n                    \"loan_intent_DEBTCONSOLIDATION\", ]\n\n\nX1_train, y_train = preprocess(df_train, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\nLR1 = LogisticRegression()\nfit1 = LR1.fit(X1_train, y_train)\nLR1.score(X1_train, y_train)\n\n0.8498712184048544\n\n\n\n# Having fit and scored a model on all feature columns, we now explore smaller feature combinations,\n# looking to see if any particular combination maximizes classification accuracy\nX2_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"person_emp_length\"]\n\nX2_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX2_train, y_train = preprocess(df_train, X2_quant_cols, X2_qual_cols, TARGET_COL)\n\nLR2 = LogisticRegression()\nfit2 = LR2.fit(X2_train, y_train)\nLR2.score(X2_train, y_train)\n\n0.8468153839437726\n\n\n\n# Similar to above block but for alternative feature combination\n\nX3_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"loan_amnt\"]\n\nX3_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX3_train, y_train = preprocess(df_train, X3_quant_cols, X3_qual_cols, TARGET_COL)\n\nLR3 = LogisticRegression()\nfit3 = LR3.fit(X3_train, y_train)\nLR3.score(X3_train, y_train)\n\n0.844632645043\n\n\n\n# Now that we have fit a Logistic Regression model on our data, we have access to the\n# Model weights via the .coef_ attribute.\n# However, we to transform this array into a column because otherwise it is just a row,\n# complicating necessary computations in the future\nprint(fit1.coef_.T)\n\n[[-0.0423979 ]\n [ 0.04612073]\n [-0.02431996]\n [ 1.03609245]\n [-0.57955282]\n [ 1.3239725 ]\n [-0.0143236 ]\n [ 0.0143236 ]\n [-0.00851694]\n [-0.27565869]\n [ 0.46232549]\n [-1.75917877]\n [-0.87364387]\n [-0.6441703 ]\n [-0.02154856]\n [ 0.24354326]\n [-0.46649075]\n [ 0.15978062]]"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 3: Find a Threshold",
    "text": "Part 3: Find a Threshold\n\n# Define a function that calculates the linear scores of our model\n# By calculating the cross product between our predictors and model weights\ndef linear_score(X, w):\n    return X@w.T\n\nX1_scores = linear_score(X1_train, fit1.coef_)\nX1_scores = X1_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX2_scores = linear_score(X2_train, fit2.coef_)\nX2_scores = X2_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX3_scores = linear_score(X3_train, fit3.coef_)\nX3_scores = X3_scores.iloc[:, 0] # All rows and first column to convert to Series\n\n\n# Plot the scores of our linear score function to see score distribution\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(X3_scores, bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\n\n# Now that we know our score distributions, we can select a range of possible thresholds\n# and define a function to identify the threshold which maximizes profit\ndef calculate_repaid_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"]) / 100) ** 10) - df[\"loan_amnt\"])\n\ndef calculate_default_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"] / 100)) ** 3) - (1.7 * df[\"loan_amnt\"]))\n\ndef plot_best_threshold(df, scores, y_train):\n    best_profit = -1000000000\n    best_threshold = 0\n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4)) \n    for t in np.linspace(0, 10, 100):\n        y_pred = scores &gt; t\n        tp = (y_pred == 0) & (y_train == 0)\n        # tp.sum()\n        fp = (y_pred == 0) & (y_train == 1)\n        # fp.sum()\n        profit = calculate_repaid_profit(df[tp]).sum() + calculate_default_profit(df[fp]).sum()\n        profit /= len(df)\n        ax.scatter(t, profit, color = \"steelblue\", s = 10)\n        if profit &gt; best_profit: \n            best_profit = profit\n            best_threshold = t\n\n\n    ax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\n    labs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net Profit\", title = f\"Best Profit Per Borrower ${best_profit:.2f} at best threshold t = {best_threshold:.3f}\")\n\nplot_best_threshold(df_train, X3_scores, y_train)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Evaluate My Model from the Bank’s Perspective",
    "text": "Evaluate My Model from the Bank’s Perspective\n\nOPTIMAL_THRESHOLD = 1.414\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test = pd.get_dummies(df_test)\ndf_test = df_test.dropna()\nX_test, y_test = preprocess(df_test, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\ntest_scores = linear_score(X_test, fit1.coef_)\ntest_scores = test_scores.iloc[:, 0]\n\ny_pred = test_scores &gt; OPTIMAL_THRESHOLD\ntest_tp = (y_pred == 0) & (y_test == 0)\ntest_tn = (y_pred == 1) & (y_test == 1)\ntest_profit = calculate_repaid_profit(df_test[test_tp]).sum() + calculate_default_profit(df_test[test_tn]).sum()\ntest_profit /= len(df_test)\nprint(f\"Optimal Profit is ${test_profit:.2f}\")\nplot_best_threshold(df_test, test_scores, y_test)\n\nOptimal Profit is $1102.18\n\n\n\n\n\n\n\n\n\n\ndf_test[\"risk_score\"] = test_scores\nage_risk_score = df_test.groupby(\"person_age\")[[\"person_age\", \"risk_score\"]].mean()\n\n\nsns.scatterplot(df_test, x = \"person_age\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_age_approved = approved[\"person_age\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_age_denied = denied[\"person_age\"].mean()\nprint(f\" Average age of approved borrower: {avg_age_approved} vs. Average age of denied borrower: {avg_age_denied}\")\n\n Average age of approved borrower: 27.87536718422157 vs. Average age of denied borrower: 26.935751295336786\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the average age of borrowers approved with our threshold is less than 1 year older than the average age of denied borrowers. As such, it appears that the people of varying age groups have a similar degree of access to credit under my proposed system.\n\ncount_medical_total =  df_test[df_test[\"loan_intent_MEDICAL\"]]\ncount_medical_approved = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_medical_default = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\n\nprint(f\"Percentage of medical seeking borrowers which were approved: {(len(count_medical_approved)/len(count_medical_total))*100:.2f}%\")\nprint(f\"Percentage of approved medical seeking borrowers who defaulted: {(len(count_medical_default)/len(count_medical_approved))*100:.2f}%\")\n\n\nPercentage of medical seeking borrowers which were approved: 77.54%\nPercentage of approved medical seeking borrowers who defaulted: 15.02%\n\n\n\ncount_venture_total =  df_test[df_test[\"loan_intent_VENTURE\"]]\ncount_venture_approved = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_venture_default = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of venture seeking borrowers which were approved: {(len(count_venture_approved)/len(count_venture_total))*100:.2f}%\")\nprint(f\"Percentage of approved venture seeking borrowers who defaulted: {(len(count_venture_default)/len(count_venture_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 89.94%\nPercentage of approved medical seeking borrowers who defaulted: 7.50%\n\n\n\ncount_education_total =  df_test[df_test[\"loan_intent_EDUCATION\"]]\ncount_education_approved = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_education_default = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of education seeking borrowers which were approved: {(len(count_education_approved)/len(count_education_total))*100:.2f}%\")\nprint(f\"Percentage of approved education seeking borrowers who defaulted: {(len(count_education_default)/len(count_education_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 88.78%\nPercentage of approved medical seeking borrowers who defaulted: 10.73%\n\n\n\n\nAnalysis:\nBased on the above data, borrowers seeking a loan for medical reasons have a more difficult time accessing credit under my proposed system. Only 77.54% of borrowers from the test set who were seeking medical loans were approved compared to venture at 89.94% and education borrowers at 88.78%. However, at the same time, the rate of default was higher amongst medical borrowers - 15.02% - compared to venture - 7.50% - and education - 10.73%.\n\nsns.scatterplot(df_test, x = \"person_income\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_income_approved = approved[\"person_income\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_income_denied = denied[\"person_income\"].mean()\nprint(f\" Average income of approved borrower: {avg_income_approved} vs. Average income of denied borrower: {avg_income_denied}\")\n\n Average income of approved borrower: 71671.01657574486 vs. Average income of denied borrower: 41555.21658031088\n\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the income of borrowers approved with our threshold is notably higher - ~$30,000 higher - than the income of denied borrowers. As such, it appears that the people of lower income groups may have a more difficult time accessing credit under this system."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nIn conclusion, this post examined decision theory in classification through both quantitative and ethical lenses, aiming to optimize a bank’s expected profit per borrower. The analysis began with visual exploration of the data, followed by fitting logistic regression models using various feature combinations. By computing linear risk scores, an initial optimal threshold of 1.414 was identified — yielding $1391.53 per borrower on the training data and $1102.18 on the test data. However, further analysis of the test set revealed an optimal threshold of 1.010, which maximized profit at $1347.19 per borrower, indicating that while the system maintained high profitability, it did not capture the test data’s full profit potential.\nAt the end of the study, I explored the fairness of this model. Fairness in general is a difficult concept to define, but is complicated to a high degree in the context of machine learning models which make predictions about people. A machine learning model can never achieve full fairness - making a decision about an outcome only after considering not only all features/traits of a person relevant to that outcome, but also the context-specificity of those features - as ML is fundamentally built on generalization from examples. So, instead the question of fairness in this post should not be understood as an absolute, but relative to the context at hand: is the model fair enough? Also, the notion of fairness depends on the position of the evaluator - what fairness means to me fundamentally as a person is different to someone whose primary goal is to develop a system which maximizes profits, and therefore makes decisions about borrowers with historical default rates in mind.\nAfter analyzing the outcome data, I noticed that the percent of approved borrowers seeking a medical loan was over 10% lower than that of education/venture seeking borrowers. However, at the same time, medical borrowers had a default rate which was 5% higher than the other two loan seeking groups. From this, I am inclined to ask whether, considering their higher rate of default, it is fair for medical borrowers to have a harder time accessing credit? On one hand, I want to say this is not fair, as even though the borrowers have a history of higher defaults, the value/importance of a medical loan - to potentially save ones life - is much higher than that of an education or venture loan seeker. But at the same time, I wonder whether a bank-loaning model is realistically possible or responsible to include this value in an automated decision making model: can a profit maximizing model actually objectively quantify the ethical/lived importance of a loan to a borrower? Ultimately, these observations highlight the inherent tension between profit optimization and equitable access to credit. While the model accurately identifies risk patterns based on historical data, it falls short in capturing broader social values, particularly for medical needs where the stakes may be much higher. This raises critical ethical questions: should decision systems rely solely on past performance, or must they also adjust to recognize the importance of timely, life-saving credit?"
  },
  {
    "objectID": "posts/Limitations of the Quantitative Approach to Bias and Fairness/index.html",
    "href": "posts/Limitations of the Quantitative Approach to Bias and Fairness/index.html",
    "title": "Limitations of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In his 2022 lecture, The Limits of the Quantitative Approach to Discrimination, Arvind Narayanan puts forth a provocative claim on the role of quantitative methods in the contemporary age: “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan 2022). However, this claim can only be fully appreciated in the broader context of his speech, including not only its evidence and subclaims, but also Narayanan’s own positionality. As Narayanan explains in his speech, the harmful effects of Quantitative Methods seen today are not due to any inherent flaw, but instead because we have misunderstood the role and value of numbers in human society. As a quantitative scholar himself, Narayanan can provide a distinct level of specificity to this claim – of insider insight – which helps illuminate the tangible realities which beg us to take seriously that – as simple as it may sound – numbers actually can lie.\nNarayanan highlights 7 key limitations of quantitative methods which he has observed in his time as a quantitative scientist: Choice of null hypothesis, Use of snapshot datasets, Use of data provided by company, Explaining away discrimination, Wrong locus of intervention, The objectivity illusion, and Performativity.\nThe objectivity illusion is the quantitative limitation which, in a way, is at the heart of all other limitations put forth in Narayanan’s essay. As such, it is at the heart of the logic which my essay puts forth. Acknowledging it necessarily entails complications related to snapshot data, explaining away discrimination, and even the performative policies which quantitative science has brought about. The objectivity illusion refers to the false belief that quantitative research is objective because numbers are objective. Quantitative research involves subjective decisions made by researchers at multiple steps throughout the process such as – as mentioned above – choice of dataset or data collection methods, how to frame research questions and findings, which variables to control and to test on, and what “counts” as statistical evidence of discrimination: “In a typical research paper I would say researchers make at least 10-20 subjective choices, each of which could substantially alter their conclusions” (Narayanan 2022). The objectivity illusion is also at the heart of the previously mentioned limitations of snapshot data sets. As D’ignazio and Klein explain, “data are not neutral or objective. They are products of unequal social relations, and this context is essential for conducting accurate, ethical analysis” (See Introduction in D’ignazio and Klein 2023). Contemporary theories in the humanities, while not directly engaging questions of the quantitative world, offer language which is helpful to clarify the objectivity illusion.\nIn his book Crossing and Dwellings, for example, Thomas Tweed draws from insights of his scholarship to encourage a dynamic and itinerant approach to religious theory. The reason I bring such a seemingly distant resource to this conversation is, well, because it may not be as distant as we think. In fact, the “meta-theory” of theory which Tweed puts forth is quite similar to the “adversarial collaboration” approach of interpreting data which Narayanan mentions as “admirable”, in that it “explicitly acknowledges that scientists have their biases, their pet theories, their favored and hoped for conclusions” which will ultimately bias – in conscious or unconscious ways – the methodological approach they bring to their research (Narayanan 2022). As Tweed writes, “it is precisely because we stand in a particular place that we are able to see, to know, to narrate” (Tweed 2008, 18). There is no independent access to the experiences or subjective states of another, only that which the scholar can ‘see’ – used by Tweed to represent the multi-sensory reception of the corporeal world before us (Tweed 17). And, it is from those sightings that a scholar is able to construct meaning using the “categories and criteria they inherit, revise, and create” (Tweed 2008, 18). In the quantitative world, such sightings are at particular risk of being incomplete not because it is inherently worse, but because it is just simply not possible yet – whether by nature or by the limitations of modern technology – to capture the depth and variety of human experience with numbers. It is in recognizing this reality that the first steps may be taken towards deconstructing the epistemic hierarchy which grips the quantitative world.\nIt is ultimately from the objectivity illusion that other limitations emerge. For example, take the Choice of null hypothesis. Quantitative communities have “settled on the absence of discrimination” as the default assumption. Choosing to see the world as impartial places the burden of proof on those alleging discrimination, and naively pretends that quantitative scholars – or really that anyone – has the capacity to live in a world void of human subjectivity. Such a choice not only ignores, but may even perpetuate discrimination as it brushes aside the possibility of systemic discrimination – which manifests in implicit, everyday forms and compound over time – and holds us back from implementing the critical interventions which may help address discriminatory realities. To standardize the absence of discrimination holds a massive normative significance in that it encourages us to be blind to inequality unless it presents itself clearly and ‘objectively’, and even then, retain a level of skepticism towards it until it can be quantitatively confirmed. Narayanan’s observations about the choice and the null hypothesis point us to consider the principles upon which we build quantitative methods, and particularly how quantitative methods and researchers operate in relation to power. Quantitative scholars who follow the framework of Data Feminism, for example, would argue that such considerations should manifest in challenges to unequal power structures and working toward justice (D’ignazio and Klein 2023, Introduction). However, even before such a principle can be acted on, scholars must first take seriously the existence of power structures, and the ways in which the data they evaluate grows within, exists in relation to, and is drawn from those contexts.\nFurthermore, most datasets upon which quantitative methods are employed are collected from a “single system at a single point in time,” and it is even rarer for people to go out and collect new data. Such snapshot datasets are thus only frames in a much larger picture which inevitably involves systemic, social, and structural trends, “forcing the researcher to ignore people’s broader circumstances and the past discrimination they may have experienced” (Narayanan 2022). If we assume that such data can provide sufficient view of discrimination, or a lack thereof, we are encouraged to believe discrimination happens in snapshots – in “discrete moments of time” and quantitatively observable ways – which is simply never how discrimination has operated. Other quantitative scholars affirm this limitation, such as D’ignazio and Klein who argue that “the process of converting life experience into data always necessarily entails a reduction of that experience” (D’ignazio and Klein 2023, Introduction).\nIt is these misapprehensions which ultimately lead to Explaining away discrimination. In evaluating statistical disparities, quantitative researchers often diminish/set aside evidence of discrimination through their choice of analytical variables. Specifically, in trying to make their research logically/interpretatively sound, they control for variables that are tied to social constructs like race, subsequently excluding many of the proximal ways in which discrimination may actually operate. For example, Narayanan cites the 2004 study by Bertrand and Mullainathan which sent fake resumes to employers to test if an applicant’s race had an impact on the likelihood of them getting an interview. The way they implemented this was by creating pairs of resumes which were identical except for the applicant’s name, which would either be “White-sounding” (Emily, Greg, etc.) or “Black-sounding” (Lakisha, Jamal, etc.) (Narayanan 2022). While the study did find that “White names” were 50% more likely to receive a callback, it also limits our understanding of what data may signal someone’s social identity to their name: “Literally every other variable that might signal race or be correlated with race [other than one’s name] is held constant between the two conditions” (Narayanan 2022). Thus, while the study may have revealed discrimination in practice, it may have also “drastically underestimated” the degree of discrimination found in hiring practices (Narayanan 2022).\nA 2016 paper by Mittelstadt et al. further highlights this limitation, asserting that “the outputs of algorithms also require interpretation.” As such, ‘objective’ correlations can come to reflect the interpreter’s “unconscious motivations, particular emotions, deliberate choices, socio-economic determinations, geographic or demographic influences” (Mittelstadt et al. 2016). This point is reiterated in Fairness and Machine Learning, in which Barocas, Hardt, and Narayanan himself discuss the complexities which come with building fair machine learning models. If systematic discrimination persists in the world through subtle and systemic channels, it can’t be wholly identified in data that excludes those channels: “In real datasets, most attributes tend to be proxies for demographic variables,” meaning it is both irresponsible and dangerous to essentialize demographic identity to a single element, such as name signifying race (Barocas, Hardt, and Narayanan 2023, 17).\nHowever, while on one hand arguing about the present harms of quantitative methods, it would be insufficient to characterize Narayanan’s lecture as an exposé. He isn’t just saying what is wrong with this system, but rather presents a call to action and reformed thinking about the quantitative world. Eliminating quantitative methods from the conversation is neither desirable nor attainable, for the limitations which are found today are “not inherent to quantitative work” (Narayanan 2022). A better way of engaging with the quantitative world is possible, and steps made toward it can be seen today. On the surface, the proposition of taking seriously the subjectivity of human experience – of both the researcher and the researched – is not technically sophisticated enough. On the surface, the idea of developing research methods to measure and interpret systemic forms of discrimination – methods which “dig deeper” into the assumptions and politics which inevitably shape our engagement with the world (Narayanan 2022) – may not seem quantitatively plausible. However, it is only once we realize that the epistemic role of quantitative methods in a human world can never be absolute, because it is one of many ways of knowing the world, that we may begin to shift from predicting the world into fully reflecting on, understanding, and progressing from the form it inhabits right now.\nThe 2019 study conducted by Obermeyer et al., Dissecting racial bias in an algorithm used to manage the health of populations, provides a critical example of this digging deeper, and how quantitative methods can be employed to uncover and address systemic biases in algorithmic decision-making processes, specifically in healthcare risk prediction. In this research, the authors examined a commercially deployed prediction algorithm intended to identify patients who could benefit from extra medical support, known as “high-risk care management” programs (HRCMPs) (Obermeyer et al. 2019). By exploring the data produced from one of these algorithms through quantitative methods, Obermeyer and colleagues revealed that the algorithm systematically underestimated the healthcare needs of Black patients. Specifically, they found that Black patients who had identical algorithmic risk scores - i.e. who were considered of equally high medical risk/eligible for HRCMP as their white counterparts - were considerably sicker on average. Specifically, they found that Black patients who were predicted as “very-high-risk” (at the point of being auto-identified from HRCMP enrollment) had 26% more chronic health conditions when compared to their White counterparts. After further searching, the authors identified that the algorithm predicts health care costs rather than illness, targeting patients with high costs as eligible for HRCMPs. However, in doing so the algorithm mistakenly equated lower healthcare expenditures by Black patients with lower healthcare needs, overlooking the structural inequalities that produce disparate costs across demographic groups.\nWhile Black patients may spend less on healthcare than white patients, it is not because they are less sick. Instead, the authors identified that such a disparity arises from “unequal access to care” which has been driven by historical relationships – such as Black patients having reduced trust in the doctors – or socioeconomic barriers – such as access to transportation – which proximally impact the extent to which Black patients engage the health care system (Obermeyer et al. 2019). If these patients are cared for less often, then they will ultimately spend less on health care, contributing to the predictive biases which the study identified.\nThis study provides an insightful application of the notion of calibration as detailed in Fairness and Machine Learning. Calibration, a quantitative fairness criterion, requires that risk scores accurately reflect outcomes equivalently across different demographic groups (Barocas et al. 61-62). In the context of this study, a calibrated model ensures that, conditional on risk score, “predictions do not favor Whites or Blacks anywhere in the risk distribution” (Obermeyer et al. 2019). Technically, the healthcare algorithm satisfied this calibration criterion when assigning risk scores based on healthcare expenditure. However, the study illuminated a crucial limitation: calibration alone does not account for systemic biases embedded within the outcomes themselves - in this case, healthcare costs rather than actual health condition or needs. While calibration seeks to produce proportional consistency, it can perpetuate deep-rooted inequities if the target outcome itself is unjustly biased. Relying on proxies like healthcare spending is problematic as it disproportionately reflects unequal access and historical discrimination rather than genuine patient health needs. In other words, even a technically fair model - one that accurately predicts on healthcare expenditures - can remain unjust if it systematically disadvantages groups facing structural inequality.\nThis study highlights the benefits of employing epistemically humble (to stay in line with Narayanan’s language) quantitative methods in analyses of discrimination. The authors’ evaluation allows them to reveal how a seemingly neutral algorithm actually perpetuated discrimination. By quantifying the precise and various extents of disparity, the researchers make the reality of discrimination tangible, clear, and actionable. Subsequently, their analysis guides readers through possible interventions – such as reformulating the algorithm itself, or changing the data it is ultimately fed – that could substantially mitigate racial disparities, thus promoting equality. Therefore, Obermeyer et al’s study not only underscores the value of quantitative methods in exposing hidden biases, but also emphasizes the need to critically evaluate and choose fairness criteria that align with equity and justice.\nNarayanan’s critique is not an outright dismissal of quantitative methods but rather a call to critically examine the assumptions and limitations that shape their applications. My analysis of Narayanan’s claim, contextualized through his concept of the objectivity illusion, has shown that the limitations he identifies are not inherent to quantitative methods, but arise from a failure to acknowledge the subjective decisions embedded in quantitative research processes. As this essay has argued, understanding that data is the product of contextually embedded relations and realities is key to dismantling the epistemic hierarchy that privileges quantitative approaches as neutral or objective.\n\n\n\n\nReferences\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism. MIT press.\n\n\nMittelstadt, Brent Daniel, Patrick Allo, Mariarosaria Taddeo, Sandra Wachter, and Luciano Floridi. 2016. “The Ethics of Algorithms: Mapping the Debate.” Big Data & Society 3 (2): 2053951716679679.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\n\nTweed, Thomas A. 2008. A Theory of Religion. Cambridge, MA; London, England: Harvard University Press. https://doi.org/doi:10.4159/9780674044517."
  },
  {
    "objectID": "posts/Evolution and DNNs - Final Report/main.html",
    "href": "posts/Evolution and DNNs - Final Report/main.html",
    "title": "Evolution Based Weight Vector Optimization",
    "section": "",
    "text": "Abstract\nThis blog post explores the application of the principals of evolution to weight vector optimization problems (Link to Codebase/project Repo). A comprehensive evolutionary optimizer class, with hyperparameters allowing for control over selection, inheritance, diversity, mutations rates, and mutation styles, was created. Exploratory experiments were then run to attempt to understand the strengths and limitations of the various values for each of these hyperparameters. Experiments were performed on generated data as well as the MNIST dataset. Due to computation limitations, a complete optimization loop on the MNIST dataset was out of scope for this project. However, a final accuracy of 82% was achieved.\n\n\nIntroduction\nThis blog post explores how features of evolution in nature can inspire solutions to overcoming the shortcomings of gradient descent. Gradient Descent only works on differentiable loss functions, meaning it can become stuck in local loss minima when attempting to model non-convex loss functions. In other words, gradient descent cannot explore the entire solution space on nondifferentiable loss functions. This limitation can be overcome by harnessing the characteristics of evolution and natural selection in nature. Evolution has a wide variety of applications concerning Machine Learning, but this project focuses on its applications to weight vector optimization Telikani et al. (2021).\nLewontin identifies 3 key population characteristics for evolution: phenotypic variation in a population, differential fitness, and fitness must be heritable Lewontin (1970). With these 3 characteristics, evolution then occurs as ‘fitter’ individuals are better able to pass on their traits to future generations, while less fit individuals are not. At the individual level, evolution requires a blueprint, self-replication, mutation, and selection. By applying these principles to machine learning models, this blog post explores the strengths and limitations of evolutionary principles when applied to weight vector optimization in machine learning. To satisfy the requirement of phenotypic variation, each evolutionary optimizer has an entire population of weight vectors storing different weights. The different weights result in different losses, which in combination with selection pressures regarding the resulting different losses, satisfy the differential fitness requirement. With weight vectors serving as our genetic blueprint, those weight vectors can be duplicated to create or refill the population of weight vectors. Slight random adjustments to those weight vectors during replication serve as the mutations, ensuring the continuation of phenotypic variation. A variety of methods can be used to eliminate population vectors during an iteration, including loss and diversity, which function as selection. Eliminating high-loss weight vectors allows only vectors with high accuracy to pass on their characteristics, while eliminating low diversity can ensure that the solution space is adequately explored. Through the implementation of hyperparameters, many variations of evolutionary machine learning algorithms are explored to better understand their strengths and weaknesses.\nThe many hyperparameters are then tested on both generated and real data from the MNIST dataset to develop initial hypotheses regarding the optimal parameterization for evolutionary weight vector optimization to succeed.\n\n\nValues Statement\nThe potential users of the evolutionary-based weight vector optimization class are researchers, data scientists, and developers, especially those who work on non-differentiable problems with which gradient descent-based solutions struggle. Our class provides both a potential solution to overcoming the limitations of gradient descent on non-differentiable classification problems and serves as a potential benchmark against which other algorithms can be compared.\nOne major potential impact of the widespread use of our algorithm, or similar ones, is the increase in computational power required to run them. Because each epoch of an evolutionary algorithm requires the computation of an entire population of new weight vectors, the computational power required for an epoch is higher than most algorithms. This has potential positive implications for the manufacturers of computational chips and the owners of servers. On the other hand, the potential negative effects of increased energy and material consumption to perform these computations cannot be overlooked either.\nBecause the majority of our work was focused on the creation of a class, and not the optimization of a specific algorithm, the potential for positive and negative impacts of our class depends on who gains access to the class and what they decide to do with it.\n\n\nMaterials and Methods\nProject dependencies: - torch - numpy - pandas - scikit-learn - matplotlib\nOur evolutionary optimizer translates biological principles into a deep neural network optimiser. Biological evolution, and thus our algorithmic approach, rely on four core attributes: blueprint, self-replication, mutation, and selection. Our “blueprints,” genes or DNA in the natural world, are our weight vectors for the parameters of the neural network. We begin with an initial “population” of \\(N\\) such vectors that are sampled uniformly at random. In each generation, every individual is evaluated on a mini-batch of examples, combining cross-entropy loss (exploitation) with an optional diversity penalty (exploration). The lowest‐loss individuals (and occasionally a small “sneaker” fraction of high-loss outliers) serve as parents for the next generation. Some elite low-loss survivors carry forward unchanged. New offspring are created via uniform crossover, where each weight entry, or gene, is inherited from \\(k\\) randomly chosen parents, then mutated by adding small Gaussian or Laplacian noise with some probability. Optionally, each child can receive a single gradient‐descent step to fine-tune its accuracy. Initially, we relied on synthetic binary-classification data generated using torch.rand to train and validate our evolutionary approach. This allowed us to develop a proof of concept that evolution could, in fact, solve problems and that our selection, crossover, mutation, etc., behaved as expected before we moved on to real-world inputs.\n\n\nExample of MNIST Digits\n\nWe decided to employ our evolutionary approach to the MNIST handwritten-digit dataset LeCun, Cortes, and Burges (2010). This dataset is made up of 70,000 gray-scale images of size 28×28 pixels, labeled 0–9. We accessed the dataset through torch datasets. In smaller experiments with the MNIST dataset, we opted to draw a random subset of anywhere from 1,000 to 20,000 digits to improve computational efficiency. Although the smaller subsets enabled rapid prototyping, they may have overrepresented certain rarer handwriting styles and potentially skewed accuracy.\n\n\nHyperparameters\n\nProof of concept/vanilla evolution:\n\n\nSelection Processes:\nIn nature, genetic traits are passed from one generation to the next by individuals that survive and successfully reproduce. These survivors make up the gene pool of their generation, while those that fail to reproduce are effectively excluded from the evolutionary process. In our implementation, we emulate this principle by defining fitness based on standard cross-entropy loss or diversity-augmented loss, depending on the user. At each generation, we sort the population in a minheap based on loss, then the top 50% (the half with the lowest loss) are selected to form the gene pool. The other half does not have the chance to reproduce. In the next section, we will dive into how we handle creating the next generation from the gene pool.\n\n\nIllustration of Gene Pool Selection\n\nMirroring the random nature of evolution, we incorporate some chance in the makeup of our gene pool. A small number of lower-performing individuals (10% by default) are included in the gene pool with low probability. These individuals, whom we call sneakers, introduce genetic variation that helps maintain a diversified population and prevents premature convergence.\n\n\nIllustration of Sneaker Population\n\nFinally, we employ an elitist strategy to preserve our high-performing solutions. Each generation, a percentage of the top performers based purely on cross-entropy loss are included in the gene pool and also survive unchanged and unmutated to the next generation. This preserves the integrity of the best solutions by keeping a lineage of high-performing individuals.\n\n\nOverview of New Generation Gene Makeup\n\n\n\nInheritance and Parent Quantity:\nAt each iteration of our evolutionary optimization, following the creation of a ‘gene pool’ in the selection stage, the population must be replenished with new individuals. There are three ways that this can be accomplished. 1: All new individuals are new randomized weight vectors with no input from the gene pool. 2: Each new individual has a single parent randomly selected from the gene pool from which its weights are inherited with random mutations. 3: Each individual has n parents randomly selected from the gene pool. Each feature weight is then inherited from the corresponding feature weight of a random one of its parents.\nThe first scenario, with no inherited weight vectors, is a baseline against which our true evolutionary models can be tested. This is not truly evolution, as it does not include any heritability of fitness for new individuals in the population Lewontin (1970).\nThe second Scenario, includes heritability of fitness, but with only a single parent for each child individual, the diversity can be expected to be more limited.\n\n\nDiagram of Inheritance when num_parents = 1\n\nThe Third Scenario, allows for a slightly reduced heritability of fitness, with the addition of diverse new individuals produced with each generation. The diversity rate is specifically limitted by the mutation_rate and mutation_intensity hyperparameters.\n\n\nDiagram of Inheritance when num_parents = 2\n\nFunctionally, this process occurs after selection has occured, and an overall gene pool of parents has been created. A random sampling with replacement is then performed on that gene pool, in which each new child is assigned n, the value of the num_parents hyperparameter passed to the function, parents from the gene pool. For each weight in each child’s weight vector, a random one of that child’s n parents is then chosen from which it inherits that specific weight. If num_parents = 0, then every child recieves a completely random weight vector. Once the weights have been assigned, the child weight vector is then mutated.\nAs discussed in the results section, the choice of the number of parents can have a significant impact on loss, accuracy, and diversity.\n\n\nHybridizing evolution with gradient descent:\nOur approach to evolutionary optimization incorporates a gradient-based refinement step that allows individuals to local optimize their performance (slightly) after being created. In essence, this hybrid evolutionary-gradient approach combines the global search strengths of evolutionary algorithms with the precise, local updates enabled by backpropagation. For each new individual generated during the evolutionary step, we apply a single gradient update to refine its weights. This is accomplished using a method implemented within the model that performs a forward pass, calculates cross-entropy, and uses PyTorch’s automatic differentiation to compute gradients. Weights are then updated according to the direction of the negative gradient, and scaled by a learning rate set to 0.5.\nThe backpropagation step is called once per individual in the evolutionary loop, immediately after crossover and mutation have produced a new weight vector candidate. The updated individual is then re-inserted into the population. By integrating this light update of gradient descent, the optimizer benefits from enhancing convergence rates - while still being able to prioritize diversity - with fewer generations of evolution.\n\n\nComputing Diversity and Diversity-Based Loss:\nOur evolutionary optimization implementation includes a mechanism for encouraging population diversity by directly incorporating a diversity term into the model’s loss function. Diversity is measured over the entire population of weight vectors, with four distinct methods implemented to quantify it. These include Euclidean distance, cosine dissimilarity, standard deviation, and variance. The Euclidean distance metric calculates the mean spatial difference between every pair of individuals in the population. Cosine dissimilarity measures the angular dissimilarity of weight vectors by computing one mines the cosine similarity between weight vectors. The standard deviation and variance metrics, on the other hand, operate across the whole population of weight vectors by computing the average distribution/variance of all weight vectors within a generation.\nOnce computed, the diversity score is used to modify the model’s loss. Specifically, during each evaluation of an individual weight vector in the population, the standard cross-entropy loss is calculated and then a diversity term is subtracted from it. This diversity term equals the above mentioned diversity value scaled by a user-set diversity coefficient. The effect of this subtraction is that models with higher diversity scores receive a lower total loss, incentivizing the optimizer to explore a broader range of solutions. This diversity-aware loss is only applied when explicitly enabled through a boolean flag in the model, giving flexibility for experiments that compare/evaluate the performance of diversity-based and non-diversity based evolutionary optimization.\n\n\nAdjustment from binary to multi-class classification:\nBecause our target task is classification on the MNIST dataset - which involves 10 possible output classes (digits 0 through 9), we implemented multiclass classification using Pytorch’s CrossEntropyLoss function. Unlike binary cross-entropy, which assumes a binary classification problem and compares scalar outputs to binary labels, cross-entropy loss compares a vector of probabilities (logits) against a single target value label. This function internally applies a softmax operation which evaluates the likelihood of each logit being the right output class.\nIn our implementation, the CrossEntropyLoss function is used in both the model’s forward loss evaluation and backpropagation step. This ensures that each prediction is treated as a multiclass decision and that the model can properly learn to distinguish between all 10 classes in the MNIST dataset.\n\n\nMutation Methods:\nOne thing that we created in our vanilla EVO optimizer was a random mutation mechanism. This mutation mechanism let’s us assign a small probability that each of the weight’s entries’ values can be nudged by a certain amount positively or negatively. This nudge and its intensity is modeled by a normal distribution around the current value, and what we call “Mutation Intensity” is the standard deviation of that normal distribution. This ensures that we are constantly updating our weights randomly, and that there is a chance for weights to get better. What we noticed is that only using normal distribution might not be sufficient in achieving fast convergence. Because the normal distribution’s tails flatten with the X-axis quickly, it does not give the slightest opportunity for the model to get an aggressive nudge.\n\nThis led us to explore different distributions that also share the characteristic that ensures the nudge is usually not too aggressive, but also allows ever so rarely for it to change the weight’s entry significantly. This distribution that we introduced is the Laplacian distribution.\n\nThis became another hyperparameter that allows us to see how different mutation methods affect different models that our EVO optimizer tries to solve.\n\n\n\nResults\n\nChoices in Selection Processes:\nBy default, we select the best 50% of the population to enter the gene pool, however, this is a hyperparameter that users can play with. We conducted some experiments on a small subset of 1000 digits from the MNIST dataset to examine how different gene pool sizes (10%–90% of the population) would affect our accuracy, loss, and diversity over 500 generations.\n\n\nGene Pool Size vs Accuracy\n\n\n\nGene Pool Size vs Loss\n\n\n\nGene Pool Size vs Diversity\n\nThere are several interesting things to note about these figures. Focusing on the extremes first, only picking 10% of the best individuals is advantageous if we look purely at accuracy. However, this came at the cost of significantly reducing diversity, with such a small portion of the population passing through at each generation. Having too homogeneous a population can lead to getting stuck in local minima without exploring the wider loss landscape. On the other hand, having too many members of the population reproduce increases exploration of the loss landscape, but reduces selection pressure, as individuals with suboptimal solutions continue to reproduce. We can see this illustrated above as the accuracy lags far behind all of the other gene pool sizes. Keeping the best half performed is a strong middle ground with comparatively great accuracy, second only to keeping the top 10%, while remaining diverse.\nWe also investigated the effects of varying the probability that “sneakers”—individuals from the bottom 10% of the population—could enter the gene pool. We tested probabilities from 0–45%.\n\n\nSneaker Probability vs Accuracy\n\n\n\nSneaker Probability vs Loss\n\n\n\nSneaker Probability vs Diversity\n\nInterestingly, across a range of sneaker probabilities, we didn’t observe much variation in loss or diversity. So it doesn’t impact our learning dynamics to a noticeable degree. However, having a 45% sneaker probability performed quite well, accuracy-wise. This may be a reflection of random variation of our dataset or starting genepool, but it may also suggest that a degree of genetic noise can occasionally help guide the population out of local minima. In future experiments, it would be insightful to set the hyperparameter to be above 50% and see the results.\nFinally, we explored the impacts of elitism by varying the percentage of top-performing individuals who we carried unchanged to the next generation.\n\n\nElitist Population Size vs Accuracy\n\n\n\nElitist Population Size vs Loss\n\n\n\nElitist Population Size vs Diversity\n\nWhen we have too many elites, we slow down evolutionary convergence. We aren’t introducing enough change from generation to generation to explore the landscape and improve our solution. We can see evidence of this in our stunted accuracy, low diversity, and higher loss when we increase the size of our elite population. However, when we eliminate elites or keep only 5%, we see noticeable improvements. Our loss is converging faster, we maintain a diverse population, and our accuracies after 500 generations are the highest. Keeping the elite population helps our accuracy, outperforming the population without elites by over 5% over 500 generations. Overall, we observed that on MNIST, modest elitism provides a valuable balance between preserving high-quality solutions and allowing diversity within the population.\n\n\nInheritance and Parent Quantity:\nTwo experiments were performed to explore the limitations and strengths of different inheritance methods, specifically the adjustment in the number of parents from which each child weight vector receives it’s own weight vector values.\nFor both experiments below hyperparameters that were held constant were:\n-Survivor Ratio: 0.1\n-Fitness Ratio: 0.5\n-Sneaker Probability: 0.01\n-Sneakers ratio: 0.1\n-mutation rate: 0.05\n-mutation intensity: 0.05\n\nGenerated Data Experiment:\nThis generated data experiment explores the performance of our model when varying the num_parents hyperparameter. A multi parent classification experiment was run on generated 2 dimensional data with 0.2 noise and 300 points. The accuracy, loss, and Euclidean diversity was tracked across 300 iterations. The experiment was run with the hyperparameter num_parents set to 0, 1, 2, 3, 5, and 10.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on Generated data using 0, 1, 2, 3, 5, and 10 parents over 300 iterations\n\nAs seen in the above visualization, Loss and Accuracy were comparable across all quantities of parents, while diversity varied significantly. In particular, with num_parents set to 0 and to a lesser extent 1, diversity was much lower than all other quantities of parents. The accuracy of the 0 parent model also performed worse than the other models over more iterations. With 0 parents evolution is conceptually replaced by random chance. The heritability, defined as a requirement for evolution by Lewontin (1970), is eliminated from the process.\nWhile this had a much smaller impact on this relatively simple experiment of generated data, the implications on a much more complex classification problem, such as MNIST, could be significant.\n\n\nMNIST Experiment:\nA similar, more complex experiment performed on a subset 1000 images from the MNIST dataset tested the accuracy, loss, and diversity of num_parents = 0, 1, 2, 5, 10 over 1000 iterations. As a significantly more complex classification problem, the strengths of including more parents become much clearer.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on a Subset of the MNIST dataset using 0, 1, 2, 5, and 10 parents over 1,000 iterations\n\nThe benefits of inheritance are clear, as the zero parent model has a significantly higher loss and lower accuracy throughout the 1000 iterations compared to all other models.\nStarting with loss, we can see that the loss for all test groups are relatively similar with the exception of num_parents = 0.\nWith regards to accuracy, we see a more nuanced picture. 0 parents performs poorly throughout the experiment, never reaching 21% accuracy. 1 parent has logarithmic like improvement in accuracy at around 35%. All higher quantities of parents follow a similar trajectory, but with major jumps in accuracy breaking the logarithmic like trend. This can be better understood by looking at diversity levels.\nDue to the random nature of the weight vectors for the 0-parent group, the diversity is constant and extremely high at 65.124. For all other groups, it is clearly shown that more parents results in maintained diversity. As selection occurs, and the population is replenished with weight vectors inherited from the gene pool, diversity decreases overall. But by allowing for more varied combinations from that gene pool, some level of diversity is preserved. This appears to have diminishing benefits as demonstrated by the similar diversity for both 5 and 10 parents.\nThis becomes a problem of optimizing the heritability of fitness and the phenotypic variation mentioned by Lewontin (1970). fewer parents means more pure inheritance of fitness, as the child will more closely resemble its parents. It also means less phenotypic diversity, as completely new weight vectors are less likely to emerge. The opposite with regards to both phenotypic variation and fitness heritability applies. The benefits of multi-parent inheritance are demonstrated by the declining improvement in accuracy when num_parents = 1. The lower diversity compared to the other models, and the importance of diversity in evolutionary algorithms in allowing for the exploration of the solution space, results in poorer performance of the single parent model. A single parent allows for the inheritance of fitness,leading to better performance compared to the 0 parent model Lewontin (1970). However, it does not allow for large enough variation in fitness. With lower diversity, the 1 parent model is less likely to find a global minimum compared to the 2+ parent models. While it does find some form of a local minimum, the lack of diversity results in a drop off in improvement at around 600 iterations, while the models with 2, 5, and 10 parents continue to have spikes in improvement.\nIn the context of classification of the MNIST dataset, evolutionary models benefit from the added diversity resulting from the use of larger quantities of parents contributing weights to each new child in the subsequent generation. While more computing power, and more iterations are required to truly optimize this hyperparameter, these experiments clearly demonstrate the benefits of multi-parent inheritance.\n\n\n\nQuantifying Diversity and Diversity-Based Loss:\nThis section evaluates the effect of diversity-aware loss functions in evolutionary training of a neural network classifier on a subset of the MNIST handwritten dataset. We experimented with four diversity metrics - Euclidean Distance, Cosine Dissimilarity, Standard Deviation (STD) and variance - and measured their influence on test accuracy, cross-entropy loss, and diversity levels in our weight population over 200 generations. Additional hyperparameter tuning was performed for the Cosine diversity metric to explore how mutation rate, mutation intensity, population size, and diversity coefficient influence outcomes.\nThe first experiment (figure 1) compared the test accuracy, loss, and normalized diversity across all four diversity metrics under a fixed training setup. All metrics enabled the model to reach between 75%-81% accuracy over 200 generations, with all other hyperparameters held constant. euclidean distance and STD slightly outperformed others in final diversity. All methods reduced loss substantially within 60 generations. When it came to Normalized diversity, all metrics except for, interestingly, cosine dissimilarity between weight vectors increased/maintained high diversity over time. Cosine dissimilarity diversity rapidly decayed to near-zero within 100 generations, while STD, variance and euclidean distance maintained high diversity levels, suggesting that cosine may be more prone to premature convergence or intrinsically mediates the impact of diverse weight populations.\n\n\n&lt;em&gt;Figure 1:&lt;/em&gt; Comparing test accuracy, loss, and normalized diversity values for all 4 diversity metrics.\n\nTo better understand the behavior of the cosine dissimilarity metric, we ran additional training with varied diversity coefficients, population sizes, and mutation hyperparameters. The default hyperparameters used were population size 50, mutation rate 0.4, mutation intensity 0.5 and diversity coefficient 0.1. Increasing the diversity coefficient to 0.3 (figure 4) significantly improved diversity values - up to 0.2 - over each generation, confirming that the penalty term has a regulating effect on population diversity. When the diversity coefficient was set to 0.0 (figure 3), the model still trained to reasonable accuracy but showed completely flat diversity values, indicating the diversity term is implemented correctly to at least affect our metric value. Increasing population size to 100 (figure 5) improved diversity over each generation, especially in the first 100 generations, but did not substantially improve test accuracy. This suggests diminishing returns from larger populations in this setting. Raising mutation rate to 0.7 and intensity to 0.8 (figure 6) had a negligible to slightly positive impact on accuracy while maintaining diversity at moderate levels. Accuracy did experience more noisiness under these conditions, but ultimately achieved reasonable levels.\n\n\nFigure 2: Baseline experiment outputs to provide reference test accuracy, loss, and diversity values for cosine driven loss.\n\n\n\nFigure 3: Confirming working implementation of diversity coefficient’s effect on diversity based loss by setting diversity coefficient to 0.0.\n\n\n\nFigure 4: Results showing the effects of increased diversity coefficient of 0.3 - i.e. higher effect of diversity punishment/reward on loss - on test accuracy, loss, and diversity values.\n\n\n\n\nFigure 5: Results for increased population size of 100 weight vectors on test accuracy, loss, and diversity values.\n\n\n\nFigure 6: Results for impact of high mutation rate and mutation intensity on test accuracy, loss, and diversity values.\n\nIn summary, all four diversity metrics led to successful convergence and comparable final test accuracies, with euclidean distance and STD slightly ahead. Cosine dissimilarity driven diversity tends to descend quickly, requiring further parameter tuning to explore what it takes to keep diversity high. Enabling the diversity penalty to loss had a clear and measurable effect on both training behavior and final diversity levels, validating its implementation. Mutation and population hyperparameters affected convergence stability and final accuracy but had less influence than the choice of diversity metric.\nThis study was constrained by computational limitations, which restricted the breadth of hyperparameter combinations we could explore. In particular, both the population size and the number of generations were limited in order to keep training time feasible. Larger populations and longer training schedules could potentially yield more robust insights into the effects of diversity-aware loss function. Further investigation into the behavior of cosine dissimilarity is warranted. Across multiple experiments we observed a consistent decline in diversity when using this metric. One possible explanation for this is that cosine dissimilarity only measures angular differences between vectors, ignoring their magnitudes. As a result, the population may converge to a set of similarly oriented but differently scaled vectors, which could be interpreted as low diversity by this metric. This limitation could implicitly constrain the optimizer’s ability to maintain variation during training, and future work could test this hypothesis more directly or explore hybrid metrics that include both angular and magnitude components. Additionally, we were limited in the size of training and test batches, which may influence generalization performance. It would be valuable to evaluate how increasing batch size or dataset subset size impact both diversity value and resulting model accuracy. Please note, all of these experiementes were run on a hybridized version of the evolution optimized DNNs which included, for every step of training one gradient descent step on each weight vector. This was done in hopes to reduce runtimes without straying too far from pure evolution. Pure evolution, we speculated, would have needed to require high data inputs, generation numbers, and population sizes to produce valuable results, which did not fit the computational capacities of our computers, nor our time constraints.\n\n\nFinal MNIST Results:\nAfter combining all of our implementations together, we trained a deep neural network with layers [764,32,10] to classify our MNIST dataset. We settled on the following hyperparameters:\nmodel.diversity_coeff = 0.2 optimizer = EvolutionOptimizer(model) optimizer.set_population_size(200) optimizer.use_backprop = False optimizer.set_survivors_ratio(0.1) optimizer.set_fitness_ratio(0.5) optimizer.set_sneaker_prob(0) optimizer.set_mutation_intensity(0.05) optimizer.mutation_rate = 0.05\n\nThe plot above shows that the accuracy rapidly increases during the early generations, indicating that the evolutionary algorithm quickly identifies promising weight vectors, significantly reducing the initial error. This is likely because the selection criteria is too strict so it rapidly eliminates poorly performing members of the population. The curve starts to smooth out, reflecting a deceleration in accuracy improvement as the optimizer converges on better solutions. After around 2000 generations, the accuracy curve stabilizes, indicating that the optimizer has reached a near-optimal solution for the given problem. The final accuracy appears to stabilize around 82%, suggesting that the current hyperparameter settings and genetic operators are effective but may have room for further optimization, possibly through adjustments to mutation rates, diversity coefficients, or parent selection mechanisms.\n\n\n\nConcluding Discussion:\nAs we combined all of our implementations of various evolutionary components, and created our unified optimizer that has significant flexibility in deciding the environment where our weights can evolve to solve the ML problems over time, we were ready to test this optimizer at a famous problem for deep neural networks: Classifying handwritten digits. Tweaking many of our hyperparameters lead to significantly different converging speeds, diversity metrics, and overall performance. This flexibility can be helpful in tailoring our algorithm to work in different context and on different models.\nOur intention from the beginning was to simulate how living creatures solve the problem of survival: Evolution. What encouraged us to explore this algorithm is the beauty of how living beings have evolved to solve the same problems very differently. This diversity that exist in nature is what got us thinking about ways we could achieve this concept in optimizing machine learning models on different datasets. If we can create a population of weights that can reproduce over time, and spread their genes and cross it with one another, what can we notice about the diversity of their solutions? This took us on a journey of simulating this natural process, abstracting it into simpler components, and specifying it to our context.\nOur project worked in many ways: our EVO optimizer managed to get the population to converge through random mutation, and maintain diversity by adjusting the diversity coefficient hyperparameter. We did however see a natural decay in diversity as the exploration phase ends and the exploitation phase begins where the population begin to converge around good solutions it found in the initial phase. Beyond the scope of the optimizer, our project worked in a sense that it provided us with the opportunity to investigate, design, and implement a complex environment for evolution. This has been the project that taught me at least the most about object oriented programming, and has taught us a lot about how to write legible code that will be built on by others.\nOur results are comparable to other evolutionary optimization algorithms in terms of convergence speed and diversity emphasis, however, different implementations have allowed for even more complex design decisions of the environment, more complex selection criteria (like tournament style), adaptive mutation rate, limitations on the mating process. These added complexity unlocks many different combinations of hyperparameters that outperform our simple-er implementation.\nIf we had more time, we would definitely work on improving speed. Currently, our code does not fully utilize GPU. There are a few python for-loops when popping our populations based on total loss and cross entropy loss. These are operations that, when vectorized, could speed up the training process significantly. In addition, we can add more design options for more complex evolutionary algorithms. Also, we would perform a grid search to find the best hyperparameters that would optimize for a deep neural network for handwritten MNIST dataset in terms of accuracy and diversity. Finally, we would implement an inference mechanism that would classify data using majority voting, assuming that the diversity in the population allows for a broader knowledge base to solve the problem, i.e, it would be interesting to see if the phenomenon of the wisdom of the crowd emerges under our current evolutionary algorithm within a population.\n\n\nGroup Contributions:\n\nLukka:\nAs a unit, the whole team contributed to the conceptualization and the early stages of building a working prototype. Lukka worked mainly on implementing, refining, and exploring the selection mechanisms in our evolutionary model. he also helped integrate the Laplacian mutation distribution. Lukka also helped include and streamline my work and the work of others into a central working file. This was work that helped build the base of how we would handle our object-oriented programming approach and handle tuning hyperparameters. He also spent considerable effort and time getting MNIST to run correctly on the Middlebury cluster to facilitate larger-scale experimentation. In all the team meetings, we all spent time digging into one another’s code, learning and helping on implementation, debugging, and developing conceptual frameworks.\n\n\nJiffy:\nFor this project, Jiffy contributed to both the conceptual development and the technical implementation of our evolutionary optimization framework. Early in the project, je created a demo notebook (evolution_demo_warmup.ipynb) that introduced the basic principles of evolutionary algorithms using synthetic data, aiming to outline a clear conceptual framework of evolution’s purpose and potential in our project. Jiffy was primarily responsible for implementing and optimizing the diversity-aware loss framework, including vectorized versions of the Euclidean distance and cosine dissimilarity metrics, as well as additional metrics based on standard deviation and variance. He added support for toggling these metrics and integrating them into the final loss calculation. Jiffy also extended our codebase to support multiclass classification, enabling us to apply our models to the MNIST dataset. Much of Jiffy’s experimentation involved running classification trials with varying diversity metrics and hyperparameters - mutation rate, intensity, and diversity coefficient - which he documented in a jupyter notebook (ExploringDiversity.ipynb). Jiffy wrote an initial training logic and data loading code for MNIST, and developed visualization tools using matplotlib to track accuracy, loss, and diversity across generations. He also implemented the hybrid optimization step, which combines evolution with gradient descent via backpropagation. For the final blog post, Jiffy focused on writing detailed technical explanations of the algorithmic components I implemented, along with reporting and analyzing the results of my experiments. This includes the materials/methods section on hybridizing evolution with gradient descent, computing diversity and how diversity-based loss was implemented, and the transition from binary to multiclass classification. It also includes the results writeup for ‘Quantifying Diversity and Diversity-based Loss’.\n\n\nJames:\nJames’ main contribution to this project was the creation, implementation, and experimentation on the benefits and limitations of adjusting the inheritance process, in terms of how many parents each child’s weight vector has. This included identifying scholarly sources which provided a framework for understanding how each change to our hyperparameters, and in the case of inheritance specifically, in the number of parents influences the forces of evolution Lewontin (1970). The majority his work is found in the multi-parent and multi-parent2 folders, although the important changes to the evo class were eventually merged with the blog-post EVO class. While the most important contributions can be found in /multi-parent2/MultiParent.ipynb, James spent considerable time working to overcome computational limitations of my computer and then working to have my code run on ADA. James wrote the abstract, the introduction, the values statement, and the Inheritance and Parent Quantity subsections of the Hyperparameter and results sections.\n\n\nYahya:\nFor this project, I contributed to both the conceptual foundation and technical implementation of our evolutionary optimization framework. Early in the project, I helped conceptualize and explain various mathematical approaches to designing the optimizer, providing the initial direction for our implementations. I also supplied resources to support my teammates in understanding the theoretical aspects of evolutionary algorithms. I implemented a proof-of-concept version of the optimizer, integrating basic mutation, crossover, and selection mechanisms, which served as the foundation for our more complex final implementation. This included designing and implementing the core structure of the optimizer, which became the base for further development. I also investigated different diversity metrics and incorporated them as terms in the loss function to maintain diverse populations and reduce the risk of premature convergence. I designed and refined the OOP structure throughout the project. Additionally, I designed the mutation mechanism and introduced a Laplacian distribution as an alternative to Gaussian mutation, allowing for a more varied exploration of the solution space. To address computational bottlenecks, I collaborated with Professor Vaccari to set up scripts for running Jupyter servers on Middlebury’s GPU nodes, resolving runtime issues for the team. Finally, I helped assemble the final version of the EVO optimizer and experimented with different hyperparameter combinations to fine-tune the model and achieve the results presented in our report.\n\n\n\nPersonal Reflection:\nWorking on this project taught me a lot about not just neural networks or optimization, but about communication and collaboration. I quickly realized how difficult it can be to clearly explain technical and often abstract concepts, especially when they are new to everyone involved. As a team, we had to find ways of communicating that worked not only collectively but also respected how each of us learns individually. Personally, I learned more about myself as a learner and developer: how to organize my code more cleanly, how to manage responsibilities in a shared codebase, and how to balance rapidly changing experimentation with readable, maintainable work.\nLooking back, I’m incredibly proud of what we achieved. At the outset, I was nervous, especially about understanding deep neural networks. But not only did we meet our goals, we built something that I initially wasn’t sure I could contribute to meaningfully. I gained confidence in my technical abilities and found real excitement in tackling difficult problems. While there were definitely bugs, missteps, and late-night debugging sessions, they were part of a process that left me feeling accomplished.\nThis experience will stay with me. I’ve discovered that I’m passionate about learning how computational and problem-solving strategies work at a deep level, and that I thrive when I can explore those questions in a collaborative environment. I also gained important skills in strategic time management and how to structure tasks to fit into a bigger picture. Whether I’m in a future course, on the job, or even working on a personal project, I’ll carry forward these lessons in both the technical and human dimensions of working on meaningful solutions to complex problems.\n\n\n\n\n\nReferences\n\nLeCun, Yann, Corinna Cortes, and CJ Burges. 2010. “MNIST Handwritten Digit Database.” ATT Labs [Online]. Available: Http://Yann.lecun.com/Exdb/Mnist 2.\n\n\nLewontin, R. C. 1970. “The Units of Selection.” Annual Review of Ecology and Systematics 1: 1–18. http://www.jstor.org/stable/2096764.\n\n\nTelikani, Akbar, Amirhessam Tahmassebi, Wolfgang Banzhaf, and Amir H. Gandomi. 2021. “Evolutionary Machine Learning: A Survey.” ACM Comput. Surv. 54 (8). https://doi.org/10.1145/3467477."
  },
  {
    "objectID": "posts/Double Descent/index.html",
    "href": "posts/Double Descent/index.html",
    "title": "Double Descent in Overparameterized Regression",
    "section": "",
    "text": "In this blog post, I explore how feature complexity/quantity affects the performance of linear regression models, particularly in the context of overparameterized optimization. I begin the post by analyzing why the standard solution to regression fails at the interpolation threshold - where the number of features begins to exceed number of data observations. I then implemented an overparameterized linear regression model using the Moore-Penrose pseudoinverse to fit data - linked here - and assessed its ability to learn on 1D data and corrupted image data. By performing a sweep of potential feature numbers - generated with random nonlinear feature maps - I generated training and testing error curves which help clarify double descent through visualization. Specifically, my results show that while performance worsens around the interpolation threshold, it quickly improves as feature count increases beyond data observations, with my lowest testing error occurring with 200 features on a synthetic dataset with 100 observations."
  },
  {
    "objectID": "posts/Double Descent/index.html#abstract",
    "href": "posts/Double Descent/index.html#abstract",
    "title": "Double Descent in Overparameterized Regression",
    "section": "",
    "text": "In this blog post, I explore how feature complexity/quantity affects the performance of linear regression models, particularly in the context of overparameterized optimization. I begin the post by analyzing why the standard solution to regression fails at the interpolation threshold - where the number of features begins to exceed number of data observations. I then implemented an overparameterized linear regression model using the Moore-Penrose pseudoinverse to fit data - linked here - and assessed its ability to learn on 1D data and corrupted image data. By performing a sweep of potential feature numbers - generated with random nonlinear feature maps - I generated training and testing error curves which help clarify double descent through visualization. Specifically, my results show that while performance worsens around the interpolation threshold, it quickly improves as feature count increases beyond data observations, with my lowest testing error occurring with 200 features on a synthetic dataset with 100 observations."
  },
  {
    "objectID": "posts/Double Descent/index.html#part-0",
    "href": "posts/Double Descent/index.html#part-0",
    "title": "Double Descent in Overparameterized Regression",
    "section": "Part 0",
    "text": "Part 0\nIn the provided equation for finding the optimal weight vector in unregularized least-squares linear regression, problems arise when p &gt; n because it disrupts the invertability of the matrix X.\nThis specific element of our equation - (XTX)-1 - only works if… well… XTX is invertible. However, when p &gt; n, the operation XTX produces a matrix which is - according to chapter 11 of our lectures - singular, meaning the columns of the matrix are linearly dependant. In order for a matrix to be invertible, its columns must be linearly independent. If XTX, then the whole closed-form solution breaks.\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom logistic import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)"
  },
  {
    "objectID": "posts/Double Descent/index.html#testing-model-on-simple-data",
    "href": "posts/Double Descent/index.html#testing-model-on-simple-data",
    "title": "Double Descent in Overparameterized Regression",
    "section": "Testing Model on Simple Data",
    "text": "Testing Model on Simple Data\nAfter implementing the MyLinearRegression model and OverParameterized Optimizer in my source code, I verify functionality by trying to fit it to some 1D data.\n\n# Generate nonlinear 1D data\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\n\n# Apply polynomial feature map to data\nphi = RandomFeatures(8)\nphi.fit(X)\nX_transformed = phi.transform(X)\n\n\n# Create and fit regression model\nMLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(MLR)\nopt.fit(X_transformed, y)\n\n\n# Predict model on same inputs\ny_preds = MLR.predict(X_transformed)\n\n# Plot it!\nplt.figure(figsize=(4,4))\n# need to convert to numpy bc that is what Matplot works with\nplt.scatter(X.numpy(), y.numpy(), label=\"Data\", color='gray')\nplt.plot(X.numpy(), y_preds.numpy(), label=\"Predictions\", color='blue')\nplt.legend()\nplt.title(\"Overparameterized Linear Regression\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.show()"
  },
  {
    "objectID": "posts/Double Descent/index.html#part-c-double-descent-in-image-corruption-detection",
    "href": "posts/Double Descent/index.html#part-c-double-descent-in-image-corruption-detection",
    "title": "Double Descent in Overparameterized Regression",
    "section": "Part C: Double Descent in Image Corruption Detection",
    "text": "Part C: Double Descent in Image Corruption Detection\nHere, I apply my model to a more complicated dataset!\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower, cmap='gray_r')\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1, cmap='gray_r')\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
  },
  {
    "objectID": "posts/Double Descent/index.html#my-task-assess-model-performance-on-image-corruption-prediction-as-vary-n_features-in-randomfeatures",
    "href": "posts/Double Descent/index.html#my-task-assess-model-performance-on-image-corruption-prediction-as-vary-n_features-in-randomfeatures",
    "title": "Double Descent in Overparameterized Regression",
    "section": "My Task: Assess model performance on image corruption prediction as vary n_features in RandomFeatures",
    "text": "My Task: Assess model performance on image corruption prediction as vary n_features in RandomFeatures\nSteps 1. Train model with different numbers of features 2. Compute training and test Mean Squared Errors (MSEs) 3. Plot Curves 4. plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍vertical line at interpolation threshold (point where number of features used exceeds number of training samples)\n\n\"\"\"\nThe first step is to train our model on a set of different feature numbers.\nSo, we perform a sweep over 200 different features numbers - based on provided\nreference plots - and keep track of training and testing errors for each number\nof features\n\"\"\"\nnum_features = list(range(1, 201))\ntraining_errors = []\ntesting_errors = []\n\nfor feats in num_features:\n    phi = RandomFeatures(n_features=feats, activation=square)\n    phi.fit(X_train)\n    X_train_features = phi.transform(X_train)\n    X_test_features  = phi.transform(X_test)\n\n    MLR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(MLR)\n    opt.fit(X_train_features, y_train)\n\n    MSE_train = MLR.loss(X_train_features, y_train).item()\n    MSE_test = MLR.loss(X_test_features, y_test).item()\n\n    training_errors.append(MSE_train)\n    testing_errors.append(MSE_test)\n\nmin_test_error = min(testing_errors)\nmin_test_error_index = testing_errors.index(min_test_error)\noptimal_num_features = num_features[min_test_error_index]\n\n\n\"\"\"\nHere, we plot the actual graphs that represent the above feature sweeps results.\nSince we know the interpolation threshold is the point where number of features\nexceeds number of training samples, we can plot our vertical line at the point where\nthe number of features equals the number of training samples.\n\"\"\"\n\nfig, axes = plt.subplots(1, 2, figsize=(14,5))\n\n# Training Plot\naxes[0].scatter(num_features, training_errors, color='gray')\n# Interp threshold is where num features &gt; num training samples\n# So, it is when num features = num training samples\naxes[0].axvline(x=len(X_train), color='black', linewidth=2)\naxes[0].set_yscale(\"log\")\naxes[0].set_title(\"Mean Squared Error (training)\")\naxes[0].set_xlabel(\"# of Features\")\naxes[0].set_xlim(0, 200)\naxes[0].grid(True)\n\naxes[1].scatter(num_features, testing_errors, color='red')\naxes[1].axvline(x=len(X_train), color='red', linewidth=2)\naxes[1].set_yscale(\"log\")\naxes[1].set_title(\"Mean Squared Error (testing)\")\naxes[1].set_xlabel(\"# of Features\")\naxes[1].set_xlim(0, 200)\naxes[1].grid(True)\n\n\n\n\n\n\n\n\n\n# And now, state the optimal number of features my exploration found\nprint(f\"Lowest Test Error = {min_test_error}, Occurring at {optimal_num_features} features\")\n\nLowest Test Error = 319.56678645266777, Occurring at 197 features\n\n\nWhat is Optimal? - Based on the results above, the lowest test error was ~287.03, which occurred at 200 features. The interpolation threshold was 100 features - since I trained on 100 data observations - meaning that the optimal number of features occurred above the interpolation threshold."
  },
  {
    "objectID": "posts/Double Descent/index.html#discussion",
    "href": "posts/Double Descent/index.html#discussion",
    "title": "Double Descent in Overparameterized Regression",
    "section": "Discussion:",
    "text": "Discussion:\nThis project gave me hands-on experience with the idea of overparameterized training. I was able to explore the complexities of matrix operations, and how the standard closed-form of linear regression fails when number of features exceeds observations due to the nature of matrix invertability. It was fascinating to see how at a point where the math seems to “break,” we are actually able to achieve algorithmic improvement once we try and push beyond those boundaries! By sweeping across increasing numbers of features, I was able to visualize how both training and testing errors behave as a model’s complexity grows. Most notably, my model achieved its best testing error - or really lowest testing error - when using 200 features, which was twice the number of training data observations I was using. This result confirmed that beyond the interpolation threshold, complexity can actually improve model performance, and reduce overfitting. This blog post ultimately helped me understand more about how machine learning algorithms can excel in high-parameter scenarios, even if the intuition of the math we knew begins to fall apart. What an powerful mystery to unfold!"
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html",
    "href": "posts/Implementing Logistic Regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, I implement logistic regression from scratch using PyTorch and apply it to both computer-generated and real-world data. I begin by developing a LogisticRegression class and GradientDescentOptimizer class that supports both vanilla gradient descent and momentum-based updates. Through a series of experiments, I analyzed the relationship of changing learning rates, momentum/beta rates, overfitting, and real-world datasets with the logistic regression algorithm. The experiments include generating 2D data for visualizing decision boundaries, a higher-dimensional overfitting case, and a real-world bot vs. user classification dataset from Kaggle. Each experiment is paired with visualizations and reflections to illustrate key concepts of the blog post.\n\nimport torch\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# Clarifying torch.mean dimensions\nX = torch.tensor([\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0]   \n])\n\nprint(torch.mean(X, dim = 0)) # Averages all elements in a column\nprint(torch.mean(X, dim = 1)) # Averages all elements in a row\n\ntensor([1., 2., 3.])\ntensor([2., 2., 2.])\n\n\n\n# Just confirmation that the pyfile contents work properly\n\nX = torch.tensor([\n    [1.0, 2.0, 1.0],\n    [2.0, 1.0, 1.0],\n    [0.0, 3.0, 1.0]   \n])\ny = torch.tensor([0.0, 1.0, 1.0])\n\nmodel = LogisticRegression()\nmodel.w = torch.tensor([0.1, -0.2, 0.0])\n\nprint(\"Scores:\", model.score(X))\nprint(\"Loss:\", model.loss(X, y))\nprint(\"Gradient:\", model.grad(X, y))\n\nScores: tensor([-0.3000,  0.0000, -0.6000])\nLoss: tensor(0.7617)\nGradient: tensor([-0.1915, -0.5286, -0.2400])\n\n\n\n# Define functions to create classification data, plot logistic classification data\n# and draw a line on classification plot\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_logistic_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nX, y = classification_data(noise = 0.5)\n\nThis function trains a logistic regression model on a given dataset using gradient descent (with optional momentum) and visualizes the training process. It tracks the logistic loss over 100 iterations and plots the loss curve to illustrate convergence results. If the input data has 2 features (plus a bias term), it also plots the decision boundary over the dataset.\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef logistic_loss_and_plot(X, y, alpha=0.1, beta=0.0, max_iters=400):\n    # Instantiate the model and optimizer\n    model = LogisticRegression()\n    model.score(X)  # ensure w is initialized\n    opt = GradientDescentOptimizer(model)\n\n    # Track loss over iterations\n    loss_vec = []\n\n    for _ in range(max_iters):\n        loss = model.loss(X, y)\n        loss_vec.append(loss.item())\n        opt.step(X, y, alpha=alpha, beta=beta)\n\n    print(f\"Final Loss: {loss_vec[-1]:.4f} after {max_iters} iterations.\")\n\n    # Plot loss per iteration\n    plt.figure(figsize=(6, 4))\n    plt.plot(loss_vec, color=\"slategrey\", linewidth=1.5)\n    plt.scatter(range(len(loss_vec)), loss_vec, color=\"slategrey\", edgecolors=\"black\", s=10)\n    plt.xlabel(\"Iteration\", fontsize=12)\n    plt.ylabel(\"Loss\", fontsize=12)\n    plt.title(f\"Final Loss: {loss_vec[-1]:.4f} after {len(loss_vec)} iterations.\", fontsize=12)\n    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n    # If data is 2D, also plot decision boundary\n    if X.shape[1] == 3:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.set(xlim=(-1, 2), ylim=(-1, 2))\n        plot_logistic_data(X, y, ax)\n\n        # Draw logistic decision boundary\n        draw_line(model.w, -1, 2, ax, color=\"black\")\n        ax.set_title(\"Decision Boundary (Logistic Regression)\")\n        plt.show()\n\n\n\nIn this experiment,I generate 2D classification data using the classification_data() function. The data is linearly separable with moderate noise. I train a logistic regression model using vanilla gradient descent and visualize the decision boundary and loss curve over 100 iterations. This aims to show that my logistic regression implementation correctly converges, decreases monotonically, and that a decision boundary can be visualized.\n\nX, y = classification_data(noise=0.3, p_dims=2)\nprint(\"Plot for beta = 0.0\")\nlogistic_loss_and_plot(X, y, alpha=0.5, beta=0.0)\n\nPlot for beta = 0.0\nFinal Loss: 0.0502 after 400 iterations."
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html#abstract",
    "href": "posts/Implementing Logistic Regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, I implement logistic regression from scratch using PyTorch and apply it to both computer-generated and real-world data. I begin by developing a LogisticRegression class and GradientDescentOptimizer class that supports both vanilla gradient descent and momentum-based updates. Through a series of experiments, I analyzed the relationship of changing learning rates, momentum/beta rates, overfitting, and real-world datasets with the logistic regression algorithm. The experiments include generating 2D data for visualizing decision boundaries, a higher-dimensional overfitting case, and a real-world bot vs. user classification dataset from Kaggle. Each experiment is paired with visualizations and reflections to illustrate key concepts of the blog post.\n\nimport torch\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# Clarifying torch.mean dimensions\nX = torch.tensor([\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0],\n    [1.0, 2.0, 3.0]   \n])\n\nprint(torch.mean(X, dim = 0)) # Averages all elements in a column\nprint(torch.mean(X, dim = 1)) # Averages all elements in a row\n\ntensor([1., 2., 3.])\ntensor([2., 2., 2.])\n\n\n\n# Just confirmation that the pyfile contents work properly\n\nX = torch.tensor([\n    [1.0, 2.0, 1.0],\n    [2.0, 1.0, 1.0],\n    [0.0, 3.0, 1.0]   \n])\ny = torch.tensor([0.0, 1.0, 1.0])\n\nmodel = LogisticRegression()\nmodel.w = torch.tensor([0.1, -0.2, 0.0])\n\nprint(\"Scores:\", model.score(X))\nprint(\"Loss:\", model.loss(X, y))\nprint(\"Gradient:\", model.grad(X, y))\n\nScores: tensor([-0.3000,  0.0000, -0.6000])\nLoss: tensor(0.7617)\nGradient: tensor([-0.1915, -0.5286, -0.2400])\n\n\n\n# Define functions to create classification data, plot logistic classification data\n# and draw a line on classification plot\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_logistic_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nX, y = classification_data(noise = 0.5)\n\nThis function trains a logistic regression model on a given dataset using gradient descent (with optional momentum) and visualizes the training process. It tracks the logistic loss over 100 iterations and plots the loss curve to illustrate convergence results. If the input data has 2 features (plus a bias term), it also plots the decision boundary over the dataset.\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef logistic_loss_and_plot(X, y, alpha=0.1, beta=0.0, max_iters=400):\n    # Instantiate the model and optimizer\n    model = LogisticRegression()\n    model.score(X)  # ensure w is initialized\n    opt = GradientDescentOptimizer(model)\n\n    # Track loss over iterations\n    loss_vec = []\n\n    for _ in range(max_iters):\n        loss = model.loss(X, y)\n        loss_vec.append(loss.item())\n        opt.step(X, y, alpha=alpha, beta=beta)\n\n    print(f\"Final Loss: {loss_vec[-1]:.4f} after {max_iters} iterations.\")\n\n    # Plot loss per iteration\n    plt.figure(figsize=(6, 4))\n    plt.plot(loss_vec, color=\"slategrey\", linewidth=1.5)\n    plt.scatter(range(len(loss_vec)), loss_vec, color=\"slategrey\", edgecolors=\"black\", s=10)\n    plt.xlabel(\"Iteration\", fontsize=12)\n    plt.ylabel(\"Loss\", fontsize=12)\n    plt.title(f\"Final Loss: {loss_vec[-1]:.4f} after {len(loss_vec)} iterations.\", fontsize=12)\n    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n    # If data is 2D, also plot decision boundary\n    if X.shape[1] == 3:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n        ax.set(xlim=(-1, 2), ylim=(-1, 2))\n        plot_logistic_data(X, y, ax)\n\n        # Draw logistic decision boundary\n        draw_line(model.w, -1, 2, ax, color=\"black\")\n        ax.set_title(\"Decision Boundary (Logistic Regression)\")\n        plt.show()\n\n\n\nIn this experiment,I generate 2D classification data using the classification_data() function. The data is linearly separable with moderate noise. I train a logistic regression model using vanilla gradient descent and visualize the decision boundary and loss curve over 100 iterations. This aims to show that my logistic regression implementation correctly converges, decreases monotonically, and that a decision boundary can be visualized.\n\nX, y = classification_data(noise=0.3, p_dims=2)\nprint(\"Plot for beta = 0.0\")\nlogistic_loss_and_plot(X, y, alpha=0.5, beta=0.0)\n\nPlot for beta = 0.0\nFinal Loss: 0.0502 after 400 iterations."
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html#experiment-2-gradient-descent-with-momentum",
    "href": "posts/Implementing Logistic Regression/index.html#experiment-2-gradient-descent-with-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Experiment 2: Gradient Descent with momentum",
    "text": "Experiment 2: Gradient Descent with momentum\nUsing the same dataset as Experiment 1, I compare vanilla gradient descent with momentum-based gradient descent. I train each model for the same number of iterations and plot the training loss over time. This aims to show how momentum accelerates convergence and leads to a lower loss being achieved earlier than vanilla gradient descent (as seen above).\n\nprint(\"Plot for beta = 0.9\")\nlogistic_loss_and_plot(X, y, alpha=0.5, beta=0.9)\n\nPlot for beta = 0.9\nFinal Loss: 0.0166 after 400 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 3: Overfitting in Higher Dimensional Space\nTo illustrate overfitting, I generate data where the number of features p &gt; n. I train the model on the training set with more features than datapoints until it achieves near perfect accuracy, and then evaluate performance on the test set which has more datapoints than features. This experiment aims to show that while a model with a lot of features can lead to good training results, it can produce overfitting. Just because you achieve good loss does not mean you will have good accuracy on test data.\n\n# Fewer data points, more dimensions = overfitting risk\nX_train, y_train = classification_data(n_points=30, p_dims=100, noise=0.3)\nX_test, y_test = classification_data(n_points=300, p_dims=100, noise=0.3)\n\n# Plot training loss\nlogistic_loss_and_plot(X_train, y_train, alpha=0.5, beta=0.0)\n\nFinal Loss: 0.0030 after 400 iterations.\n\n\n\n\n\n\n\n\n\n\n# Recreate model for evaluation - logistic_loss_and_plot doesn't create global models\nmodel = LogisticRegression()\nmodel.score(X_train)\nopt = GradientDescentOptimizer(model)\n\nfor _ in range(100):\n    opt.step(X_train, y_train, alpha=0.1, beta=0.0)\n\ntrain_accuracy = (model.predict(X_train) == y_train)\ntest_accuracy = (model.predict(X_test) == y_test)\n\nprint(f\"Train Accuracy: {train_accuracy.float().mean():.4f}\")\nprint(f\"Test Accuracy:  {test_accuracy.float().mean():.4f}\")\n\nTrain Accuracy: 1.0000\nTest Accuracy:  0.8867\n\n\n\n\nIn this section, I analyze a real-world dataset containing features from user profiles on a Russian social media platform, VKontakte. The goal od this analysis is to train a model to predict whether a profile is a bot (target = 1) or a human (target = 0). I implement this using a logistic regression model with gradient descent, both with and without momentum. The analysis includes the following steps:\n\nData preprocessing: All feature values from the dataset must be converted to numeric values. I will drop column entries that are mostly missing, as well as data entries with absent values. Then I will split the dataset into training and test set using train-test split from scikit learn.\nFeature engineering\nTrain the Model: Fit the logistic regression model using gradient descent with and without momentum.\nVisualize Results: Plot the training loss data across gradient descent iterations for both approaches mentioned in step 3.\nCalculate the accuracy of the model on test data.\n\n\n\nStep 1: Data Loading and Preprocessing\nNotes on data provided on Kaggle\n\nNumerical features have NaNs preserved on purpose (i.e. these missing values might carry a meaning, such as inactivity)\nCategorical features: Any missing values are explicitly labeled ‘unknown’, and Boolean values are already converted to binary where applicable\n\nAs such, preprocessing should be optimized to only transform/pre-process entries where necessary, with these feature notes kept in mind. In other words, ‘unknown’ entries can be converted to 0 (not removed) and NaNs should be left in place/unchanged.\n\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Data Loading and Preprocessing\n\n# Load the data\ndf = pd.read_csv(\"bots_vs_users.csv\")\n\n# Replace 'Unknown' with 0 to indicate missing categorical value\ndf.replace(\"Unknown\", 0.0, inplace=True)\n\n# The only feature with string entry values is 'city', so we one-hot encode it\n# This creates a new column for each unique city value, with binary 0.0/1.0 value\n# Since there are only 4 distinct entries for this feature, this operation preferred to factorizing these values\ndf = pd.get_dummies(df, columns=[\"city\"], dtype=float)\n\n# Now, since we have possibly meaningful NaN values, i choose to replace them with -1\ndf = df.fillna(-1.0)\n\n# Drop the target column to create X and y\nX_df = df.drop(\"target\", axis=1)\n\n# Convert the feature matrix and target column to NumPy array for eventually use with PyTorch\n# Include astype(float) to ensure any possible type object entries are made into floats\nX_np = X_df.astype(float).values\ny = df[\"target\"].values\n\n# Scale the data to make sure all features on same scale\nscaler = StandardScaler()\nX = scaler.fit_transform(X_np)\n\n\n\nSplitting the Data: Train/Validation/Test Split\n\n# Split data into train (60%) and temp (40%) sets (To be turned to test and validation)\n\"\"\"\nI choose to pass stratify = y so that target distribution is maintained\nStratify makes sure that the distribution of entries in a certain column \nstays the same in the output data subset. In this case, we want the \ndistribution of bots and users to be the same. So, for example, say the \noriginal dataset had 50% bots and 50% users, us passing y to stratify\nwill ensure those proportions are maintained in X_train/y_train and \nX_temp/y_temp\n\"\"\"\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, train_size=0.6, random_state=802, stratify=y\n)\n\n# Split temp set evenly into validation (20%) and test (20%)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, train_size=0.5, random_state = 802, stratify=y_temp\n)\n\n# Convert these to PyTorch Tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\n\nX_validation = torch.tensor(X_val, dtype=torch.float32)\ny_validation = torch.tensor(y_val, dtype=torch.float32)\n\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n\n\n\nTraining Loop Function\n\ndef train_model(X_train, y_train, X_val, y_val, alpha=0.1, beta=0.0, num__iterations = 100):\n    LR = LogisticRegression()\n    opt = GradientDescentOptimizer(LR)\n    train_losses = []\n    val_losses = []\n\n    # Perform training iterations\n    for i in range(num__iterations):\n\n        # Since beta activates momentum in step, pass 0.0 unless otherwise indicated in function call\n        opt.step(X_train, y_train, alpha=alpha, beta=beta)\n\n        # Track the loss\n        # .item() extracts the Python scalar bc loss returns Tensor\n        train_loss = LR.loss(X_train, y_train).item()\n        val_loss = LR.loss(X_val, y_val).item()\n\n        # Append loss to arrays for plotting\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n    \n    return LR, train_losses, val_losses\n\n\n\nRun the Training Loop Function\n\n# Each function call below runs a training loop on our training/validation data.\n# The first call uses vanilla gradient descent (beta = 0.0)\n# The second call uses gradient descent with momentum (0.9)\n\nvanilla_LR, vanilla_train_loss, vanilla_validation_loss = train_model(\n    X_train, y_train, X_validation, y_validation, alpha=0.05, beta=0.0, num__iterations=100 \n)\n\nmomentum_LR, momentum_train_loss, momentum_validation_loss = train_model(\n    X_train, y_train, X_validation, y_validation, alpha=0.05, beta=0.9, num__iterations=100 \n)\n\n\n\nPlot Loss Curves\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\n\n# Plot 4 loss combinations\n\nplt.plot(vanilla_train_loss, \n         label=\"Vanilla Gradient Descent Training Loss\", \n         linestyle=\"-\", \n         color = \"red\")\n\nplt.plot(vanilla_validation_loss, \n         label=\"Vanilla Gradient Descent Validation Loss\", \n         linestyle=\"-\", \n         color = \"purple\")\n\nplt.plot(momentum_train_loss, \n         label=\"Gradient Descent W/ Momentum Training Loss\", \n         linestyle=\"--\", \n         color = \"red\")\n\nplt.plot(momentum_validation_loss, \n         label=\"Gradient Descent W/ Momentum Validation Loss\", \n         linestyle=\"--\", \n         color = \"purple\")\n\n# Give plot a title\nplt.title(\"Training and Validation Loss Over 100 Iterations\")\n\n# X-axis title\nplt.xlabel(\"# Iterations\")\n\n# y-label\nplt.ylabel(\"Loss\")\n\nplt.legend()\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTest Data Accuracy and Loss\n\ndef test_model(model, X_test, y_test):\n    preds = model.predict(X_test)\n    accuracy = (preds == y_test).float().mean().item()\n    loss = model.loss(X_test, y_test).item()\n\n    return accuracy, loss\n\n\nvanilla_accuracy, vanilla_loss = test_model(vanilla_LR, X_test, y_test)\nmomentum_accuracy, momentum_loss = test_model(momentum_LR, X_test, y_test)\n\nprint(f\"Vanilla Gradient Descent: \\nAccuracy = {vanilla_accuracy:.4f}; \\nLoss = {vanilla_loss:.2f}\\n\")\nprint(f\"Gradient Descent W/ Momentum: \\nAccuracy = {momentum_accuracy:.4f}; \\nLoss = {momentum_loss:.2f}\")\n\nVanilla Gradient Descent: \nAccuracy = 0.9064; \nLoss = 0.53\n\nGradient Descent W/ Momentum: \nAccuracy = 0.9379; \nLoss = 0.45\n\n\nExperiment 4 Conclusion: As we can see from the printed results, Vanilla gradient descent produces relatively high classification accuracy, but with a higher loss. This means the vanilla algorithm is less calibrated. As we transition to gradient descent with momentum, we achieve a higher accuracy AND lower loss, suggesting that keeping a record of prior model training steps can help improve future results!"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#abstract",
    "href": "posts/Classifying Palmers Penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract:",
    "text": "Abstract:\nThis blog post is an introductory exploration into understanding the process and procedures of ML classifications and predictions. Using the Palmers Penguin dataset, I was able to identify a set three features - Culmen Length (mm), Culmen Depth (mm), and Clutch Completion (T/F) - which produced the highest Logistic Regression (LR) cross validation score across all feature combinations. From this, I tested a variety of potential classification models - such as SVC, Random Forest, and Decision Tree, alongside the original LR model - adjusting parameter options such as gamma and max-depth identifying the LR model as that which performed best at classifying the training data. Then, with an “optimal” feature set and classification set identified - insofar as my experimentation indicated it to be optimal - I evaluated my model by splitting it our over the qualitative feature of my feature set, and showed that it only misidentified 1 penguin in the test data using a confusion matrix.\n\n# Imports\nimport pandas as pd\nimport numpy as np\n\n# Access data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "href": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "title": "Classifying Palmer Penguins",
    "section": "Our Data: A First Look",
    "text": "Our Data: A First Look\nUsing the preprocessing code provided by Prof. Phil as a basis, I copied the training data, and subsequently remove/revise columns of our dataset to make it easily useable for visualization. Specifically, I leave the Species column in so that I can group our data points in a more accessible manner - given the skills in Pandas/Seaborn I currently have\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create DataFrame for visualization\nX_train_visualize = train.copy()\nX_train_visualize[\"Species\"] = X_train_visualize[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nX_train_visualize[\"species_label\"] = le.fit_transform(X_train_visualize[\"Species\"])\nX_train_visualize = X_train_visualize.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Island\", \"Stage\"], axis = 1)\nX_train_visualize = X_train_visualize[X_train_visualize[\"Sex\"] != \".\"]\nX_train_visualize = X_train_visualize.dropna()\n\nNow, let’s take a look at our DataFrame\n\nX_train_visualize\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nspecies_label\n\n\n\n\n0\nChinstrap\nYes\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\n1\n\n\n1\nChinstrap\nYes\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\n1\n\n\n2\nGentoo\nYes\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\n2\n\n\n3\nGentoo\nYes\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\n2\n\n\n4\nChinstrap\nYes\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nGentoo\nYes\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\n2\n\n\n271\nAdelie\nNo\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\n0\n\n\n272\nAdelie\nYes\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\n0\n\n\n273\nAdelie\nYes\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\n0\n\n\n274\nChinstrap\nYes\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\n1\n\n\n\n\n256 rows × 10 columns\n\n\n\nWith our DataFrame on hand, we now move forward to part 1 of the blog post: Exploration of the data\n\n# Import Seaborn & matplotlib for visualizations\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 1: Pair Plotting Penguin Quantitative Features",
    "text": "Figure 1: Pair Plotting Penguin Quantitative Features\nUsing Seaborn’s pairplot function, I can plot all quantitative feature-pair scatterplots. Though I can’t add a title directly to a seaborn pairplot, since it is built on top of matplotlib, I can access plot attributes/modules such as figure, which holds all plot elements and can be used to add a title. See here for more info on the figure module: https://matplotlib.org/stable/api/figure_api.html#module-matplotlib.figure\n\npairplot = sns.pairplot(X_train_visualize[[\"Culmen Length (mm)\", \n                                \"Culmen Depth (mm)\", \n                                \"Flipper Length (mm)\", \n                                \"Body Mass (g)\", \n                                \"Delta 15 N (o/oo)\", \n                                \"Delta 13 C (o/oo)\", \n                                \"Species\"]], \n                                hue = \"Species\", \n                                palette = \"tab10\",\n                                corner=True)\n\n\npairplot.figure.suptitle(\"Pairplot of All Quantitative Penguin Features and Corresponding Distributions\",\n                         fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\nDiscussion:\nAbove we have used the Seaborn pairplot function to plot pairwise relationships between all quantitative features in our penguins training data set. Using Pandas fancy indexing, I have selected only the quantitative data in order to A.) limit the number of pairs plotted, as including qualitative data could make an excessively large figure, and B.) when I tried with qualitative data, data-points became so clustered around the limited values for each of those features - i.e. they would cluster around True and False in linear blobs - which made it very hard to distinguish between entries. While this visualization doesn’t necessarily offer a clear choice for optimal features - those features that would be “best” to fit a model on - it can at least offer some insight into where the penguin features experience overlap across species. If you are working with larger data sets, this may be a helpful first step in allowing the Data Scientist to get a “functional sense” of which entries may be productive to focus processing on, and eliminate early on unhelpful feature combinations which could speed up cross-validation/feature selection later down the line."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 2: Looking for Potential Biases in our Data Set",
    "text": "Figure 2: Looking for Potential Biases in our Data Set\nGiven that the above figure provides a pretty comprehensive look at our feature pairs, I thought it would be helpful to turn towards penguin counts to see if there might be biases in our data set. In other words, are there equal data points for each penguin species we are classifying/predicting?\n\nsns.histplot(X_train_visualize, x = \"Species\", hue = \"Species\", shrink = .75)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWhile this may seem to be a fairly simple visualization, I’d argue that it is one of the most insightful. By showing the count distribution of our penguin species, we begin to see where biases lie in our data set. In the case of the penguins, this means seeing which species are more or less present in the training dataset. If there is a dramatic difference between our different species, this means that we might run the risk of building a prediction model which performs unevenly across new samples. For example, it may be more poorly trained on Chinstrap penguins since there is a far lower count - and thus fewer data points to fit a model on - which could lead to incorrect predictions on future Chinstrap Penguins whose features fall outside the decision boundaries of our model. As such, the we should be wary of the difference between Gentoo and Adelie counts to Chinstrap, and as such should maybe consider features in which Chinstrap penguins have a higher standard deviation - such as Culmen Length - relative to that of Adelie and Gentoo."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 3: Summary Table",
    "text": "Figure 3: Summary Table\nWith both of previous figures in mind, it might now be helpful to get quantified information about how our feature relate across species and to other features.\n\nX_train_visualize.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]].aggregate([\"std\", \"mean\"])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n2.685713\n38.961111\n1.218430\n18.380556\n6.652184\n190.527778\n462.850335\n3722.916667\n0.428454\n8.861431\n0.567815\n-25.808814\n\n\nChinstrap\n3.456257\n48.771429\n1.137935\n18.346429\n7.366033\n195.821429\n410.148997\n3739.732143\n0.370290\n9.331004\n0.224608\n-24.567075\n\n\nGentoo\n2.783242\n47.133696\n1.016336\n14.926087\n6.061715\n216.739130\n498.976123\n5057.336957\n0.282566\n8.252573\n0.561689\n-26.145754\n\n\n\n\n\n\n\n\nDiscussion:\nThe above summary table shows the standard deviation and mean of all quantitative features in our data set, across the three species of penguins. This is again offers important insight into the breadth of our data points - how much variation we might find in a feature for a specific penguin species - and how each species’ feature sets might relate to one another - that is how close is the average culmen length for an Adelie penguin vs. a Gentoo penguin. Features which a higher standard deviation indicates that the data set might cover a wider/more representative portion of the species population, which could mean stronger predictions, but it could also mean more overlap with other species on that feature. This is pretty relative to the mean of those features, of course, but nonetheless offers an important reminder that there isn’t one “ideal” standard deviation. Instead, we must always consider it as a statistical value relative to the goal of our model and the context of our dataset. Additionally, features with a similar mean across species could indicate overlap in the data set, meaning it may not be the most optimal feature to fit our model on.\nFrom this data - which is visualized by the corresponding distributions on the diagonal of Figure 1 - we can see there are some sections of our data with significant overlap across species. This means it would not be best to build a model on these feature combinations."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 4 (After the Fact)",
    "text": "Figure 4 (After the Fact)\nHaving noticed that a qualitative feature was used to build our model, I wanted to see if there was some way that I could have visualized the quantitative data before building the model to see which feature - Sex or Clutch Completion - would be more helpful than the other.\n\nfig, ax = plt.subplots(1, 2, figsize = (9, 5))\n\np1 = sns.countplot(X_train_visualize, x = \"Clutch Completion\", hue = \"Species\", ax = ax[0])\np1 = sns.countplot(X_train_visualize, x = \"Sex\", hue = \"Species\", ax = ax[1], legend=False)\n\n\n\n\n\n\n\n\n\nDiscussion: (Extra - not part of original three visualizations)\nAfter completing the blog post, I returned to the data exploration/visualization. I was curious about how - considering the fact that clutch completion made it to our “optimal” feature set for classification - we could/would be able to predict if a qualitative feature would be part of our model’s feature set? So, while I tried a variety of different plotting functions, this was the best or really most useful visualization I could come up with, a simple count plot. Considering the fact that Clutch Completion made it to the classifier, I don’t see anything that would indicate it to be a strong distinguishing feature across the data set. All penguins have far more successful clutch completions (Yes entries) than not (No entries). Ultimately, this shows that the power of our model isn’t in single feature classifications, but in being able to compute the relationship between features across our data set."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "href": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Building The Model",
    "text": "Building The Model\nMoving now to part two, we need to actually build our model using features from the data set\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nNow that I have pre-processed the data, it’s time to determine which features will be used to build our model. Here we take a bit of a long approach, iterating through a variety of feature combinations and seeing which set has the best cross-validation score when fit by a Logistic Regression model.\nBefore processing the data, I decided to scale the quantitative features in order to speed up the computations. This process is result of receiving consistent warnings about max_iterations, and code remaining too slow even when the max_iter parameter on my LogisticRegression() constructor was set to 10,000. The warning produced this link: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\nAs such, I have included the StandardScaler() object and scaled/transformed my data as per the instructions on the linked page.\n\n# Cross Validations\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n# All feature columns\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', \n                  'Culmen Depth (mm)', \n                  'Flipper Length (mm)', \n                  'Body Mass (g)', \n                  'Delta 15 N (o/oo)', \n                  'Delta 13 C (o/oo)']\n\n# Initialize array for optimal columns and variable to store cross-validation\n# score for the optimal columns\noptimal_columns = []\nbest_LR_cv_score = 0\n\n# Scale data to facilitate a clean cross-validation process\nscaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_train_scaled[['Culmen Length (mm)', \n                'Culmen Depth (mm)', \n                'Flipper Length (mm)', \n                'Body Mass (g)', \n                'Delta 15 N (o/oo)', \n                'Delta 13 C (o/oo)']] = scaler.fit_transform(X_train_scaled[['Culmen Length (mm)', \n                                                                              'Culmen Depth (mm)', \n                                                                              'Flipper Length (mm)', \n                                                                              'Body Mass (g)', \n                                                                              'Delta 15 N (o/oo)', \n                                                                              'Delta 13 C (o/oo)']])\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    LR = LogisticRegression()\n    cv_scores_LR = cross_val_score(LR, X_train_scaled, y_train, cv=5)\n    cv_scores_mean = cv_scores_LR.mean()\n    if cv_scores_mean &gt; best_LR_cv_score:\n      best_LR_cv_score = cv_scores_mean\n      optimal_columns = cols\n\nprint(f\"The highest Cross-Validation Score for Logistic Regression was {best_LR_cv_score} \\nIt was modeled on the following features: \\n{optimal_columns}\")\n\n\nThe highest Cross-Validation Score for Logistic Regression was 0.996078431372549 \nIt was modeled on the following features: \n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nHaving identified our “optimal columns” - i.e. those with the best cross-validation score - I now explore how those features score when fit with different models.\n\n# Cross Validation for SVC\nfrom sklearn.svm import SVC\n# Define range of gamma values (as suggested in blog post assignment)\ngamma_values = 10.0**np.arange(-10, 10)\n\nbest_svc_cv_score = 0\noptimal_gamma = 0.0\n\nfor gamma in gamma_values:\n    svc = SVC(gamma = gamma)\n    cv_scores_svc = cross_val_score(svc, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_svc.mean()\n    if cv_scores_mean &gt; best_svc_cv_score:\n        best_svc_cv_score = cv_scores_mean\n        optimal_gamma = gamma\n        \n\nprint(f\"The highest Cross-Validation Score for SVC was {best_svc_cv_score} \\nIt was calculated with a gamma of {optimal_gamma}\")\n\nThe highest Cross-Validation Score for SVC was 0.9491704374057315 \nIt was calculated with a gamma of 0.1\n\n\n\n# Cross Validation for Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_DT_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    DT = DecisionTreeClassifier(max_depth = depth)\n    cv_scores_DT = cross_val_score(DT, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_DT.mean()\n    if cv_scores_mean &gt; best_DT_cv_score:\n        best_DT_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Decision Tree was {best_DT_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Decision Tree was 0.9452488687782805 \nIt was calculated with a max depth of 9\n\n\n\n# Cross Validation for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_RF_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    RF = RandomForestClassifier(max_depth = depth)\n    cv_scores_RF = cross_val_score(RF, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_RF.mean()\n    if cv_scores_mean &gt; best_RF_cv_score:\n        best_RF_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Random Forest was {best_RF_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Random Forest was 0.9491704374057315 \nIt was calculated with a max depth of 9"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "href": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluate",
    "text": "Evaluate\nWith our features chosen as well as our model - Logistic Regression - it is time to evaluate and test our model to see how well it performed\n\nPlotting Decision Regions\n\n# Preprocessing: Scale my data, leaving qualitative features alone\nscaler = StandardScaler()\noptimal_X_train_scaled = X_train[optimal_columns].copy()\noptimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n# Swap qualitative and quantitative to fit function provided on assignment\nplottable_opt_x_train_scaled = optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]]\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nLR = LogisticRegression()\nLR.fit(plottable_opt_x_train_scaled, y_train)\nplot_regions(LR, plottable_opt_x_train_scaled, y_train)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "href": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Testing & Confusion Matrix",
    "text": "Testing & Confusion Matrix\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Preprocess\nscaler = StandardScaler()\nX_test = X_test[optimal_columns].copy()\nX_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n\n# Produce a Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(optimal_X_train_scaled, y_train)\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 1, 10,  0],\n       [ 0,  0, 26]])\n\n\n\n# Print what each value in the Confusion Matrix represents in the context of our data\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "href": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Final Discussion:",
    "text": "Final Discussion:\nThis blog post taught me that, in a way, constructing predictive models is not a purely objective science. While the operations and computations encountered along the way are fundamentally based on logic, the problem of prediction is deeply shaped by the context of the data and the data scientist. At the end of the day the limitations of hardware, software, and our implementations of the algorithmic world cannot - or at least have not yet - account for all the variability in the physical world: The realm of computational classification and prediction is but a finite set of instructions to put together a puzzle of infinite possibilities! Even though the model I coded scored well on the test data, my capacity to correctly predict future penguin species is still constrained: 1 penguin incorrectly predicted still means the model is not perfect. Furthermore, it was trained on much fewer Chinstrap penguins than Adelie or Gentoo, which means I might run a greater risk of mis-predicting Chinstrap penguins down the line. Even more, though it scored well on the test data, that is still only a fraction of penguins that there are out there to predict: there is not necessarily a guarantee that my model will continue to predict at such a high level. This isn’t to take away from the performance of the model, or the power of prediction, but more a matter of respecting the dynamism of the physical world. If I have taken anything away from the blog post, it’s that we must look to build models in such a way that respects this truth."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Limitations of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\n\n\n\n\n\n\nJiffy Lesica\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2: Design and Impact of Automated Decision Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nJiffy Lesica\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\n\n\n\n\n\n\nJiffy Lesica\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution Based Weight Vector Optimization\n\n\n\n\n\nImplementation of Evolution Based Weight Vector Optimization\n\n\n\n\n\nJul 5, 2026\n\n\nJames Cummings, Jiffy Lesica, Yahya Rahhawi, Lukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Descent in Overparameterized Regression\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2025\n\n\nJiffy Lesica\n\n\n\n\n\n\nNo matching items"
  }
]