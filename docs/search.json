[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Jiffy and this is my blog for CS451!"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#abstract",
    "href": "posts/Classifying Palmers Penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract:",
    "text": "Abstract:\nThis blog post is an introductory exploration into understanding the process and procedures of ML classifications and predictions. Using the Palmers Penguin dataset, I was able to identify a set three features - Culmen Length (mm), Culmen Depth (mm), and Clutch Completion (T/F) - which produced the highest Logistic Regression (LR) cross validation score across all feature combinations. From this, I tested a variety of potential classification models - such as SVC, Random Forest, and Decision Tree, alongside the original LR model - adjusting parameter options such as gamma and max-depth identifying the LR model as that which performed best at classifying the training data. Then, with an “optimal” feature set and classification set identified - insofar as my experimentation indicated it to be optimal - I evaluated my model by splitting it our over the qualitative feature of my feature set, and showed that it only misidentified 1 penguin in the test data using a confusion matrix.\n\n# Imports\nimport pandas as pd\nimport numpy as np\n\n# Access data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "href": "posts/Classifying Palmers Penguins/index.html#our-data-a-first-look",
    "title": "Classifying Palmer Penguins",
    "section": "Our Data: A First Look",
    "text": "Our Data: A First Look\nUsing the preprocessing code provided by Prof. Phil as a basis, I copied the training data, and subsequently remove/revise columns of our dataset to make it easily useable for visualization. Specifically, I leave the Species column in so that I can group our data points in a more accessible manner - given the skills in Pandas/Seaborn I currently have\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create DataFrame for visualization\nX_train_visualize = train.copy()\nX_train_visualize[\"Species\"] = X_train_visualize[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nX_train_visualize[\"species_label\"] = le.fit_transform(X_train_visualize[\"Species\"])\nX_train_visualize = X_train_visualize.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Island\", \"Stage\"], axis = 1)\nX_train_visualize = X_train_visualize[X_train_visualize[\"Sex\"] != \".\"]\nX_train_visualize = X_train_visualize.dropna()\n\nNow, let’s take a look at our DataFrame\n\nX_train_visualize\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nspecies_label\n\n\n\n\n0\nChinstrap\nYes\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\n1\n\n\n1\nChinstrap\nYes\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\n1\n\n\n2\nGentoo\nYes\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\n2\n\n\n3\nGentoo\nYes\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\n2\n\n\n4\nChinstrap\nYes\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nGentoo\nYes\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\n2\n\n\n271\nAdelie\nNo\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\n0\n\n\n272\nAdelie\nYes\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\n0\n\n\n273\nAdelie\nYes\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\n0\n\n\n274\nChinstrap\nYes\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\n1\n\n\n\n\n256 rows × 10 columns\n\n\n\nWith our DataFrame on hand, we now move forward to part 1 of the blog post: Exploration of the data\n\n# Import Seaborn & matplotlib for visualizations\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-1-pair-plotting-penguin-quantitative-features",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 1: Pair Plotting Penguin Quantitative Features",
    "text": "Figure 1: Pair Plotting Penguin Quantitative Features\nUsing Seaborn’s pairplot function, I can plot all quantitative feature-pair scatterplots. Though I can’t add a title directly to a seaborn pairplot, since it is built on top of matplotlib, I can access plot attributes/modules such as figure, which holds all plot elements and can be used to add a title. See here for more info on the figure module: https://matplotlib.org/stable/api/figure_api.html#module-matplotlib.figure\n\npairplot = sns.pairplot(X_train_visualize[[\"Culmen Length (mm)\", \n                                \"Culmen Depth (mm)\", \n                                \"Flipper Length (mm)\", \n                                \"Body Mass (g)\", \n                                \"Delta 15 N (o/oo)\", \n                                \"Delta 13 C (o/oo)\", \n                                \"Species\"]], \n                                hue = \"Species\", \n                                palette = \"tab10\",\n                                corner=True)\n\n\npairplot.figure.suptitle(\"Pairplot of All Quantitative Penguin Features and Corresponding Distributions\",\n                         fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\nDiscussion:\nAbove we have used the Seaborn pairplot function to plot pairwise relationships between all quantitative features in our penguins training data set. Using Pandas fancy indexing, I have selected only the quantitative data in order to A.) limit the number of pairs plotted, as including qualitative data could make an excessively large figure, and B.) when I tried with qualitative data, data-points became so clustered around the limited values for each of those features - i.e. they would cluster around True and False in linear blobs - which made it very hard to distinguish between entries. While this visualization doesn’t necessarily offer a clear choice for optimal features - those features that would be “best” to fit a model on - it can at least offer some insight into where the penguin features experience overlap across species. If you are working with larger data sets, this may be a helpful first step in allowing the Data Scientist to get a “functional sense” of which entries may be productive to focus processing on, and eliminate early on unhelpful feature combinations which could speed up cross-validation/feature selection later down the line."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-2-looking-for-potential-biases-in-our-data-set",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 2: Looking for Potential Biases in our Data Set",
    "text": "Figure 2: Looking for Potential Biases in our Data Set\nGiven that the above figure provides a pretty comprehensive look at our feature pairs, I thought it would be helpful to turn towards penguin counts to see if there might be biases in our data set. In other words, are there equal data points for each penguin species we are classifying/predicting?\n\nsns.histplot(X_train_visualize, x = \"Species\", hue = \"Species\", shrink = .75)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWhile this may seem to be a fairly simple visualization, I’d argue that it is one of the most insightful. By showing the count distribution of our penguin species, we begin to see where biases lie in our data set. In the case of the penguins, this means seeing which species are more or less present in the training dataset. If there is a dramatic difference between our different species, this means that we might run the risk of building a prediction model which performs unevenly across new samples. For example, it may be more poorly trained on Chinstrap penguins since there is a far lower count - and thus fewer data points to fit a model on - which could lead to incorrect predictions on future Chinstrap Penguins whose features fall outside the decision boundaries of our model. As such, the we should be wary of the difference between Gentoo and Adelie counts to Chinstrap, and as such should maybe consider features in which Chinstrap penguins have a higher standard deviation - such as Culmen Length - relative to that of Adelie and Gentoo."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-3-summary-table",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 3: Summary Table",
    "text": "Figure 3: Summary Table\nWith both of previous figures in mind, it might now be helpful to get quantified information about how our feature relate across species and to other features.\n\nX_train_visualize.groupby(\"Species\")[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]].aggregate([\"std\", \"mean\"])\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n2.685713\n38.961111\n1.218430\n18.380556\n6.652184\n190.527778\n462.850335\n3722.916667\n0.428454\n8.861431\n0.567815\n-25.808814\n\n\nChinstrap\n3.456257\n48.771429\n1.137935\n18.346429\n7.366033\n195.821429\n410.148997\n3739.732143\n0.370290\n9.331004\n0.224608\n-24.567075\n\n\nGentoo\n2.783242\n47.133696\n1.016336\n14.926087\n6.061715\n216.739130\n498.976123\n5057.336957\n0.282566\n8.252573\n0.561689\n-26.145754\n\n\n\n\n\n\n\n\nDiscussion:\nThe above summary table shows the standard deviation and mean of all quantitative features in our data set, across the three species of penguins. This is again offers important insight into the breadth of our data points - how much variation we might find in a feature for a specific penguin species - and how each species’ feature sets might relate to one another - that is how close is the average culmen length for an Adelie penguin vs. a Gentoo penguin. Features which a higher standard deviation indicates that the data set might cover a wider/more representative portion of the species population, which could mean stronger predictions, but it could also mean more overlap with other species on that feature. This is pretty relative to the mean of those features, of course, but nonetheless offers an important reminder that there isn’t one “ideal” standard deviation. Instead, we must always consider it as a statistical value relative to the goal of our model and the context of our dataset. Additionally, features with a similar mean across species could indicate overlap in the data set, meaning it may not be the most optimal feature to fit our model on.\nFrom this data - which is visualized by the corresponding distributions on the diagonal of Figure 1 - we can see there are some sections of our data with significant overlap across species. This means it would not be best to build a model on these feature combinations."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "href": "posts/Classifying Palmers Penguins/index.html#figure-4-after-the-fact",
    "title": "Classifying Palmer Penguins",
    "section": "Figure 4 (After the Fact)",
    "text": "Figure 4 (After the Fact)\nHaving noticed that a qualitative feature was used to build our model, I wanted to see if there was some way that I could have visualized the quantitative data before building the model to see which feature - Sex or Clutch Completion - would be more helpful than the other.\n\nfig, ax = plt.subplots(1, 2, figsize = (9, 5))\n\np1 = sns.countplot(X_train_visualize, x = \"Clutch Completion\", hue = \"Species\", ax = ax[0])\np1 = sns.countplot(X_train_visualize, x = \"Sex\", hue = \"Species\", ax = ax[1], legend=False)\n\n\n\n\n\n\n\n\n\nDiscussion: (Extra - not part of original three visualizations)\nAfter completing the blog post, I returned to the data exploration/visualization. I was curious about how - considering the fact that clutch completion made it to our “optimal” feature set for classification - we could/would be able to predict if a qualitative feature would be part of our model’s feature set? So, while I tried a variety of different plotting functions, this was the best or really most useful visualization I could come up with, a simple count plot. Considering the fact that Clutch Completion made it to the classifier, I don’t see anything that would indicate it to be a strong distinguishing feature across the data set. All penguins have far more successful clutch completions (Yes entries) than not (No entries). Ultimately, this shows that the power of our model isn’t in single feature classifications, but in being able to compute the relationship between features across our data set."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "href": "posts/Classifying Palmers Penguins/index.html#building-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Building The Model",
    "text": "Building The Model\nMoving now to part two, we need to actually build our model using features from the data set\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nNow that I have pre-processed the data, it’s time to determine which features will be used to build our model. Here we take a bit of a long approach, iterating through a variety of feature combinations and seeing which set has the best cross-validation score when fit by a Logistic Regression model.\nBefore processing the data, I decided to scale the quantitative features in order to speed up the computations. This process is result of receiving consistent warnings about max_iterations, and code remaining too slow even when the max_iter parameter on my LogisticRegression() constructor was set to 10,000. The warning produced this link: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\nAs such, I have included the StandardScaler() object and scaled/transformed my data as per the instructions on the linked page.\n\n# Cross Validations\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n# All feature columns\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', \n                  'Culmen Depth (mm)', \n                  'Flipper Length (mm)', \n                  'Body Mass (g)', \n                  'Delta 15 N (o/oo)', \n                  'Delta 13 C (o/oo)']\n\n# Initialize array for optimal columns and variable to store cross-validation\n# score for the optimal columns\noptimal_columns = []\nbest_LR_cv_score = 0\n\n# Scale data to facilitate a clean cross-validation process\nscaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_train_scaled[['Culmen Length (mm)', \n                'Culmen Depth (mm)', \n                'Flipper Length (mm)', \n                'Body Mass (g)', \n                'Delta 15 N (o/oo)', \n                'Delta 13 C (o/oo)']] = scaler.fit_transform(X_train_scaled[['Culmen Length (mm)', \n                                                                              'Culmen Depth (mm)', \n                                                                              'Flipper Length (mm)', \n                                                                              'Body Mass (g)', \n                                                                              'Delta 15 N (o/oo)', \n                                                                              'Delta 13 C (o/oo)']])\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    LR = LogisticRegression()\n    cv_scores_LR = cross_val_score(LR, X_train_scaled, y_train, cv=5)\n    cv_scores_mean = cv_scores_LR.mean()\n    if cv_scores_mean &gt; best_LR_cv_score:\n      best_LR_cv_score = cv_scores_mean\n      optimal_columns = cols\n\nprint(f\"The highest Cross-Validation Score for Logistic Regression was {best_LR_cv_score} \\nIt was modeled on the following features: \\n{optimal_columns}\")\n\n\nThe highest Cross-Validation Score for Logistic Regression was 0.996078431372549 \nIt was modeled on the following features: \n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nHaving identified our “optimal columns” - i.e. those with the best cross-validation score - I now explore how those features score when fit with different models.\n\n# Cross Validation for SVC\nfrom sklearn.svm import SVC\n# Define range of gamma values (as suggested in blog post assignment)\ngamma_values = 10.0**np.arange(-10, 10)\n\nbest_svc_cv_score = 0\noptimal_gamma = 0.0\n\nfor gamma in gamma_values:\n    svc = SVC(gamma = gamma)\n    cv_scores_svc = cross_val_score(svc, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_svc.mean()\n    if cv_scores_mean &gt; best_svc_cv_score:\n        best_svc_cv_score = cv_scores_mean\n        optimal_gamma = gamma\n        \n\nprint(f\"The highest Cross-Validation Score for SVC was {best_svc_cv_score} \\nIt was calculated with a gamma of {optimal_gamma}\")\n\nThe highest Cross-Validation Score for SVC was 0.9491704374057315 \nIt was calculated with a gamma of 0.1\n\n\n\n# Cross Validation for Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_DT_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    DT = DecisionTreeClassifier(max_depth = depth)\n    cv_scores_DT = cross_val_score(DT, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_DT.mean()\n    if cv_scores_mean &gt; best_DT_cv_score:\n        best_DT_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Decision Tree was {best_DT_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Decision Tree was 0.9452488687782805 \nIt was calculated with a max depth of 9\n\n\n\n# Cross Validation for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Define range of gamma values (as suggested in blog post assignment)\ndepth_values = np.arange(1, 20)\n\nbest_RF_cv_score = 0\noptimal_depth = 0\n\nfor depth in depth_values:\n    RF = RandomForestClassifier(max_depth = depth)\n    cv_scores_RF = cross_val_score(RF, X_train[optimal_columns], y_train, cv=5)\n    cv_scores_mean = cv_scores_RF.mean()\n    if cv_scores_mean &gt; best_RF_cv_score:\n        best_RF_cv_score = cv_scores_mean\n        optimal_depth = depth\n        \n\nprint(f\"The highest Cross-Validation Score for Random Forest was {best_RF_cv_score} \\nIt was calculated with a max depth of {optimal_depth}\")\n\nThe highest Cross-Validation Score for Random Forest was 0.9491704374057315 \nIt was calculated with a max depth of 9"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "href": "posts/Classifying Palmers Penguins/index.html#evaluate",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluate",
    "text": "Evaluate\nWith our features chosen as well as our model - Logistic Regression - it is time to evaluate and test our model to see how well it performed\n\nPlotting Decision Regions\n\n# Preprocessing: Scale my data, leaving qualitative features alone\nscaler = StandardScaler()\noptimal_X_train_scaled = X_train[optimal_columns].copy()\noptimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n# Swap qualitative and quantitative to fit function provided on assignment\nplottable_opt_x_train_scaled = optimal_X_train_scaled[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]]\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nLR = LogisticRegression()\nLR.fit(plottable_opt_x_train_scaled, y_train)\nplot_regions(LR, plottable_opt_x_train_scaled, y_train)"
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "href": "posts/Classifying Palmers Penguins/index.html#testing-confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Testing & Confusion Matrix",
    "text": "Testing & Confusion Matrix\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Preprocess\nscaler = StandardScaler()\nX_test = X_test[optimal_columns].copy()\nX_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]] = scaler.fit_transform(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]])\n\n\n# Produce a Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(optimal_X_train_scaled, y_train)\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 1, 10,  0],\n       [ 0,  0, 26]])\n\n\n\n# Print what each value in the Confusion Matrix represents in the context of our data\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "href": "posts/Classifying Palmers Penguins/index.html#final-discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Final Discussion:",
    "text": "Final Discussion:\nThis blog post taught me that, in a way, constructing predictive models is not a purely objective science. While the operations and computations encountered along the way are fundamentally based on logic, the problem of prediction is deeply shaped by the context of the data and the data scientist. At the end of the day the limitations of hardware, software, and our implementations of the algorithmic world cannot - or at least have not yet - account for all the variability in the physical world: The realm of computational classification and prediction is but a finite set of instructions to put together a puzzle of infinite possibilities! Even though the model I coded scored well on the test data, my capacity to correctly predict future penguin species is still constrained: 1 penguin incorrectly predicted still means the model is not perfect. Furthermore, it was trained on much fewer Chinstrap penguins than Adelie or Gentoo, which means I might run a greater risk of mis-predicting Chinstrap penguins down the line. Even more, though it scored well on the test data, that is still only a fraction of penguins that there are out there to predict: there is not necessarily a guarantee that my model will continue to predict at such a high level. This isn’t to take away from the performance of the model, or the power of prediction, but more a matter of respecting the dynamism of the physical world. If I have taken anything away from the blog post, it’s that we must look to build models in such a way that respects this truth."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog Post 2: Design and Impact of Automated Decision Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#introduction",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Introduction:",
    "text": "Introduction:\nThe purpose of this blog post is to explore the quantitative and ethical depths of decision theory in classification. The specific goal of the assignment was to try and identify a scoring function and threshold which would optimize expected profit per borrower for a bank who is deciding who to give loans to, using any of the features contained in our data set and any method. I started by exploring the data visually, trying to understand how different feature columns related to each other, and how bivariate feature combinations may correlate to loan status - i.e. whether a borrower repaid their loan or defaulted. After this, I fit a logistic regression classification onto the data set using varying feature combinations to see if there were any particular combinations which optimized initial classification accuracy. Once a model was fit, I used the model weights to calculate linear risk scores for each of the data entries/borrowers, and used these scores to identify a threshold risk score (borrowers lower than the threshold were approved, and above the threshold denied) which maximized profit per borrower for the bank. I found that the optimal threshold score was 1.414, which produced a profit per borrower of $1391.53 on the training data. Using this score as a threshold on the test data, I identified a maximized profit of $1102.18 per borrower. While this was lower than the training value, that is expected. However, upon processing through the test data to find its specific optimizing threshold, I found that my proposed threshold of 1.414 was not maximizing profit to its full potential on the test data. Instead, maximum profits of $1347.19 was found at a threshold of 1.010. So, while the proposed system kept profit high, it did not achieve its highest potential on the test data.\n\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-1-explore-the-data",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 1: Explore the Data",
    "text": "Part 1: Explore the Data\nRemember, 1 means a person defaulted on loan, and 0 means they repaid in full\n\n# Import seaborn, our visualization library\nimport seaborn as sns\n\n# Create a copy of our dataset, as since we use df_train later in this blogpost we want to prevent\n# any errors from being passed on to these visualizations due to the scope of df_train\nfor_viz = df_train.copy()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-1",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 1:",
    "text": "Figure 1:\nFirst, we want to understand how loan intent varies with different features, specifically borrower age, length of employment, and homeownership status. Not only with this offer us qualitative insights, but also help us understand the distribution of borrowers we are encountering, and what patterns may emerge in the data.\n\nsns.displot(data = for_viz, x = \"person_age\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe are dealing with, for the most part, younger borrowers. The vast majority of borrowers across intents are 40 or younger, and above 40 years of age we see a steep drop off in borrowers. We see an especially high number of young borrowers in the data set seeking a loan for education."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-2",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 2:",
    "text": "Figure 2:\n\nsns.displot(data = for_viz, x = \"person_emp_length\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nWe see similar shapes in these visualizations as in that comparing borrower age and loan intent, but the context of course gives it a different meaning. Across the board, we see that the majority of borrowers have been employed for under 20 years. Again, the highest count of borrowers is found in the subset of those seeking education loans. After 20 years of employment, loan seekers effectively disappear from view. The lowest number of loan seekers are found for home improvement loans."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-3",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 3:",
    "text": "Figure 3:\n\nsns.displot(data = for_viz, x = \"person_home_ownership\", col = \"loan_intent\", kind = \"hist\", hue = \"loan_intent\", col_wrap = 2)\n\n\n\n\n\n\n\n\n\nDiscussion:\nIn nearly all of the visualizations, the highest number of borrowers seeking a loan are also renters. From highest to lowest counts, the borrowers’ home ownership status is RENT, MORTGAGE, and OWN. This makes sense intuitively as home ownership is likely a proxy for financial secure individuals who may be less likely to seek a loan as a form of financial support. The only place we do not see this trend is in the Home Improvement visualization. Here, the plurality of borrowers are mortgaging their home. This also makes sense, however, because those who have mortgages are likely “newer” homeowners who are undergoing more renovations/home improvements than those who have owned a home for a long time."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#figure-4",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Figure 4:",
    "text": "Figure 4:\nNext, I want to see the relationship between loan interest rates and loans as a percent of a borrowers income, and how this relationship may shape loan status - i.e. whether a borrower repays or defaults. Additionally, it will be helpful to compare visualization methods to see which Seaborn plots are more effective for such a large dataset\n\nsns.jointplot(for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\", kind = \"kde\", height=6)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = for_viz, x = \"loan_int_rate\", y = \"loan_percent_income\", hue = \"loan_status\")\n\n\n\n\n\n\n\n\n\nDiscussion:\nAs seen above, using a scatterplot to visualize such a large set of data points is not a very effective approach. There is a great deal of overlap across our points, meaning that classification trends may be getting lost behind other data points. When looking at the joint plot, we first note from the edge plots that there are far more repaying than defaulting borrowers in our data set. Additionally, a higher distribution of borrowers repay loans they have a low interest rate, and their loan is a lower percent of their income. However, the reality remains that - based on the inner JointGrid - that there is still a great deal of overlap between borrowers of both loan status in relation to these two variables. Feature selection could not be done here purely through visualization."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-2-building-a-model",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 2: Building a Model",
    "text": "Part 2: Building a Model\n\n# First, establish the target column. This won't change so I initialize it here.\nTARGET_COL = \"loan_status\"\n\n# Even before processing the data, we perform some preliminary changes to the DataFrame to ensure\n# future processing does not interfere with indexing, such as dropping entries with empty values,\n# and converting qualitative features to one-hot encoded dummies\ndf_train = pd.get_dummies(df_train)\ndf_train = df_train.dropna()\n\n\n# Create a preprocessing function that will scale data and \n# create an X_train and y_train based on chosen feature columns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\ndef preprocess(df, quant_cols, qual_cols, target_col):\n\n    df_new = pd.DataFrame()\n    df_new[quant_cols] = df[quant_cols]\n    df_new[qual_cols] = df[qual_cols]\n    df_new_target = df[target_col]\n\n    scaler = StandardScaler()\n    \n    df_new[quant_cols] = scaler.fit_transform(df_new[quant_cols])\n    df_new[qual_cols] = df_new[qual_cols]\n    \n    return df_new, df_new_target\n\n\nExperiment with features/models\n\n# Model imports\nfrom sklearn.linear_model import LogisticRegression\n\n\n# First, I will test a model on all quantitative/qualitative features in the dataset\n\nX1_quant_cols = [\"person_age\", \n                    \"person_income\", \n                    \"person_emp_length\",\n                    \"loan_int_rate\", \n                    \"loan_amnt\", \n                    \"loan_percent_income\",\n                    \"cb_person_default_on_file_N\",\n                    \"cb_person_default_on_file_Y\",\n                    \"cb_person_cred_hist_length\"]\n\nX1_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\",\n                    \"loan_intent_VENTURE\",\n                    \"loan_intent_EDUCATION\",\n                    \"loan_intent_MEDICAL\",\n                    \"loan_intent_HOMEIMPROVEMENT\",\n                    \"loan_intent_PERSONAL\",\n                    \"loan_intent_DEBTCONSOLIDATION\", ]\n\n\nX1_train, y_train = preprocess(df_train, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\nLR1 = LogisticRegression()\nfit1 = LR1.fit(X1_train, y_train)\nLR1.score(X1_train, y_train)\n\n0.8498712184048544\n\n\n\n# Having fit and scored a model on all feature columns, we now explore smaller feature combinations,\n# looking to see if any particular combination maximizes classification accuracy\nX2_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"person_emp_length\"]\n\nX2_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX2_train, y_train = preprocess(df_train, X2_quant_cols, X2_qual_cols, TARGET_COL)\n\nLR2 = LogisticRegression()\nfit2 = LR2.fit(X2_train, y_train)\nLR2.score(X2_train, y_train)\n\n0.8468153839437726\n\n\n\n# Similar to above block but for alternative feature combination\n\nX3_quant_cols = [\"loan_int_rate\",  \n                    \"loan_percent_income\",\n                    \"loan_amnt\"]\n\nX3_qual_cols = [\"person_home_ownership_MORTGAGE\",\n                    \"person_home_ownership_RENT\",\n                    \"person_home_ownership_OWN\"]\n\nX3_train, y_train = preprocess(df_train, X3_quant_cols, X3_qual_cols, TARGET_COL)\n\nLR3 = LogisticRegression()\nfit3 = LR3.fit(X3_train, y_train)\nLR3.score(X3_train, y_train)\n\n0.844632645043\n\n\n\n# Now that we have fit a Logistic Regression model on our data, we have access to the\n# Model weights via the .coef_ attribute.\n# However, we to transform this array into a column because otherwise it is just a row,\n# complicating necessary computations in the future\nprint(fit1.coef_.T)\n\n[[-0.0423979 ]\n [ 0.04612073]\n [-0.02431996]\n [ 1.03609245]\n [-0.57955282]\n [ 1.3239725 ]\n [-0.0143236 ]\n [ 0.0143236 ]\n [-0.00851694]\n [-0.27565869]\n [ 0.46232549]\n [-1.75917877]\n [-0.87364387]\n [-0.6441703 ]\n [-0.02154856]\n [ 0.24354326]\n [-0.46649075]\n [ 0.15978062]]"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#part-3-find-a-threshold",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Part 3: Find a Threshold",
    "text": "Part 3: Find a Threshold\n\n# Define a function that calculates the linear scores of our model\n# By calculating the cross product between our predictors and model weights\ndef linear_score(X, w):\n    return X@w.T\n\nX1_scores = linear_score(X1_train, fit1.coef_)\nX1_scores = X1_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX2_scores = linear_score(X2_train, fit2.coef_)\nX2_scores = X2_scores.iloc[:, 0] # All rows and first column to convert to Series\n\nX3_scores = linear_score(X3_train, fit3.coef_)\nX3_scores = X3_scores.iloc[:, 0] # All rows and first column to convert to Series\n\n\n# Plot the scores of our linear score function to see score distribution\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(X3_scores, bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\n\n# Now that we know our score distributions, we can select a range of possible thresholds\n# and define a function to identify the threshold which maximizes profit\ndef calculate_repaid_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"]) / 100) ** 10) - df[\"loan_amnt\"])\n\ndef calculate_default_profit(df):\n    return (df[\"loan_amnt\"] * ((1 + 0.25 * (df[\"loan_int_rate\"] / 100)) ** 3) - (1.7 * df[\"loan_amnt\"]))\n\ndef plot_best_threshold(df, scores, y_train):\n    best_profit = -1000000000\n    best_threshold = 0\n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4)) \n    for t in np.linspace(0, 10, 100):\n        y_pred = scores &gt; t\n        tp = (y_pred == 0) & (y_train == 0)\n        # tp.sum()\n        fp = (y_pred == 0) & (y_train == 1)\n        # fp.sum()\n        profit = calculate_repaid_profit(df[tp]).sum() + calculate_default_profit(df[fp]).sum()\n        profit /= len(df)\n        ax.scatter(t, profit, color = \"steelblue\", s = 10)\n        if profit &gt; best_profit: \n            best_profit = profit\n            best_threshold = t\n\n\n    ax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\n    labs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net Profit\", title = f\"Best Profit Per Borrower ${best_profit:.2f} at best threshold t = {best_threshold:.3f}\")\n\nplot_best_threshold(df_train, X3_scores, y_train)"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#evaluate-my-model-from-the-banks-perspective",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Evaluate My Model from the Bank’s Perspective",
    "text": "Evaluate My Model from the Bank’s Perspective\n\nOPTIMAL_THRESHOLD = 1.414\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test = pd.get_dummies(df_test)\ndf_test = df_test.dropna()\nX_test, y_test = preprocess(df_test, X1_quant_cols, X1_qual_cols, TARGET_COL)\n\ntest_scores = linear_score(X_test, fit1.coef_)\ntest_scores = test_scores.iloc[:, 0]\n\ny_pred = test_scores &gt; OPTIMAL_THRESHOLD\ntest_tp = (y_pred == 0) & (y_test == 0)\ntest_tn = (y_pred == 1) & (y_test == 1)\ntest_profit = calculate_repaid_profit(df_test[test_tp]).sum() + calculate_default_profit(df_test[test_tn]).sum()\ntest_profit /= len(df_test)\nprint(f\"Optimal Profit is ${test_profit:.2f}\")\nplot_best_threshold(df_test, test_scores, y_test)\n\nOptimal Profit is $1102.18\n\n\n\n\n\n\n\n\n\n\ndf_test[\"risk_score\"] = test_scores\nage_risk_score = df_test.groupby(\"person_age\")[[\"person_age\", \"risk_score\"]].mean()\n\n\nsns.scatterplot(df_test, x = \"person_age\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_age_approved = approved[\"person_age\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_age_denied = denied[\"person_age\"].mean()\nprint(f\" Average age of approved borrower: {avg_age_approved} vs. Average age of denied borrower: {avg_age_denied}\")\n\n Average age of approved borrower: 27.87536718422157 vs. Average age of denied borrower: 26.935751295336786\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the average age of borrowers approved with our threshold is less than 1 year older than the average age of denied borrowers. As such, it appears that the people of varying age groups have a similar degree of access to credit under my proposed system.\n\ncount_medical_total =  df_test[df_test[\"loan_intent_MEDICAL\"]]\ncount_medical_approved = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_medical_default = df_test[df_test[\"loan_intent_MEDICAL\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\n\nprint(f\"Percentage of medical seeking borrowers which were approved: {(len(count_medical_approved)/len(count_medical_total))*100:.2f}%\")\nprint(f\"Percentage of approved medical seeking borrowers who defaulted: {(len(count_medical_default)/len(count_medical_approved))*100:.2f}%\")\n\n\nPercentage of medical seeking borrowers which were approved: 77.54%\nPercentage of approved medical seeking borrowers who defaulted: 15.02%\n\n\n\ncount_venture_total =  df_test[df_test[\"loan_intent_VENTURE\"]]\ncount_venture_approved = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_venture_default = df_test[df_test[\"loan_intent_VENTURE\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of venture seeking borrowers which were approved: {(len(count_venture_approved)/len(count_venture_total))*100:.2f}%\")\nprint(f\"Percentage of approved venture seeking borrowers who defaulted: {(len(count_venture_default)/len(count_venture_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 89.94%\nPercentage of approved medical seeking borrowers who defaulted: 7.50%\n\n\n\ncount_education_total =  df_test[df_test[\"loan_intent_EDUCATION\"]]\ncount_education_approved = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD)]\ncount_education_default = df_test[df_test[\"loan_intent_EDUCATION\"] & (df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD) & (df_test[\"loan_status\"])]\nprint(f\"Percentage of education seeking borrowers which were approved: {(len(count_education_approved)/len(count_education_total))*100:.2f}%\")\nprint(f\"Percentage of approved education seeking borrowers who defaulted: {(len(count_education_default)/len(count_education_approved))*100:.2f}%\")\n\nPercentage of medical seeking borrowers which were approved: 88.78%\nPercentage of approved medical seeking borrowers who defaulted: 10.73%\n\n\n\n\nAnalysis:\nBased on the above data, borrowers seeking a loan for medical reasons have a more difficult time accessing credit under my proposed system. Only 77.54% of borrowers from the test set who were seeking medical loans were approved compared to venture at 89.94% and education borrowers at 88.78%. However, at the same time, the rate of default was higher amongst medical borrowers - 15.02% - compared to venture - 7.50% - and education - 10.73%.\n\nsns.scatterplot(df_test, x = \"person_income\", y = \"risk_score\")\nplt.axhline(y=OPTIMAL_THRESHOLD, color='red', linestyle='--')\napproved = df_test[df_test[\"risk_score\"] &lt; OPTIMAL_THRESHOLD]\navg_income_approved = approved[\"person_income\"].mean()\ndenied = df_test[df_test[\"risk_score\"] &gt; OPTIMAL_THRESHOLD]\navg_income_denied = denied[\"person_income\"].mean()\nprint(f\" Average income of approved borrower: {avg_income_approved} vs. Average income of denied borrower: {avg_income_denied}\")\n\n Average income of approved borrower: 71671.01657574486 vs. Average income of denied borrower: 41555.21658031088\n\n\n\n\n\n\n\n\n\n\n\nAnalysis:\nOn average, the data indicates that the income of borrowers approved with our threshold is notably higher - ~$30,000 higher - than the income of denied borrowers. As such, it appears that the people of lower income groups may have a more difficult time accessing credit under this system."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#concluding-remarks",
    "title": "Blog Post 2: Design and Impact of Automated Decision Systems",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nIn conclusion, this post examined decision theory in classification through both quantitative and ethical lenses, aiming to optimize a bank’s expected profit per borrower. The analysis began with visual exploration of the data, followed by fitting logistic regression models using various feature combinations. By computing linear risk scores, an initial optimal threshold of 1.414 was identified — yielding $1391.53 per borrower on the training data and $1102.18 on the test data. However, further analysis of the test set revealed an optimal threshold of 1.010, which maximized profit at $1347.19 per borrower, indicating that while the system maintained high profitability, it did not capture the test data’s full profit potential.\nAt the end of the study, I explored the fairness of this model. Fairness in general is a difficult concept to define, but is complicated to a high degree in the context of machine learning models which make predictions about people. A machine learning model can never achieve full fairness - making a decision about an outcome only after considering not only all features/traits of a person relevant to that outcome, but also the context-specificity of those features - as ML is fundamentally built on generalization from examples. So, instead the question of fairness in this post should not be understood as an absolute, but relative to the context at hand: is the model fair enough? Also, the notion of fairness depends on the position of the evaluator - what fairness means to me fundamentally as a person is different to someone whose primary goal is to develop a system which maximizes profits, and therefore makes decisions about borrowers with historical default rates in mind.\nAfter analyzing the outcome data, I noticed that the percent of approved borrowers seeking a medical loan was over 10% lower than that of education/venture seeking borrowers. However, at the same time, medical borrowers had a default rate which was 5% higher than the other two loan seeking groups. From this, I am inclined to ask whether, considering their higher rate of default, it is fair for medical borrowers to have a harder time accessing credit? On one hand, I want to say this is not fair, as even though the borrowers have a history of higher defaults, the value/importance of a medical loan - to potentially save ones life - is much higher than that of an education or venture loan seeker. But at the same time, I wonder whether a bank-loaning model is realistically possible or responsible to include this value in an automated decision making model: can a profit maximizing model actually objectively quantify the ethical/lived importance of a loan to a borrower? Ultimately, these observations highlight the inherent tension between profit optimization and equitable access to credit. While the model accurately identifies risk patterns based on historical data, it falls short in capturing broader social values, particularly for medical needs where the stakes may be much higher. This raises critical ethical questions: should decision systems rely solely on past performance, or must they also adjust to recognize the importance of timely, life-saving credit?"
  }
]