---
title: "Limitations of the Quantitative Approach to Bias and Fairness"
author: "Jiffy Lesica"
bibliography: refs.bib
format: html
---
In his 2022 lecture, *The Limits of the Quantitative Approach to Discrimination*, Arvind Narayanan puts forth a provocative claim on the role of quantitative methods in the contemporary age: “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan 2022). However, such a claim can only be fully appreciated in the fuller context of his speech, including not only its evidence and subclaims, but also Narayanan’s own positionality. As Narayanan explains in his speech, the harmful effects of Quantitative Methods seen today are not due of any inherent flaw, but instead because we have misunderstood the role and value of numbers in a human society. As a quantitative scholar himself, Narayanan can provide a distinct level of specificity to this claim – of insider insight – which helps illuminate the tangible realities which beg us to take seriously that – as simple as it may sound – numbers actually can lie.

Narayanan highlights 7 key limitations of quantitative methods which he has observed in his time as a quantitative scientist: Choice of null hypothesis, Use of snapshot datasets, Use of data provided by company, Explaining away discrimination, Wrong locus of intervention, The objectivity illusion, and Performativity.

The *objectivity illusion* is the quantitative limitation which, in a way, is at the heart of all other limitations put forth in Narayanan’s essay. As such, it is at the heart of the logic which my essay puts forth. Acknowledging it necessarily brings about complications of snapshot data, explaining away discrimination, and even the performative policies which quantitative science has brought about. The objectivity illusion refers to the false belief that quantitative research is objective because numbers are objective. Quantitative research involves subjective decisions made by researchers at multiple steps throughout the process such as – as mentioned above – choice of dataset or data collection methods, how to frame research questions and findings, which variables to control and to test on, and what “counts” as statistical evidence of discrimination: “In a typical research paper I would say researchers make at least 10-20 subjective choices, each of which could substantially alter their conclusions” (Narayanan 2022). The objectivity illusion is also at the heart of the previously mentioned limitations of snapshot data sets. As D’ignazio and Klein explain, “data are not neutral or objective. They are products of unequal social relations, and this context is essential for conducting accurate, ethical analysis” (Data Feminism, Introduction). Contemporary theories in the humanities, while not directly engaging questions of the quantitative world, offer language which is helpful to clarify the objectivity illusion.

In his book *Crossing and Dwellings*, for example, Thomas Tweed draws from insights of his scholarship to encourage a dynamic and itinerant approach to religious theory. The reason I bring such a seemingly distant resource to this conversation is, well, because it may not be as distant as we think. In fact, the model of theory which Tweed puts forth is quite similar to the “adversarial collaboration” approach of interpreting data which Narayanan mentions as “admirable”, in that it “explicitly acknowledges that scientists have their biases, their pet theories, their favored and hoped for conclusions” which will ultimately bias – in conscious or unconscious ways – the methodological approach they bring to their research (Narayanan 2022). As Tweed writes, “it is precisely because we stand in a particular place that we are able to see, to know, to narrate” (Tweed 18). There is no independent access to the experiences or subjective states of another, only that which the scholar can ‘see’ – used by Tweed to represent the multi-sensory reception of the corporeal world before us (Tweed 17). And, it is from those sightings that a scholar is able to construct meaning using the “categories and criteria they inherit, revise, and create” (Tweed 18). In the quantitative world, such sightings are at particular risk of being incomplete not because it is inherently worse, but because it is just simply not possible yet – whether by nature or by the limitations of modern technology – to capture the depth and variety of human experience with numbers. It is in recognizing this reality that the first steps may be taken towards deconstructing the epistemic hierarchy which grips the quantitative world.

It is ultimately from the objectivity illusion that other limitations emerge. For example, take the *Choice of null hypothesis*. Quantitative communities have “settled on the absence of discrimination” as the default assumption. Choosing to see the world as impartial puts the burden of proof on those alleging discrimination, and naively pretends that quantitative scholars – or really that anyone – has the capacity to live in a world void of human subjectivity. Such a choice not only ignores, but may even perpetuate discrimination as it brushes aside the possibility of systemic discrimination – which manifests in implicit, everyday forms and compound over time – and holds us back from implementing the critical interventions which may help address discriminatory realities. To standardize the absence of discrimination holds a massive normative significance in that it encourages us to be blind to inequality unless it presents itself clearly and ‘objectively’, and even then, retain a level of skepticism towards it until it can be quantitatively confirmed. Narayanan’s observations about the choice and the null hypothesis point us to consider the principles upon which we build quantitative methods, and particularly how quantitative methods and researchers operate in relation to power. Quantitative scholars who follow the framework of Data Feminism, for example, would argue that such considerations should manifest in challenges to unequal power structures and working toward justice (Data Feminism, Introduction). However, even before such a principle can be acted on, scholars must first take seriously the existence of power structures, and the ways in which the data they evaluate grows within, exists in relation to, and is drawn from those contexts. 

Furthermore, most datasets upon which quantitative methods are employed are collected from a “single system at a single point in time,” and it is even rarer for people to go out and collect new data. Such *snapshot datasets* are thus only frames in a much larger picture which inevitably involves systemic, social, and structural trends, “forcing the researcher to ignore people’s broader circumstances and the past discrimination they may have experienced” (Narayanan, 2022). If we assume that such data can provide sufficient view of discrimination, or a lack thereof, we are encouraged to believe discrimination happens in snapshots – in “discrete moments of time” and quantitatively observable ways – which is simply never how discrimination has operated. Other quantitative scholars affirm this limitation, such as D’ignazio and Klein who argue that “the process of converting life experience into data always necessarily entails a reduction of that experience” (Data Feminism, Introduction).

It is these misapprehensions which ultimately lead to Explaining away discrimination. In evaluating statistical disparities, quantitative researchers often diminish/set aside evidence of discrimination through their choice of analytical variables. Specifically, in trying to make their research logically/interpretatively sound, they control for variables that are tied to social constructs like race, subsequently excluding many of the proximal ways in which discrimination may actually operate. For example, Narayanan cites the 2004 study by Bertrand and Mullainathan which sent fake resumes to employers to test if an applicant’s race had an impact on the likelihood of them getting an interview. The way they implemented this was by creating pairs of resumes which were identical except for the applicant’s name, which would either be “White-sounding” (Emily, Greg, etc.) or “Black-sounding” (Lakisha, Jamal, etc.) (Narayanan 2022). While the study did find that “White names” were 50% more likely to receive a callback, it also limits our understanding of what data may signal someone’s social identity to their name: “Literally every other variable that might signal race or be correlated with race [other than one’s name] is held constant between the two conditions” (Narayanan 2022). Thus, while the study may have revealed discrimination in practice, it may have also “drastically underestimated” the degree of discrimination found in hiring practices (Narayanan 2022). 

A 2016 paper by Mittelstadt et al. further highlights this limitation, asserting that “the outputs of algorithms also require interpretation.” As such, ‘objective’ correlations can come to reflect the interpreter’s “unconscious motivations, particular emotions, deliberate choices, socio-economic determinations, geographic or demographic influences” (Mittelstadt et al. 2016). This point is reiterated in *Fairness and Machine Learning*, in which Barocas, Hardt, and Narayanan himself discuss the complexities which come with building fair machine learning models. If systematic discrimination persists in the world through subtle and systemic channels, it can’t be wholly identified in data that excludes those channels: “In real datasets, most attributes tend to be proxies for demographic variables,” meaning it is both irresponsible and dangerous to essentialize demographic identity to a single element, such as name signifying race (Fairness and Machine Learning 17).

However, while on one hand arguing about the present harms of quantitative methods, it would be insufficient to characterize Narayanan’s lecture as an exposé. He isn’t just saying what is wrong with this system, but rather presents a call to action and reformed thinking about the quantitative world. Eliminating quantitative methods from the conversation is neither desirable nor attainable, for the limitations which are found today are “not inherent to quantitative work” (Narayanan 2022). A better way of engaging with the quantitative world is possible, and steps made toward it can be seen today. On the surface, the proposition of taking seriously the subjectivity of human experience – of both the researcher and the researched – is not technically sophisticated enough. On the surface, the idea of developing research methods to measure and interpret systemic forms of discrimination – methods which “dig deeper” into the assumptions and politics which inevitably shape our engagement with the world (Narayanan 2022) – may not seem quantitatively plausible. However, it is only once we realize that the epistemic role of quantitative methods in a human world can never be absolute, because it is one of many ways of knowing the world, that we may begin to shift from predicting the world into fully reflecting on, understanding, and progressing from the form it inhabits right now.

The 2019 study conducted by Obermeyer et al., *Dissecting racial bias in an algorithm used to manage the health of populations*, provides a critical example of this digging deeper, and how quantitative methods can be employed to uncover and address systemic biases in algorithmic decision-making processes, specifically in healthcare risk prediction. In this research, the authors examined a commercially deployed prediction algorithm intended to identify patients who could benefit from extra medical support, known as "high-risk care management" programs (HRCMPs) (Obermeyer et al. 2019). By exploring the data produced from one of these algorithms through quantitative methods, Obermeyer and colleagues revealed that the algorithm systematically underestimated the healthcare needs of Black patients. Specifically, they found that Black patients who had identical algorithmic risk scores - i.e. who were considered of equally high medical risk/eligible for HRCMP as their white counterparts - were considerably sicker on average. Specifically, they found that Black patients who were predicted as “very-high-risk” (at the point of being auto-identified from HRCMP enrollment) had 26% more chronic health conditions when compared to their White counterparts. After further searching, the authors identified that the algorithm predicts health care costs rather than illness, targeting patients with high costs are eligible for HRCMPs. However, in doing so the algorithm mistakenly equated lower healthcare expenditures by Black patients with lower healthcare needs, overlooking the structural inequalities that produce disparate costs across demographic groups. 

While Black patients may spend less on healthcare than white patients, it is not because they are less sick. Instead, the authors identified that such a disparity arises from “unequal access to care” which has been driven by historical relationships – such as Black patients having reduced trust in the doctors – or socioeconomic barriers – such as access to transportation – which proximally impact the extent to which Black patients engage the health care system (Obermeyer et al. 2019). If these patients are cared for less often, then they will ultimately spend less on health care, contributing to the predictive biases which the study identified.

This study provides an insightful application of the notion of calibration as detailed in *Fairness and Machine Learning*. Calibration, a quantitative fairness criterion, requires that risk scores accurately reflect outcomes equivalently across different demographic groups (Barocas et al. 61-62). In the context of this study, a calibrated model ensures that, conditional on risk score, “predictions do not favor Whites or Blacks anywhere in the risk distribution” (Obermeyer et al. 2019). Technically, the healthcare algorithm satisfied this calibration criterion when assigning risk scores based on healthcare expenditure. However, the study illuminated a crucial limitation: calibration alone does not account for systemic biases embedded within the outcomes themselves - in this case, healthcare costs rather than actual health condition or needs. While calibration seeks to produce proportional consistency, it can perpetuate deep-rooted inequities if the target outcome itself is unjustly biased. Relying on proxies like healthcare spending is problematic as it disproportionately reflects unequal access and historical discrimination rather than genuine patient health needs. In other words, even a technically fair model - one that accurately predicts on healthcare expenditures - can remain unjust if it systematically disadvantages groups facing structural inequality.

This study highlights the benefits of employing ‘epistemically humble’ (to stay in line with Narayanan’s language) quantitative methods in analyses of discrimination. The authors' evaluation allows them to reveal how a seemingly neutral algorithm actually perpetuated discrimination. By quantifying the precise and various extents of disparity, the researchers make the reality of discrimination tangible, clear, and actionable. Subsequently, their analysis guides readers through possible interventions – such as reformulating the algorithm itself, or changing the data it is ultimately fed – that could substantially mitigate racial disparities, thus promoting equality. Therefore, Obermeyer et al’s study not only underscores the value of quantitative methods in exposing hidden biases, but also emphasizes the need to critically evaluate and choose fairness criteria that align with equity and justice. 

Narayanan’s critique is not an outright dismissal of quantitative methods but rather a call to critically examine the assumptions and limitations that shape their applications. My analysis of Narayanan’s claim, contextualized through his concept of the *objectivity illusion*, has shown that the limitations he identifies are not inherent to quantitative methods, but arise from a failure to acknowledge the subjective decisions embedded in quantitative research processes. As this essay has argued, understanding that data is the product of contextually embedded relations and realities is key to dismantling the epistemic hierarchy that privileges quantitative approaches as neutral or objective.

@barocasFairnessMachineLearning2023
@d2023data
@mittelstadt2016ethics
@narayanan2022limits
@Tweed+2008
@obermeyer2019dissecting